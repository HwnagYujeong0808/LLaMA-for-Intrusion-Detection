{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pickle\n",
    "import math\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "class LLMGraphTransformer(nn.Module):\n",
    "    def __init__(self, model_name=\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\", device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # Load the tokenizer and model for TinyLlama\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(self.device)\n",
    "\n",
    "        # Ensure padding token is set for TinyLlama\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "        # New layers to process edge features and reduce text logits dimension\n",
    "        self.edge_fc = nn.Linear(77, 64).to(self.device)\n",
    "        self.edge_dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "        # Reduce text logits to match edge embedding dimensions\n",
    "        self.text_fc = nn.Linear(self.model.config.vocab_size, 64).to(self.device)\n",
    "        \n",
    "        # Final classification layer to output 7 classes\n",
    "        self.classifier = nn.Linear(128, 7).to(self.device)\n",
    "\n",
    "    def forward(self, batch_text, edge_features):\n",
    "        # Tokenize text\n",
    "        inputs = self.tokenizer(batch_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
    "        outputs = self.model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "        \n",
    "        # Get the logits for the last token in each sequence\n",
    "        text_logits = outputs.logits[:, -1, :]  # Shape is (batch_size, vocab_size)\n",
    "        text_emb = self.text_fc(text_logits)    # Reduce text logits to (batch_size, 64)\n",
    "\n",
    "        # Process edge features through a fully connected layer\n",
    "        edge_emb = self.edge_fc(edge_features)  # Shape (batch_size, 64)\n",
    "        edge_emb = self.edge_dropout(edge_emb)\n",
    "\n",
    "        # Concatenate the text logits and the edge feature embeddings\n",
    "        combined_logits = torch.cat((text_emb, edge_emb), dim=1)  # Shape (batch_size, 128)\n",
    "        \n",
    "        # Pass through final classifier layer to get 7-class output\n",
    "        final_logits = self.classifier(combined_logits)  # Shape (batch_size, 7)\n",
    "        \n",
    "        return final_logits\n",
    "\n",
    "    def generate_text(self, graph_data, labels, max_new_tokens=50):\n",
    "        # Convert the graph adjacency list to text directly within this method\n",
    "        batch_text = []\n",
    "        for node, neighbors in enumerate(graph_data):\n",
    "            if isinstance(neighbors, (list, set, np.ndarray)):\n",
    "                for neighbor in neighbors:\n",
    "                    question = f\"What is the relationship between Node {node} and Node {neighbor}? Choices: {', '.join(labels)}.\"\n",
    "                    batch_text.append(question)\n",
    "            else:\n",
    "                question = f\"What is the relationship between Node {node} and Node {neighbors}? Choices: {', '.join(labels)}.\"\n",
    "                batch_text.append(question)\n",
    "\n",
    "        # Tokenize and generate predictions\n",
    "        inputs = self.tokenizer(batch_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
    "        outputs = self.model.generate(\n",
    "            inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "        generated_text = [self.tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "        return generated_text\n",
    "\n",
    "\n",
    "def balance_data(data, labels, n_samples_per_label):\n",
    "    # Find unique labels and their counts\n",
    "    random.seed(42)\n",
    "    label_groups = {}\n",
    "    for label in np.unique(labels):\n",
    "        label_indices = np.where(labels == label)[0]\n",
    "        # If the label has fewer samples than the target, we use replace=True to oversample.\n",
    "        sampled_indices = np.random.choice(label_indices, size=n_samples_per_label, replace=(len(label_indices) < n_samples_per_label))\n",
    "        label_groups[label] = sampled_indices\n",
    "\n",
    "    # Concatenate the balanced data\n",
    "    balanced_indices = np.concatenate(list(label_groups.values()))\n",
    "    balanced_data = data[balanced_indices]\n",
    "    balanced_labels = labels[balanced_indices]\n",
    "\n",
    "    return balanced_data, balanced_labels\n",
    "\n",
    "\n",
    "def process_llm_output(llm_output):\n",
    "    llm_output = llm_output.lower().strip()\n",
    "    label_mapping = {\n",
    "        'Benign':0, 'BruteForce':1, 'DoS':2, 'DDoS':3, 'Web':4, 'Bot':5, 'Infilteration':6\n",
    "    }\n",
    "    for keyword, index in label_mapping.items():\n",
    "        if keyword in llm_output:\n",
    "            return index\n",
    "    return -1\n",
    "\n",
    "\n",
    "def save_data_splits(train, val, test, train_labels, val_labels, test_labels, path=\"data_splits/ces-cic\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    with open(os.path.join(path, \"train.pkl\"), \"wb\") as f:\n",
    "        pickle.dump((train, train_labels), f)\n",
    "    with open(os.path.join(path, \"val.pkl\"), \"wb\") as f:\n",
    "        pickle.dump((val, val_labels), f)\n",
    "    with open(os.path.join(path, \"test.pkl\"), \"wb\") as f:\n",
    "        pickle.dump((test, test_labels), f)\n",
    "    print(\"Data splits and labels saved successfully.\")\n",
    "\n",
    "def load_data_splits(path=\"data_splits/ces-cic\"):\n",
    "    with open(os.path.join(path, \"train.pkl\"), \"rb\") as f:\n",
    "        train, train_labels = pickle.load(f)\n",
    "    with open(os.path.join(path, \"val.pkl\"), \"rb\") as f:\n",
    "        val, val_labels = pickle.load(f)\n",
    "    with open(os.path.join(path, \"test.pkl\"), \"rb\") as f:\n",
    "        test, test_labels = pickle.load(f)\n",
    "    print(\"Data splits and labels loaded successfully.\")\n",
    "    return train, val, test, train_labels, val_labels, test_labels\n",
    "\n",
    "def fit(args):\n",
    "    data = args[\"dataset\"]\n",
    "    binary = args[\"binary\"]\n",
    "\n",
    "    # Update the path to use ../cyber_gnn/ instead of datasets/\n",
    "    path = \"datasets/\" + data\n",
    "    if not path.endswith('/'):\n",
    "        path += '/'\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load the data manually (edge_feat, label, adj, adj_lists, config)\n",
    "    edge_feat = np.load(path + \"edge_feat_scaled.npy\", allow_pickle=True)\n",
    "    edge_feat = torch.tensor(edge_feat, dtype=torch.float, device=device)\n",
    "\n",
    "    # Load the label for multiclass classification\n",
    "    label = np.load(path + \"label_mul.npy\", allow_pickle=True)\n",
    "    label = torch.tensor(label, dtype=torch.long, device=device)\n",
    "    adj = np.load(path + \"adj_random.npy\", allow_pickle=True)\n",
    "    with open(path + 'adj_random_list.dict', 'rb') as file:\n",
    "        adj_lists = pickle.load(file)\n",
    "\n",
    "    config = {\n",
    "        \"num_of_layers\": 3,\n",
    "        \"num_heads_per_layer\": [6, 6, 6],\n",
    "        \"num_features_per_layer\": [edge_feat.shape[1], 8, 8, 8],\n",
    "        \"num_identity_feats\": 8,\n",
    "        \"add_skip_connection\": False,\n",
    "        \"bias\": True,\n",
    "        \"dropout\": 0.2\n",
    "    }\n",
    "\n",
    "    # Initialize LLMGraphTransformer using TinyLlama\n",
    "    llm_graph_transformer = LLMGraphTransformer(model_name=\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\", device=device)\n",
    "\n",
    "    # Define labels for relationship types\n",
    "    labels = ['Benign', 'BruteForce', 'DoS', 'DDoS', 'Web', 'Bot', 'Infilteration']\n",
    "    # Define the optimizer with Adam\n",
    "    optimizer = torch.optim.Adam(llm_graph_transformer.parameters(), lr=1e-5)\n",
    "    \n",
    "    # Assuming `train_labels` holds your training set labels\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    # Update CrossEntropyLoss with class weights\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "    num_edges = len(edge_feat)\n",
    "    label_cpu = label.cpu().numpy()\n",
    "    unique, counts = np.unique(label_cpu, return_counts=True)\n",
    "\n",
    "    balanced_data, balanced_labels = balance_data(np.arange(num_edges), label_cpu, n_samples_per_label=120)\n",
    "\n",
    "    # Check if saved splits exist, else create and save them\n",
    "    if not os.path.exists(\"data_splits/train.pkl\"):\n",
    "        # Perform initial train-validation-test split and save the splits\n",
    "        train_val, test, train_val_labels, test_labels = train_test_split(\n",
    "            balanced_data, balanced_labels, test_size=0.1, stratify=balanced_labels, random_state=42\n",
    "        )\n",
    "        train, val, train_labels, val_labels = train_test_split(\n",
    "            train_val, train_val_labels, test_size=0.1, stratify=train_val_labels, random_state=42\n",
    "        )\n",
    "        save_data_splits(train, val, test, train_labels, val_labels, test_labels)\n",
    "    else:\n",
    "        # Load the saved splits and their labels for consistent use\n",
    "        train, val, test, train_labels, val_labels, test_labels = load_data_splits()\n",
    "\n",
    "    print(len(train), len(val), len(test))\n",
    "\n",
    "    # Print the distribution of labels for each set\n",
    "    print(\"Label distribution in Train Set:\")\n",
    "    unique_train, counts_train = np.unique(train_labels, return_counts=True)\n",
    "    print(dict(zip(unique_train, counts_train)))\n",
    "\n",
    "    print(\"Label distribution in Validation Set:\")\n",
    "    unique_val, counts_val = np.unique(val_labels, return_counts=True)\n",
    "    print(dict(zip(unique_val, counts_val)))\n",
    "\n",
    "    print(\"Label distribution in Test Set:\")\n",
    "    unique_test, counts_test = np.unique(test_labels, return_counts=True)\n",
    "    print(dict(zip(unique_test, counts_test)))\n",
    "\n",
    "    times = []\n",
    "    trainscores = []\n",
    "    valscores = []\n",
    "\n",
    "    for epoch in range(10):\n",
    "        print(\"Epoch: \", epoch)\n",
    "        random.shuffle(train)\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Print the number of batches\n",
    "        print(f\"Training data size: {len(train)}\")\n",
    "        print(f\"Number of batches: {len(train) // 10}\")\n",
    "        \n",
    "        for batch in range(int(len(train) / 10)):  # Batch size is 10\n",
    "            batch_edges = train[10 * batch:10 * (batch + 1)]\n",
    "            \n",
    "            if len(batch_edges) == 0:\n",
    "                print(f\"Skipping empty batch {batch + 1}\")\n",
    "                continue\n",
    "\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Convert batch_edges to text\n",
    "            batch_text = llm_graph_transformer.generate_text(batch_edges, labels, max_new_tokens=10)\n",
    "\n",
    "            # Generate logits from text inputs\n",
    "            edge_batch = edge_feat[batch_edges]\n",
    "            logits = llm_graph_transformer(batch_text, edge_batch)\n",
    "            \n",
    "            # Ensure logits and labels are both on the same device\n",
    "            logits = logits.to(device)\n",
    "            batch_labels = label[batch_edges].to(device)\n",
    "\n",
    "            # Calculate loss using logits and target labels\n",
    "            loss = loss_fn(logits, batch_labels)\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "            acc_train = f1_score(label_cpu[batch_edges], predicted_labels.cpu().numpy(), average=\"weighted\")\n",
    "\n",
    "            end_time = time.time()\n",
    "            times.append(end_time - start_time)\n",
    "            trainscores.append(acc_train)\n",
    "\n",
    "            # Print the result\n",
    "            print(f'batch: {batch + 1:03d}, loss_train: {loss.item():.4f}, acc_train: {acc_train:.4f}, time: {end_time - start_time:.4f}s')\n",
    "\n",
    "            if batch >= 179:\n",
    "                break\n",
    "\n",
    "        # Perform validation after each epoch\n",
    "        print(f\"Validation after epoch {epoch}:\")\n",
    "        val_acc, val_loss, val_output = predict_(llm_graph_transformer, label, loss_fn, val, device, edge_feat)\n",
    "        print(f\"Validation set results: loss= {val_loss:.4f}, accuracy= {val_acc:.4f}, label acc= {f1_score(label_cpu[val], val_output, average=None)}\")\n",
    "        valscores.append(val_acc)\n",
    "\n",
    "    acc_test, loss_test, predict_output = predict_(llm_graph_transformer, label, loss_fn, test, device, edge_feat)\n",
    "    print(f\"Test set results: loss= {loss_test:.4f}, accuracy= {acc_test:.4f}, label acc= {f1_score(label_cpu[test], predict_output, average=None)}\")\n",
    "    save_model(llm_graph_transformer, optimizer, epoch)\n",
    "\n",
    "\n",
    "\n",
    "def save_model(model, optimizer, epoch, path=\"llm_w_edgefeat.pth\"):\n",
    "    # Get current time and format it\n",
    "    current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    \n",
    "    # Add the directory 'model/' and append the time to the path\n",
    "    path = f\"model/{current_time}_{path}\"\n",
    "    \n",
    "    # Create checkpoint to save model and optimizer state\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    \n",
    "    # Save the checkpoint\n",
    "    torch.save(checkpoint, path)\n",
    "    \n",
    "    # Print confirmation that the model has been saved\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def predict_(model, label, loss_fn, data_idx, device, edge_feat):\n",
    "    predict_output = []\n",
    "    loss = 0.0\n",
    "    num_batches = math.ceil(len(data_idx) / 10)\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "        batch_edges = data_idx[10 * batch:10 * (batch + 1)]\n",
    "        labels = ['Benign', 'BruteForce', 'DoS', 'DDoS', 'Web', 'Bot', 'Infilteration']\n",
    "\n",
    "        # Generate text from batch_edges\n",
    "        batch_text = model.generate_text(batch_edges, labels, max_new_tokens=10)\n",
    "        edge_batch = edge_feat[batch_edges]\n",
    "        # Get logits from the model (floating point values representing class probabilities)\n",
    "        logits = model(batch_text, edge_batch).to(device)  # Use the model to get logits\n",
    "\n",
    "        # Target labels\n",
    "        batch_labels = label[batch_edges].to(device)  # Long type labels for cross_entropy\n",
    "\n",
    "        # Compute the loss using logits (input) and batch_labels (target)\n",
    "        batch_loss = loss_fn(logits, batch_labels)\n",
    "        loss += batch_loss.item()\n",
    "\n",
    "        # Calculate predictions based on logits\n",
    "        predicted_labels = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        predict_output.extend(predicted_labels)\n",
    "\n",
    "    # Normalize loss by the number of batches\n",
    "    loss /= num_batches\n",
    "\n",
    "    # Calculate accuracy using F1 score\n",
    "    acc = f1_score(label.cpu().numpy()[data_idx], predict_output, average=\"weighted\")\n",
    "    return acc, loss, predict_output\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seeds(42) \n",
    "    fit({\n",
    "        \"dataset\": \"CSE-CIC\",\n",
    "        \"binary\": False,\n",
    "        \"residual\": True\n",
    "    })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
