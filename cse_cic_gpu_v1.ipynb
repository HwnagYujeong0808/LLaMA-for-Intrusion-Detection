{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  datasets.zip\n",
      "   creating: datasets/datasets/\n",
      "   creating: datasets/datasets/CSE-CIC/\n",
      "  inflating: datasets/datasets/CSE-CIC/adj.npy  \n",
      "  inflating: datasets/datasets/CSE-CIC/adj_random.npy  \n",
      "  inflating: datasets/datasets/CSE-CIC/adj_random_list.dict  \n",
      "  inflating: datasets/datasets/CSE-CIC/CIC.pkl  \n",
      "  inflating: datasets/datasets/CSE-CIC/edge_feat.npy  \n",
      "  inflating: datasets/datasets/CSE-CIC/edge_feat_.npy  \n",
      "  inflating: datasets/datasets/CSE-CIC/edge_feat_scaled.npy  \n",
      "  inflating: datasets/datasets/CSE-CIC/label_bi.npy  \n",
      "  inflating: datasets/datasets/CSE-CIC/label_mul.npy  \n",
      "  inflating: datasets/datasets/CSE-CIC/nodes.npy  \n",
      "  inflating: datasets/datasets/CSE-CIC/node_random.npy  \n",
      "  inflating: datasets/datasets/CSE-CIC/prepare_CIC.ipynb  \n",
      "   creating: datasets/datasets/Darknet/\n",
      "   creating: datasets/datasets/Darknet/.ipynb_checkpoints/\n",
      "  inflating: datasets/datasets/Darknet/.ipynb_checkpoints/Prepare_Darknet-checkpoint.ipynb  \n",
      "  inflating: datasets/datasets/Darknet/.ipynb_checkpoints/Prepare_Darknet-checkpoint.py  \n",
      "  inflating: datasets/datasets/Darknet/adj.npy  \n",
      "  inflating: datasets/datasets/Darknet/adj_random.npy  \n",
      "  inflating: datasets/datasets/Darknet/adj_random_list.dict  \n",
      "  inflating: datasets/datasets/Darknet/adj_rus.npy  \n",
      "  inflating: datasets/datasets/Darknet/adj_rus_list.dict  \n",
      "  inflating: datasets/datasets/Darknet/Darknet.CSV  \n",
      "  inflating: datasets/datasets/Darknet/dk.pkl  \n",
      "  inflating: datasets/datasets/Darknet/edge_feat.npy  \n",
      "  inflating: datasets/datasets/Darknet/edge_feat_rus.npy  \n",
      "  inflating: datasets/datasets/Darknet/edge_feat_scaled.npy  \n",
      "  inflating: datasets/datasets/Darknet/label_bi.npy  \n",
      "  inflating: datasets/datasets/Darknet/label_bi_rus.npy  \n",
      "  inflating: datasets/datasets/Darknet/label_mul.npy  \n",
      "  inflating: datasets/datasets/Darknet/label_mul_rus.npy  \n",
      "  inflating: datasets/datasets/Darknet/nodes.npy  \n",
      "  inflating: datasets/datasets/Darknet/node_random.npy  \n",
      "  inflating: datasets/datasets/Darknet/Prepare_Darknet.ipynb  \n",
      "  inflating: datasets/datasets/Darknet/Prepare_Darknet.py  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file_name = \"datasets.zip\"\n",
    "output_dir = \"datasets\"\n",
    "os.system(\"unzip \"+file_name+\" -d \"+output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/\" + \"CSE-CIC\"\n",
    "if not path.endswith('/'):\n",
    "    path += '/'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the data manually (edge_feat, label, adj, adj_lists, config)\n",
    "edge_feat = np.load(path + \"edge_feat_scaled.npy\", allow_pickle=True)\n",
    "edge_feat = torch.tensor(edge_feat, dtype=torch.float, device=device)\n",
    "\n",
    "# Load the label for multiclass classification\n",
    "label = np.load(path + \"label_mul.npy\", allow_pickle=True)\n",
    "label = torch.tensor(label, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 177566 occurrences\n",
      "Label 1: 18154 occurrences\n",
      "Label 2: 2468 occurrences\n",
      "Label 3: 32761 occurrences\n",
      "Label 4: 928 occurrences\n",
      "Label 5: 13475 occurrences\n",
      "Label 6: 5576 occurrences\n"
     ]
    }
   ],
   "source": [
    "# Get the unique values and their counts\n",
    "unique_labels, counts = label.unique(return_counts=True)\n",
    "\n",
    "# Print the counts for each unique label\n",
    "for label_value, count in zip(unique_labels.cpu().numpy(), counts.cpu().numpy()):\n",
    "    print(f\"Label {label_value}: {count} occurrences\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Learning | Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splits and labels loaded successfully.\n",
      "1360 152 168\n",
      "Label distribution in Train Set:\n",
      "{0: 195, 1: 195, 2: 194, 3: 194, 4: 194, 5: 194, 6: 194}\n",
      "Label distribution in Validation Set:\n",
      "{0: 21, 1: 21, 2: 22, 3: 22, 4: 22, 5: 22, 6: 22}\n",
      "Label distribution in Test Set:\n",
      "{0: 24, 1: 24, 2: 24, 3: 24, 4: 24, 5: 24, 6: 24}\n",
      "Epoch:  0\n",
      "Training data size: 1360\n",
      "Number of batches: 136\n",
      "batch: 001, loss_train: 2.7506, acc_train: 0.0000, time: 0.5284s\n",
      "batch: 002, loss_train: 1.9442, acc_train: 0.1733, time: 0.5213s\n",
      "batch: 003, loss_train: 1.3268, acc_train: 0.3000, time: 0.5223s\n",
      "batch: 004, loss_train: 2.4315, acc_train: 0.1333, time: 0.5232s\n",
      "batch: 005, loss_train: 2.9981, acc_train: 0.1000, time: 0.5230s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 349\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    348\u001b[0m     set_seeds(\u001b[38;5;241m42\u001b[39m) \n\u001b[0;32m--> 349\u001b[0m     \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCSE-CIC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbinary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresidual\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 250\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    247\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Convert batch_edges to text\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m batch_text \u001b[38;5;241m=\u001b[39m \u001b[43mllm_graph_transformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_edges\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# Generate logits from text inputs\u001b[39;00m\n\u001b[1;32m    253\u001b[0m edge_batch \u001b[38;5;241m=\u001b[39m edge_feat[batch_edges]\n",
      "Cell \u001b[0;32mIn[9], line 83\u001b[0m, in \u001b[0;36mLLMGraphTransformer.generate_text\u001b[0;34m(self, graph_data, labels, max_new_tokens)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Tokenize and generate predictions\u001b[39;00m\n\u001b[1;32m     82\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(batch_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 83\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     90\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generated_text\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3203\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3205\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3208\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3209\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3210\u001b[0m     outputs,\n\u001b[1;32m   3211\u001b[0m     model_kwargs,\n\u001b[1;32m   3212\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3213\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:945\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    934\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    935\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m         position_embeddings,\n\u001b[1;32m    943\u001b[0m     )\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:676\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    675\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 676\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    689\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:577\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m     cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[0;32m--> 577\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[1;32m    581\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position}\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:224\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[1;32m    222\u001b[0m cos \u001b[38;5;241m=\u001b[39m cos\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[1;32m    223\u001b[0m sin \u001b[38;5;241m=\u001b[39m sin\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[0;32m--> 224\u001b[0m q_embed \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (\u001b[43mrotate_half\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m sin)\n\u001b[1;32m    225\u001b[0m k_embed \u001b[38;5;241m=\u001b[39m (k \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (rotate_half(k) \u001b[38;5;241m*\u001b[39m sin)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_embed, k_embed\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:199\u001b[0m, in \u001b[0;36mrotate_half\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    197\u001b[0m x1 \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, : x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    198\u001b[0m x2 \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m :]\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pickle\n",
    "import math\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "class LLMGraphTransformer(nn.Module):\n",
    "    def __init__(self, model_name=\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\", device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # Load the tokenizer and model for TinyLlama\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(self.device)\n",
    "\n",
    "        # Ensure padding token is set for TinyLlama\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "        # New layers to process edge features and reduce text logits dimension\n",
    "        self.edge_fc = nn.Linear(77, 64).to(self.device)\n",
    "        self.edge_dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "        # Reduce text logits to match edge embedding dimensions\n",
    "        self.text_fc = nn.Linear(self.model.config.vocab_size, 64).to(self.device)\n",
    "        \n",
    "        # Final classification layer to output 7 classes\n",
    "        self.classifier = nn.Linear(128, 7).to(self.device)\n",
    "\n",
    "    def forward(self, batch_text, edge_features):\n",
    "        # Tokenize text\n",
    "        inputs = self.tokenizer(batch_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
    "        outputs = self.model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "        \n",
    "        # Get the logits for the last token in each sequence\n",
    "        text_logits = outputs.logits[:, -1, :]  # Shape is (batch_size, vocab_size)\n",
    "        text_emb = self.text_fc(text_logits)    # Reduce text logits to (batch_size, 64)\n",
    "\n",
    "        # Process edge features through a fully connected layer\n",
    "        edge_emb = self.edge_fc(edge_features)  # Shape (batch_size, 64)\n",
    "        edge_emb = self.edge_dropout(edge_emb)\n",
    "\n",
    "        # Concatenate the text logits and the edge feature embeddings\n",
    "        combined_logits = torch.cat((text_emb, edge_emb), dim=1)  # Shape (batch_size, 128)\n",
    "        \n",
    "        # Pass through final classifier layer to get 7-class output\n",
    "        final_logits = self.classifier(combined_logits)  # Shape (batch_size, 7)\n",
    "        \n",
    "        return final_logits\n",
    "\n",
    "    def generate_text(self, graph_data, labels, max_new_tokens=50):\n",
    "        # Convert the graph adjacency list to text directly within this method\n",
    "        batch_text = []\n",
    "        for node, neighbors in enumerate(graph_data):\n",
    "            if isinstance(neighbors, (list, set, np.ndarray)):\n",
    "                for neighbor in neighbors:\n",
    "                    question = f\"What is the relationship between Node {node} and Node {neighbor}? Choices: {', '.join(labels)}.\"\n",
    "                    batch_text.append(question)\n",
    "            else:\n",
    "                question = f\"What is the relationship between Node {node} and Node {neighbors}? Choices: {', '.join(labels)}.\"\n",
    "                batch_text.append(question)\n",
    "\n",
    "        # Tokenize and generate predictions\n",
    "        inputs = self.tokenizer(batch_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
    "        outputs = self.model.generate(\n",
    "            inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "        generated_text = [self.tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "        return generated_text\n",
    "\n",
    "\n",
    "def balance_data(data, labels, n_samples_per_label):\n",
    "    # Find unique labels and their counts\n",
    "    random.seed(42)\n",
    "    label_groups = {}\n",
    "    for label in np.unique(labels):\n",
    "        label_indices = np.where(labels == label)[0]\n",
    "        # If the label has fewer samples than the target, we use replace=True to oversample.\n",
    "        sampled_indices = np.random.choice(label_indices, size=n_samples_per_label, replace=(len(label_indices) < n_samples_per_label))\n",
    "        label_groups[label] = sampled_indices\n",
    "\n",
    "    # Concatenate the balanced data\n",
    "    balanced_indices = np.concatenate(list(label_groups.values()))\n",
    "    balanced_data = data[balanced_indices]\n",
    "    balanced_labels = labels[balanced_indices]\n",
    "\n",
    "    return balanced_data, balanced_labels\n",
    "\n",
    "\n",
    "def save_data_splits(train, val, test, train_labels, val_labels, test_labels, path=\"data_splits/cse-cic\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    with open(os.path.join(path, \"train.pkl\"), \"wb\") as f:\n",
    "        pickle.dump((train, train_labels), f)\n",
    "    with open(os.path.join(path, \"val.pkl\"), \"wb\") as f:\n",
    "        pickle.dump((val, val_labels), f)\n",
    "    with open(os.path.join(path, \"test.pkl\"), \"wb\") as f:\n",
    "        pickle.dump((test, test_labels), f)\n",
    "    print(\"Data splits and labels saved successfully.\")\n",
    "\n",
    "def load_data_splits(path=\"data_splits/cse-cic\"):\n",
    "    with open(os.path.join(path, \"train.pkl\"), \"rb\") as f:\n",
    "        train, train_labels = pickle.load(f)\n",
    "    with open(os.path.join(path, \"val.pkl\"), \"rb\") as f:\n",
    "        val, val_labels = pickle.load(f)\n",
    "    with open(os.path.join(path, \"test.pkl\"), \"rb\") as f:\n",
    "        test, test_labels = pickle.load(f)\n",
    "    print(\"Data splits and labels loaded successfully.\")\n",
    "    return train, val, test, train_labels, val_labels, test_labels\n",
    "\n",
    "def fit(args):\n",
    "    data = args[\"dataset\"]\n",
    "    binary = args[\"binary\"]\n",
    "\n",
    "    # Update the path to use ../cyber_gnn/ instead of datasets/\n",
    "    path = \"datasets/\" + data\n",
    "    if not path.endswith('/'):\n",
    "        path += '/'\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load the data manually (edge_feat, label, adj, adj_lists, config)\n",
    "    edge_feat = np.load(path + \"edge_feat_scaled.npy\", allow_pickle=True)\n",
    "    edge_feat = torch.tensor(edge_feat, dtype=torch.float, device=device)\n",
    "\n",
    "    # Load the label for multiclass classification\n",
    "    label = np.load(path + \"label_mul.npy\", allow_pickle=True)\n",
    "    label = torch.tensor(label, dtype=torch.long, device=device)\n",
    "    adj = np.load(path + \"adj_random.npy\", allow_pickle=True)\n",
    "    with open(path + 'adj_random_list.dict', 'rb') as file:\n",
    "        adj_lists = pickle.load(file)\n",
    "\n",
    "  \n",
    "\n",
    "    # Initialize LLMGraphTransformer using TinyLlama\n",
    "    llm_graph_transformer = LLMGraphTransformer(model_name=\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\", device=device)\n",
    "\n",
    "    # Define labels for relationship types\n",
    "    labels = ['Benign', 'BruteForce', 'DoS', 'DDoS', 'Web', 'Bot', 'Infilteration']\n",
    "    # Define the optimizer with Adam\n",
    "    optimizer = torch.optim.Adam(llm_graph_transformer.parameters(), lr=1e-5)\n",
    "    \n",
    "\n",
    "\n",
    "    num_edges = len(edge_feat)\n",
    "    label_cpu = label.cpu().numpy()\n",
    "    unique, counts = np.unique(label_cpu, return_counts=True)\n",
    "\n",
    "    balanced_data, balanced_labels = balance_data(np.arange(num_edges), label_cpu, n_samples_per_label=240)\n",
    "\n",
    "    # Check if saved splits exist, else create and save them\n",
    "    if not os.path.exists(\"data_splits/cse-cic/train.pkl\"):\n",
    "        # Perform initial train-validation-test split and save the splits\n",
    "        train_val, test, train_val_labels, test_labels = train_test_split(\n",
    "            balanced_data, balanced_labels, test_size=0.1, stratify=balanced_labels, random_state=42\n",
    "        )\n",
    "        train, val, train_labels, val_labels = train_test_split(\n",
    "            train_val, train_val_labels, test_size=0.1, stratify=train_val_labels, random_state=42\n",
    "        )\n",
    "        save_data_splits(train, val, test, train_labels, val_labels, test_labels)\n",
    "    else:\n",
    "        # Load the saved splits and their labels for consistent use\n",
    "        train, val, test, train_labels, val_labels, test_labels = load_data_splits()\n",
    "        # Assuming `train_labels` holds your training set labels\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    # Update CrossEntropyLoss with class weights\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    print(len(train), len(val), len(test))\n",
    "\n",
    "    # Print the distribution of labels for each set\n",
    "    print(\"Label distribution in Train Set:\")\n",
    "    unique_train, counts_train = np.unique(train_labels, return_counts=True)\n",
    "    print(dict(zip(unique_train, counts_train)))\n",
    "\n",
    "    print(\"Label distribution in Validation Set:\")\n",
    "    unique_val, counts_val = np.unique(val_labels, return_counts=True)\n",
    "    print(dict(zip(unique_val, counts_val)))\n",
    "\n",
    "    print(\"Label distribution in Test Set:\")\n",
    "    unique_test, counts_test = np.unique(test_labels, return_counts=True)\n",
    "    print(dict(zip(unique_test, counts_test)))\n",
    "\n",
    "    times = []\n",
    "    trainscores = []\n",
    "    valscores = []\n",
    "\n",
    "    for epoch in range(10):\n",
    "        print(\"Epoch: \", epoch)\n",
    "        random.shuffle(train)\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Print the number of batches\n",
    "        print(f\"Training data size: {len(train)}\")\n",
    "        print(f\"Number of batches: {len(train) // 10}\")\n",
    "        \n",
    "        for batch in range(int(len(train) / 10)):  # Batch size is 10\n",
    "            batch_edges = train[10 * batch:10 * (batch + 1)]\n",
    "            \n",
    "            if len(batch_edges) == 0:\n",
    "                print(f\"Skipping empty batch {batch + 1}\")\n",
    "                continue\n",
    "\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Convert batch_edges to text\n",
    "            batch_text = llm_graph_transformer.generate_text(batch_edges, labels, max_new_tokens=10)\n",
    "\n",
    "            # Generate logits from text inputs\n",
    "            edge_batch = edge_feat[batch_edges]\n",
    "            logits = llm_graph_transformer(batch_text, edge_batch)\n",
    "            \n",
    "            # Ensure logits and labels are both on the same device\n",
    "            logits = logits.to(device)\n",
    "            batch_labels = label[batch_edges].to(device)\n",
    "\n",
    "            # Calculate loss using logits and target labels\n",
    "            loss = loss_fn(logits, batch_labels)\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "            acc_train = f1_score(label_cpu[batch_edges], predicted_labels.cpu().numpy(), average=\"weighted\")\n",
    "\n",
    "            end_time = time.time()\n",
    "            times.append(end_time - start_time)\n",
    "            trainscores.append(acc_train)\n",
    "\n",
    "            # Print the result\n",
    "            print(f'batch: {batch + 1:03d}, loss_train: {loss.item():.4f}, acc_train: {acc_train:.4f}, time: {end_time - start_time:.4f}s')\n",
    "\n",
    "            if batch >= 179:\n",
    "                break\n",
    "\n",
    "        # Perform validation after each epoch\n",
    "        print(f\"Validation after epoch {epoch}:\")\n",
    "        val_acc, val_loss, val_output = predict_(llm_graph_transformer, label, loss_fn, val, device, edge_feat)\n",
    "        print(f\"Validation set results: loss= {val_loss:.4f}, accuracy= {val_acc:.4f}, label acc= {f1_score(label_cpu[val], val_output, average=None)}\")\n",
    "        valscores.append(val_acc)\n",
    "\n",
    "    acc_test, loss_test, predict_output = predict_(llm_graph_transformer, label, loss_fn, test, device, edge_feat)\n",
    "    print(f\"Test set results: loss= {loss_test:.4f}, accuracy= {acc_test:.4f}, label acc= {f1_score(label_cpu[test], predict_output, average=None)}\")\n",
    "    save_model(llm_graph_transformer, optimizer, epoch)\n",
    "\n",
    "\n",
    "\n",
    "def save_model(model, optimizer, epoch, path=\"llm_w_edgefeat.pth\"):\n",
    "    # Get current time and format it\n",
    "    current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    \n",
    "    # Add the directory 'model/' and append the time to the path\n",
    "    path = f\"model/{current_time}_{path}\"\n",
    "    \n",
    "    # Create checkpoint to save model and optimizer state\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    \n",
    "    # Save the checkpoint\n",
    "    torch.save(checkpoint, path)\n",
    "    \n",
    "    # Print confirmation that the model has been saved\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def predict_(model, label, loss_fn, data_idx, device, edge_feat):\n",
    "    predict_output = []\n",
    "    loss = 0.0\n",
    "    num_batches = math.ceil(len(data_idx) / 10)\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "        batch_edges = data_idx[10 * batch:10 * (batch + 1)]\n",
    "        labels = ['Benign', 'BruteForce', 'DoS', 'DDoS', 'Web', 'Bot', 'Infilteration']\n",
    "\n",
    "        # Generate text from batch_edges\n",
    "        batch_text = model.generate_text(batch_edges, labels, max_new_tokens=10)\n",
    "        edge_batch = edge_feat[batch_edges]\n",
    "        # Get logits from the model (floating point values representing class probabilities)\n",
    "        logits = model(batch_text, edge_batch).to(device)  # Use the model to get logits\n",
    "\n",
    "        # Target labels\n",
    "        batch_labels = label[batch_edges].to(device)  # Long type labels for cross_entropy\n",
    "\n",
    "        # Compute the loss using logits (input) and batch_labels (target)\n",
    "        batch_loss = loss_fn(logits, batch_labels)\n",
    "        loss += batch_loss.item()\n",
    "\n",
    "        # Calculate predictions based on logits\n",
    "        predicted_labels = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        predict_output.extend(predicted_labels)\n",
    "\n",
    "    # Normalize loss by the number of batches\n",
    "    loss /= num_batches\n",
    "\n",
    "    # Calculate accuracy using F1 score\n",
    "    acc = f1_score(label.cpu().numpy()[data_idx], predict_output, average=\"weighted\")\n",
    "    return acc, loss, predict_output\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seeds(42) \n",
    "    fit({\n",
    "        \"dataset\": \"CSE-CIC\",\n",
    "        \"binary\": False,\n",
    "        \"residual\": True\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Learning | Phase 2\n",
    "## Unused 120 Sampling + Pre-Fixed Seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_172663/860533329.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model/20241113-181010_llm_w_edgefeat.pth, epoch 9\n",
      "Data splits and labels loaded successfully.\n",
      "Unused Train Data: 1344, Validation: 168, Test: 168\n",
      "Label distribution in Unused Train Set: {0: 192, 1: 192, 2: 192, 3: 192, 4: 192, 5: 192, 6: 192}\n",
      "Label distribution in Unused Validation Set: {0: 24, 1: 24, 2: 24, 3: 24, 4: 24, 5: 24, 6: 24}\n",
      "Label distribution in Unused Test Set: {0: 24, 1: 24, 2: 24, 3: 24, 4: 24, 5: 24, 6: 24}\n",
      "Epoch: 0\n",
      "[Train] batch: 001, loss_train: 0.4009, acc_train: 0.7156\n",
      "[Train] batch: 002, loss_train: 0.6614, acc_train: 0.5267\n",
      "[Train] batch: 003, loss_train: 0.9544, acc_train: 0.7067\n",
      "[Train] batch: 004, loss_train: 0.0869, acc_train: 1.0000\n",
      "[Train] batch: 005, loss_train: 0.3054, acc_train: 0.8667\n",
      "[Train] batch: 006, loss_train: 0.0400, acc_train: 1.0000\n",
      "[Train] batch: 007, loss_train: 0.5929, acc_train: 0.8667\n",
      "[Train] batch: 008, loss_train: 0.3419, acc_train: 0.8600\n",
      "[Train] batch: 009, loss_train: 0.4143, acc_train: 0.8545\n",
      "[Train] batch: 010, loss_train: 0.4680, acc_train: 0.8571\n",
      "[Train] batch: 011, loss_train: 0.2490, acc_train: 0.8667\n",
      "[Train] batch: 012, loss_train: 0.3535, acc_train: 0.8571\n",
      "[Train] batch: 013, loss_train: 1.5870, acc_train: 0.4238\n",
      "[Train] batch: 014, loss_train: 0.7507, acc_train: 0.6600\n",
      "[Train] batch: 015, loss_train: 0.9678, acc_train: 0.5600\n",
      "[Train] batch: 016, loss_train: 0.5565, acc_train: 0.5000\n",
      "[Train] batch: 017, loss_train: 0.4387, acc_train: 0.7267\n",
      "[Train] batch: 018, loss_train: 0.8681, acc_train: 0.4600\n",
      "[Train] batch: 019, loss_train: 0.2730, acc_train: 0.9000\n",
      "[Train] batch: 020, loss_train: 0.3381, acc_train: 0.8000\n",
      "[Train] batch: 021, loss_train: 0.6839, acc_train: 0.7167\n",
      "[Train] batch: 022, loss_train: 0.6113, acc_train: 0.6971\n",
      "[Train] batch: 023, loss_train: 0.5566, acc_train: 0.5333\n",
      "[Train] batch: 024, loss_train: 0.5006, acc_train: 0.7905\n",
      "[Train] batch: 025, loss_train: 0.3604, acc_train: 0.8600\n",
      "[Train] batch: 026, loss_train: 0.6963, acc_train: 0.5167\n",
      "[Train] batch: 027, loss_train: 0.6007, acc_train: 0.6000\n",
      "[Train] batch: 028, loss_train: 0.2188, acc_train: 0.9333\n",
      "[Train] batch: 029, loss_train: 0.2587, acc_train: 0.8667\n",
      "[Train] batch: 030, loss_train: 0.3800, acc_train: 0.8333\n",
      "[Train] batch: 031, loss_train: 0.2200, acc_train: 0.8667\n",
      "[Train] batch: 032, loss_train: 0.5375, acc_train: 0.8000\n",
      "[Train] batch: 033, loss_train: 0.2517, acc_train: 0.8000\n",
      "[Train] batch: 034, loss_train: 0.3422, acc_train: 0.8000\n",
      "[Train] batch: 035, loss_train: 0.2573, acc_train: 0.8667\n",
      "[Train] batch: 036, loss_train: 0.0276, acc_train: 1.0000\n",
      "[Train] batch: 037, loss_train: 0.6732, acc_train: 0.8571\n",
      "[Train] batch: 038, loss_train: 0.5462, acc_train: 0.8600\n",
      "[Train] batch: 039, loss_train: 1.1211, acc_train: 0.6222\n",
      "[Train] batch: 040, loss_train: 0.1051, acc_train: 1.0000\n",
      "[Train] batch: 041, loss_train: 0.2624, acc_train: 0.8571\n",
      "[Train] batch: 042, loss_train: 0.5731, acc_train: 0.8667\n",
      "[Train] batch: 043, loss_train: 0.0615, acc_train: 1.0000\n",
      "[Train] batch: 044, loss_train: 0.5039, acc_train: 0.7600\n",
      "[Train] batch: 045, loss_train: 0.2500, acc_train: 0.9000\n",
      "[Train] batch: 046, loss_train: 0.0711, acc_train: 1.0000\n",
      "[Train] batch: 047, loss_train: 0.0830, acc_train: 1.0000\n",
      "[Train] batch: 048, loss_train: 0.3644, acc_train: 0.8600\n",
      "[Train] batch: 049, loss_train: 0.0772, acc_train: 1.0000\n",
      "[Train] batch: 050, loss_train: 0.4052, acc_train: 0.8571\n",
      "[Train] batch: 051, loss_train: 1.2791, acc_train: 0.5267\n",
      "[Train] batch: 052, loss_train: 0.8317, acc_train: 0.7400\n",
      "[Train] batch: 053, loss_train: 0.4033, acc_train: 0.8333\n",
      "[Train] batch: 054, loss_train: 0.5475, acc_train: 0.7667\n",
      "[Train] batch: 055, loss_train: 0.9832, acc_train: 0.6571\n",
      "[Train] batch: 056, loss_train: 0.3120, acc_train: 0.8571\n",
      "[Train] batch: 057, loss_train: 0.5851, acc_train: 0.5000\n",
      "[Train] batch: 058, loss_train: 0.2271, acc_train: 0.9333\n",
      "[Train] batch: 059, loss_train: 0.5893, acc_train: 0.5467\n",
      "[Train] batch: 060, loss_train: 0.6423, acc_train: 0.7667\n",
      "[Train] batch: 061, loss_train: 0.1892, acc_train: 1.0000\n",
      "[Train] batch: 062, loss_train: 0.9292, acc_train: 0.7619\n",
      "[Train] batch: 063, loss_train: 0.2801, acc_train: 1.0000\n",
      "[Train] batch: 064, loss_train: 0.4544, acc_train: 0.8571\n",
      "[Train] batch: 065, loss_train: 0.4294, acc_train: 0.9067\n",
      "[Train] batch: 066, loss_train: 0.5010, acc_train: 0.7333\n",
      "[Train] batch: 067, loss_train: 0.1379, acc_train: 1.0000\n",
      "[Train] batch: 068, loss_train: 0.5523, acc_train: 0.7333\n",
      "[Train] batch: 069, loss_train: 0.8046, acc_train: 0.7571\n",
      "[Train] batch: 070, loss_train: 0.5591, acc_train: 0.7600\n",
      "[Train] batch: 071, loss_train: 0.1982, acc_train: 1.0000\n",
      "[Train] batch: 072, loss_train: 1.2466, acc_train: 0.5467\n",
      "[Train] batch: 073, loss_train: 0.5806, acc_train: 0.8667\n",
      "[Train] batch: 074, loss_train: 0.5173, acc_train: 0.9000\n",
      "[Train] batch: 075, loss_train: 0.3684, acc_train: 0.8667\n",
      "[Train] batch: 076, loss_train: 0.7140, acc_train: 0.6500\n",
      "[Train] batch: 077, loss_train: 0.5472, acc_train: 0.5917\n",
      "[Train] batch: 078, loss_train: 0.3506, acc_train: 0.8667\n",
      "[Train] batch: 079, loss_train: 0.5256, acc_train: 0.6167\n",
      "[Train] batch: 080, loss_train: 0.1422, acc_train: 1.0000\n",
      "[Train] batch: 081, loss_train: 0.5936, acc_train: 0.6933\n",
      "[Train] batch: 082, loss_train: 0.4733, acc_train: 0.8000\n",
      "[Train] batch: 083, loss_train: 0.8370, acc_train: 0.4733\n",
      "[Train] batch: 084, loss_train: 0.2792, acc_train: 0.8667\n",
      "[Train] batch: 085, loss_train: 0.3453, acc_train: 0.7000\n",
      "[Train] batch: 086, loss_train: 0.4562, acc_train: 0.8000\n",
      "[Train] batch: 087, loss_train: 0.3123, acc_train: 0.8000\n",
      "[Train] batch: 088, loss_train: 0.3836, acc_train: 1.0000\n",
      "[Train] batch: 089, loss_train: 0.2421, acc_train: 1.0000\n",
      "[Train] batch: 090, loss_train: 0.4596, acc_train: 0.8600\n",
      "[Train] batch: 091, loss_train: 0.6010, acc_train: 0.7333\n",
      "[Train] batch: 092, loss_train: 0.2831, acc_train: 0.8600\n",
      "[Train] batch: 093, loss_train: 0.0919, acc_train: 1.0000\n",
      "[Train] batch: 094, loss_train: 0.0504, acc_train: 1.0000\n",
      "[Train] batch: 095, loss_train: 1.0622, acc_train: 0.6500\n",
      "[Train] batch: 096, loss_train: 0.5811, acc_train: 0.7238\n",
      "[Train] batch: 097, loss_train: 0.0765, acc_train: 1.0000\n",
      "[Train] batch: 098, loss_train: 0.6346, acc_train: 0.7267\n",
      "[Train] batch: 099, loss_train: 0.6746, acc_train: 0.7333\n",
      "[Train] batch: 100, loss_train: 0.9770, acc_train: 0.6250\n",
      "[Train] batch: 101, loss_train: 0.1267, acc_train: 1.0000\n",
      "[Train] batch: 102, loss_train: 0.0799, acc_train: 1.0000\n",
      "[Train] batch: 103, loss_train: 0.9325, acc_train: 0.5267\n",
      "[Train] batch: 104, loss_train: 1.6695, acc_train: 0.1667\n",
      "[Train] batch: 105, loss_train: 0.9021, acc_train: 0.6533\n",
      "[Train] batch: 106, loss_train: 0.9580, acc_train: 0.6000\n",
      "[Train] batch: 107, loss_train: 0.2454, acc_train: 1.0000\n",
      "[Train] batch: 108, loss_train: 0.4808, acc_train: 0.9000\n",
      "[Train] batch: 109, loss_train: 0.5783, acc_train: 0.8000\n",
      "[Train] batch: 110, loss_train: 0.3678, acc_train: 0.8667\n",
      "[Train] batch: 111, loss_train: 0.3317, acc_train: 0.9000\n",
      "[Train] batch: 112, loss_train: 0.5699, acc_train: 0.4600\n",
      "[Train] batch: 113, loss_train: 0.7242, acc_train: 0.7333\n",
      "[Train] batch: 114, loss_train: 0.4114, acc_train: 0.8571\n",
      "[Train] batch: 115, loss_train: 0.3574, acc_train: 0.8000\n",
      "[Train] batch: 116, loss_train: 0.6341, acc_train: 0.8000\n",
      "[Train] batch: 117, loss_train: 0.7147, acc_train: 0.8100\n",
      "[Train] batch: 118, loss_train: 0.2496, acc_train: 1.0000\n",
      "[Train] batch: 119, loss_train: 0.0732, acc_train: 1.0000\n",
      "[Train] batch: 120, loss_train: 0.7256, acc_train: 0.6333\n",
      "[Train] batch: 121, loss_train: 0.4267, acc_train: 0.8571\n",
      "[Train] batch: 122, loss_train: 0.4087, acc_train: 0.8667\n",
      "[Train] batch: 123, loss_train: 0.3558, acc_train: 0.7500\n",
      "[Train] batch: 124, loss_train: 0.6533, acc_train: 0.6167\n",
      "[Train] batch: 125, loss_train: 0.4735, acc_train: 0.8600\n",
      "[Train] batch: 126, loss_train: 0.2691, acc_train: 0.8667\n",
      "[Train] batch: 127, loss_train: 0.6028, acc_train: 0.7667\n",
      "[Train] batch: 128, loss_train: 0.4158, acc_train: 0.6714\n",
      "[Train] batch: 129, loss_train: 0.3082, acc_train: 0.7500\n",
      "[Train] batch: 130, loss_train: 0.1616, acc_train: 0.9000\n",
      "[Train] batch: 131, loss_train: 0.8529, acc_train: 0.6929\n",
      "[Train] batch: 132, loss_train: 0.3038, acc_train: 0.7667\n",
      "[Train] batch: 133, loss_train: 0.1076, acc_train: 1.0000\n",
      "[Train] batch: 134, loss_train: 0.3405, acc_train: 0.9000\n",
      "[Val] loss= 0.3914, accuracy= 0.7927, label acc= [0.         0.94117647 0.88888889 0.92307692 1.         0.92307692\n",
      " 0.87272727]\n",
      "Epoch: 1\n",
      "[Train] batch: 001, loss_train: 0.3304, acc_train: 0.9000\n",
      "[Train] batch: 002, loss_train: 0.3496, acc_train: 0.7600\n",
      "[Train] batch: 003, loss_train: 0.8228, acc_train: 0.7333\n",
      "[Train] batch: 004, loss_train: 0.3851, acc_train: 0.7500\n",
      "[Train] batch: 005, loss_train: 0.5672, acc_train: 0.8333\n",
      "[Train] batch: 006, loss_train: 0.2290, acc_train: 0.8667\n",
      "[Train] batch: 007, loss_train: 0.8390, acc_train: 0.6333\n",
      "[Train] batch: 008, loss_train: 0.3711, acc_train: 0.7333\n",
      "[Train] batch: 009, loss_train: 0.5403, acc_train: 0.8667\n",
      "[Train] batch: 010, loss_train: 0.1113, acc_train: 1.0000\n",
      "[Train] batch: 011, loss_train: 0.1861, acc_train: 0.9000\n",
      "[Train] batch: 012, loss_train: 0.4189, acc_train: 0.9000\n",
      "[Train] batch: 013, loss_train: 0.9604, acc_train: 0.4833\n",
      "[Train] batch: 014, loss_train: 0.4027, acc_train: 0.7667\n",
      "[Train] batch: 015, loss_train: 0.4157, acc_train: 0.9000\n",
      "[Train] batch: 016, loss_train: 0.2381, acc_train: 0.8600\n",
      "[Train] batch: 017, loss_train: 0.3147, acc_train: 0.8600\n",
      "[Train] batch: 018, loss_train: 0.6550, acc_train: 0.8000\n",
      "[Train] batch: 019, loss_train: 0.4146, acc_train: 0.8000\n",
      "[Train] batch: 020, loss_train: 0.7663, acc_train: 0.5667\n",
      "[Train] batch: 021, loss_train: 0.7118, acc_train: 0.5000\n",
      "[Train] batch: 022, loss_train: 0.4801, acc_train: 0.8667\n",
      "[Train] batch: 023, loss_train: 0.2798, acc_train: 1.0000\n",
      "[Train] batch: 024, loss_train: 0.6606, acc_train: 0.6000\n",
      "[Train] batch: 025, loss_train: 0.4373, acc_train: 0.7571\n",
      "[Train] batch: 026, loss_train: 0.4445, acc_train: 0.8000\n",
      "[Train] batch: 027, loss_train: 0.5412, acc_train: 0.7333\n",
      "[Train] batch: 028, loss_train: 0.3885, acc_train: 0.7571\n",
      "[Train] batch: 029, loss_train: 0.1796, acc_train: 1.0000\n",
      "[Train] batch: 030, loss_train: 0.2924, acc_train: 1.0000\n",
      "[Train] batch: 031, loss_train: 0.3279, acc_train: 0.9333\n",
      "[Train] batch: 032, loss_train: 0.5138, acc_train: 0.6933\n",
      "[Train] batch: 033, loss_train: 0.8588, acc_train: 0.6600\n",
      "[Train] batch: 034, loss_train: 0.2096, acc_train: 0.8600\n",
      "[Train] batch: 035, loss_train: 1.0986, acc_train: 0.6167\n",
      "[Train] batch: 036, loss_train: 0.7492, acc_train: 0.8000\n",
      "[Train] batch: 037, loss_train: 0.4545, acc_train: 0.6571\n",
      "[Train] batch: 038, loss_train: 0.2671, acc_train: 0.8600\n",
      "[Train] batch: 039, loss_train: 0.6921, acc_train: 0.6556\n",
      "[Train] batch: 040, loss_train: 0.6010, acc_train: 0.5889\n",
      "[Train] batch: 041, loss_train: 0.4753, acc_train: 0.7600\n",
      "[Train] batch: 042, loss_train: 0.4622, acc_train: 0.7600\n",
      "[Train] batch: 043, loss_train: 0.3613, acc_train: 0.8067\n",
      "[Train] batch: 044, loss_train: 0.2170, acc_train: 1.0000\n",
      "[Train] batch: 045, loss_train: 0.5560, acc_train: 0.7333\n",
      "[Train] batch: 046, loss_train: 0.3189, acc_train: 1.0000\n",
      "[Train] batch: 047, loss_train: 0.2246, acc_train: 1.0000\n",
      "[Train] batch: 048, loss_train: 0.2020, acc_train: 0.9000\n",
      "[Train] batch: 049, loss_train: 0.4559, acc_train: 0.8933\n",
      "[Train] batch: 050, loss_train: 0.0989, acc_train: 1.0000\n",
      "[Train] batch: 051, loss_train: 0.0680, acc_train: 1.0000\n",
      "[Train] batch: 052, loss_train: 0.8301, acc_train: 0.6333\n",
      "[Train] batch: 053, loss_train: 0.2464, acc_train: 0.9000\n",
      "[Train] batch: 054, loss_train: 0.6819, acc_train: 0.7238\n",
      "[Train] batch: 055, loss_train: 0.4528, acc_train: 0.9000\n",
      "[Train] batch: 056, loss_train: 0.2370, acc_train: 0.9000\n",
      "[Train] batch: 057, loss_train: 0.5215, acc_train: 0.7200\n",
      "[Train] batch: 058, loss_train: 0.0611, acc_train: 1.0000\n",
      "[Train] batch: 059, loss_train: 0.5144, acc_train: 0.5933\n",
      "[Train] batch: 060, loss_train: 0.2450, acc_train: 0.8600\n",
      "[Train] batch: 061, loss_train: 0.7872, acc_train: 0.7143\n",
      "[Train] batch: 062, loss_train: 0.3680, acc_train: 0.8667\n",
      "[Train] batch: 063, loss_train: 0.4230, acc_train: 0.8667\n",
      "[Train] batch: 064, loss_train: 1.0417, acc_train: 0.7500\n",
      "[Train] batch: 065, loss_train: 0.4600, acc_train: 0.7600\n",
      "[Train] batch: 066, loss_train: 0.3177, acc_train: 0.8538\n",
      "[Train] batch: 067, loss_train: 0.7400, acc_train: 0.7333\n",
      "[Train] batch: 068, loss_train: 0.6611, acc_train: 0.5905\n",
      "[Train] batch: 069, loss_train: 0.2746, acc_train: 0.8667\n",
      "[Train] batch: 070, loss_train: 0.5638, acc_train: 0.7267\n",
      "[Train] batch: 071, loss_train: 0.1769, acc_train: 1.0000\n",
      "[Train] batch: 072, loss_train: 0.3355, acc_train: 0.8571\n",
      "[Train] batch: 073, loss_train: 0.6936, acc_train: 0.7067\n",
      "[Train] batch: 074, loss_train: 0.3452, acc_train: 0.8667\n",
      "[Train] batch: 075, loss_train: 0.5103, acc_train: 0.6667\n",
      "[Train] batch: 076, loss_train: 0.1690, acc_train: 1.0000\n",
      "[Train] batch: 077, loss_train: 0.2919, acc_train: 0.8600\n",
      "[Train] batch: 078, loss_train: 0.2683, acc_train: 0.9000\n",
      "[Train] batch: 079, loss_train: 0.4471, acc_train: 0.8600\n",
      "[Train] batch: 080, loss_train: 0.3087, acc_train: 0.7571\n",
      "[Train] batch: 081, loss_train: 0.2340, acc_train: 1.0000\n",
      "[Train] batch: 082, loss_train: 0.4492, acc_train: 0.8600\n",
      "[Train] batch: 083, loss_train: 0.1645, acc_train: 0.8667\n",
      "[Train] batch: 084, loss_train: 0.1059, acc_train: 1.0000\n",
      "[Train] batch: 085, loss_train: 0.2828, acc_train: 0.8571\n",
      "[Train] batch: 086, loss_train: 0.7931, acc_train: 0.7200\n",
      "[Train] batch: 087, loss_train: 0.4309, acc_train: 0.8667\n",
      "[Train] batch: 088, loss_train: 0.3701, acc_train: 0.8571\n",
      "[Train] batch: 089, loss_train: 0.9297, acc_train: 0.5000\n",
      "[Train] batch: 090, loss_train: 0.6298, acc_train: 0.7200\n",
      "[Train] batch: 091, loss_train: 0.0764, acc_train: 1.0000\n",
      "[Train] batch: 092, loss_train: 0.6581, acc_train: 0.7267\n",
      "[Train] batch: 093, loss_train: 0.2697, acc_train: 0.8556\n",
      "[Train] batch: 094, loss_train: 0.1381, acc_train: 1.0000\n",
      "[Train] batch: 095, loss_train: 0.5706, acc_train: 0.7200\n",
      "[Train] batch: 096, loss_train: 0.2670, acc_train: 0.9000\n",
      "[Train] batch: 097, loss_train: 0.4687, acc_train: 0.8000\n",
      "[Train] batch: 098, loss_train: 0.7689, acc_train: 0.6071\n",
      "[Train] batch: 099, loss_train: 0.1799, acc_train: 1.0000\n",
      "[Train] batch: 100, loss_train: 0.3499, acc_train: 0.8667\n",
      "[Train] batch: 101, loss_train: 0.8235, acc_train: 0.6171\n",
      "[Train] batch: 102, loss_train: 0.2347, acc_train: 0.9333\n",
      "[Train] batch: 103, loss_train: 0.0998, acc_train: 1.0000\n",
      "[Train] batch: 104, loss_train: 0.4987, acc_train: 0.8000\n",
      "[Train] batch: 105, loss_train: 0.2677, acc_train: 0.9000\n",
      "[Train] batch: 106, loss_train: 0.5524, acc_train: 0.5333\n",
      "[Train] batch: 107, loss_train: 0.5046, acc_train: 0.6571\n",
      "[Train] batch: 108, loss_train: 0.6382, acc_train: 0.8400\n",
      "[Train] batch: 109, loss_train: 0.0947, acc_train: 1.0000\n",
      "[Train] batch: 110, loss_train: 0.2844, acc_train: 0.9000\n",
      "[Train] batch: 111, loss_train: 0.0690, acc_train: 1.0000\n",
      "[Train] batch: 112, loss_train: 0.4296, acc_train: 0.8600\n",
      "[Train] batch: 113, loss_train: 0.3368, acc_train: 0.8600\n",
      "[Train] batch: 114, loss_train: 0.0801, acc_train: 1.0000\n",
      "[Train] batch: 115, loss_train: 0.6757, acc_train: 0.7333\n",
      "[Train] batch: 116, loss_train: 0.0265, acc_train: 1.0000\n",
      "[Train] batch: 117, loss_train: 0.2425, acc_train: 0.8600\n",
      "[Train] batch: 118, loss_train: 0.6806, acc_train: 0.7267\n",
      "[Train] batch: 119, loss_train: 0.4743, acc_train: 0.9000\n",
      "[Train] batch: 120, loss_train: 0.4708, acc_train: 0.8600\n",
      "[Train] batch: 121, loss_train: 0.3465, acc_train: 0.8667\n",
      "[Train] batch: 122, loss_train: 0.6489, acc_train: 0.7333\n",
      "[Train] batch: 123, loss_train: 0.3243, acc_train: 0.8667\n",
      "[Train] batch: 124, loss_train: 0.6871, acc_train: 0.8000\n",
      "[Train] batch: 125, loss_train: 0.7250, acc_train: 0.7500\n",
      "[Train] batch: 126, loss_train: 0.1072, acc_train: 1.0000\n",
      "[Train] batch: 127, loss_train: 0.1191, acc_train: 1.0000\n",
      "[Train] batch: 128, loss_train: 0.3168, acc_train: 0.8667\n",
      "[Train] batch: 129, loss_train: 0.6672, acc_train: 0.7667\n",
      "[Train] batch: 130, loss_train: 0.3974, acc_train: 0.7667\n",
      "[Train] batch: 131, loss_train: 0.4264, acc_train: 0.7238\n",
      "[Train] batch: 132, loss_train: 0.4921, acc_train: 0.7267\n",
      "[Train] batch: 133, loss_train: 0.3470, acc_train: 1.0000\n",
      "[Train] batch: 134, loss_train: 0.6410, acc_train: 0.6600\n",
      "[Val] loss= 0.4180, accuracy= 0.7897, label acc= [0.         0.92       0.88888889 0.92307692 1.         0.92307692\n",
      " 0.87272727]\n",
      "Epoch: 2\n",
      "[Train] batch: 001, loss_train: 0.2072, acc_train: 1.0000\n",
      "[Train] batch: 002, loss_train: 0.3312, acc_train: 0.8600\n",
      "[Train] batch: 003, loss_train: 0.4942, acc_train: 0.7600\n",
      "[Train] batch: 004, loss_train: 0.5435, acc_train: 0.6000\n",
      "[Train] batch: 005, loss_train: 0.5746, acc_train: 0.6000\n",
      "[Train] batch: 006, loss_train: 0.3617, acc_train: 0.8600\n",
      "[Train] batch: 007, loss_train: 0.3648, acc_train: 0.9400\n",
      "[Train] batch: 008, loss_train: 0.4175, acc_train: 0.8600\n",
      "[Train] batch: 009, loss_train: 0.4432, acc_train: 0.9000\n",
      "[Train] batch: 010, loss_train: 0.4325, acc_train: 0.7333\n",
      "[Train] batch: 011, loss_train: 0.1845, acc_train: 0.8667\n",
      "[Train] batch: 012, loss_train: 0.2736, acc_train: 0.8600\n",
      "[Train] batch: 013, loss_train: 0.1994, acc_train: 0.9333\n",
      "[Train] batch: 014, loss_train: 0.2302, acc_train: 0.8667\n",
      "[Train] batch: 015, loss_train: 0.3013, acc_train: 0.8600\n",
      "[Train] batch: 016, loss_train: 0.1061, acc_train: 1.0000\n",
      "[Train] batch: 017, loss_train: 0.7616, acc_train: 0.6500\n",
      "[Train] batch: 018, loss_train: 0.6855, acc_train: 0.7333\n",
      "[Train] batch: 019, loss_train: 0.5543, acc_train: 0.9000\n",
      "[Train] batch: 020, loss_train: 0.0645, acc_train: 1.0000\n",
      "[Train] batch: 021, loss_train: 0.2851, acc_train: 0.8667\n",
      "[Train] batch: 022, loss_train: 0.9176, acc_train: 0.4838\n",
      "[Train] batch: 023, loss_train: 0.1735, acc_train: 1.0000\n",
      "[Train] batch: 024, loss_train: 0.0855, acc_train: 1.0000\n",
      "[Train] batch: 025, loss_train: 0.0751, acc_train: 1.0000\n",
      "[Train] batch: 026, loss_train: 0.3527, acc_train: 0.8667\n",
      "[Train] batch: 027, loss_train: 0.3860, acc_train: 0.8600\n",
      "[Train] batch: 028, loss_train: 0.2197, acc_train: 0.9000\n",
      "[Train] batch: 029, loss_train: 0.4771, acc_train: 0.6571\n",
      "[Train] batch: 030, loss_train: 0.9510, acc_train: 0.4533\n",
      "[Train] batch: 031, loss_train: 0.9940, acc_train: 0.6500\n",
      "[Train] batch: 032, loss_train: 0.4453, acc_train: 0.7500\n",
      "[Train] batch: 033, loss_train: 0.3938, acc_train: 0.7556\n",
      "[Train] batch: 034, loss_train: 0.4471, acc_train: 0.8571\n",
      "[Train] batch: 035, loss_train: 0.6070, acc_train: 0.6167\n",
      "[Train] batch: 036, loss_train: 0.4938, acc_train: 0.8333\n",
      "[Train] batch: 037, loss_train: 0.4386, acc_train: 0.9000\n",
      "[Train] batch: 038, loss_train: 0.2593, acc_train: 0.8667\n",
      "[Train] batch: 039, loss_train: 0.2666, acc_train: 1.0000\n",
      "[Train] batch: 040, loss_train: 0.4025, acc_train: 0.7400\n",
      "[Train] batch: 041, loss_train: 0.5454, acc_train: 0.6667\n",
      "[Train] batch: 042, loss_train: 0.5116, acc_train: 0.5933\n",
      "[Train] batch: 043, loss_train: 0.3248, acc_train: 0.8600\n",
      "[Train] batch: 044, loss_train: 0.2432, acc_train: 0.8556\n",
      "[Train] batch: 045, loss_train: 0.9071, acc_train: 0.5333\n",
      "[Train] batch: 046, loss_train: 0.3469, acc_train: 0.7238\n",
      "[Train] batch: 047, loss_train: 0.3101, acc_train: 0.8667\n",
      "[Train] batch: 048, loss_train: 0.1503, acc_train: 1.0000\n",
      "[Train] batch: 049, loss_train: 0.1943, acc_train: 1.0000\n",
      "[Train] batch: 050, loss_train: 0.3304, acc_train: 0.8600\n",
      "[Train] batch: 051, loss_train: 0.3134, acc_train: 0.8667\n",
      "[Train] batch: 052, loss_train: 0.2690, acc_train: 0.8571\n",
      "[Train] batch: 053, loss_train: 0.5241, acc_train: 0.9000\n",
      "[Train] batch: 054, loss_train: 0.1343, acc_train: 1.0000\n",
      "[Train] batch: 055, loss_train: 0.2908, acc_train: 0.8667\n",
      "[Train] batch: 056, loss_train: 0.4245, acc_train: 0.7333\n",
      "[Train] batch: 057, loss_train: 0.7476, acc_train: 0.6571\n",
      "[Train] batch: 058, loss_train: 0.4690, acc_train: 0.7200\n",
      "[Train] batch: 059, loss_train: 0.0758, acc_train: 1.0000\n",
      "[Train] batch: 060, loss_train: 0.3383, acc_train: 0.8667\n",
      "[Train] batch: 061, loss_train: 0.2500, acc_train: 0.8556\n",
      "[Train] batch: 062, loss_train: 0.6630, acc_train: 0.7000\n",
      "[Train] batch: 063, loss_train: 0.1089, acc_train: 1.0000\n",
      "[Train] batch: 064, loss_train: 0.1707, acc_train: 1.0000\n",
      "[Train] batch: 065, loss_train: 0.1793, acc_train: 1.0000\n",
      "[Train] batch: 066, loss_train: 0.2434, acc_train: 0.8600\n",
      "[Train] batch: 067, loss_train: 0.6429, acc_train: 0.6600\n",
      "[Train] batch: 068, loss_train: 0.4340, acc_train: 0.7571\n",
      "[Train] batch: 069, loss_train: 0.0599, acc_train: 1.0000\n",
      "[Train] batch: 070, loss_train: 0.5892, acc_train: 0.6600\n",
      "[Train] batch: 071, loss_train: 0.2963, acc_train: 0.8000\n",
      "[Train] batch: 072, loss_train: 0.5713, acc_train: 0.8600\n",
      "[Train] batch: 073, loss_train: 0.8854, acc_train: 0.6467\n",
      "[Train] batch: 074, loss_train: 0.4914, acc_train: 0.6600\n",
      "[Train] batch: 075, loss_train: 0.2798, acc_train: 0.8933\n",
      "[Train] batch: 076, loss_train: 0.2588, acc_train: 0.8571\n",
      "[Train] batch: 077, loss_train: 0.6800, acc_train: 0.5200\n",
      "[Train] batch: 078, loss_train: 0.2592, acc_train: 0.9000\n",
      "[Train] batch: 079, loss_train: 0.4834, acc_train: 0.7267\n",
      "[Train] batch: 080, loss_train: 0.6927, acc_train: 0.6333\n",
      "[Train] batch: 081, loss_train: 0.3661, acc_train: 0.8667\n",
      "[Train] batch: 082, loss_train: 0.3299, acc_train: 0.8667\n",
      "[Train] batch: 083, loss_train: 0.3564, acc_train: 0.8667\n",
      "[Train] batch: 084, loss_train: 0.5067, acc_train: 0.8000\n",
      "[Train] batch: 085, loss_train: 0.1370, acc_train: 1.0000\n",
      "[Train] batch: 086, loss_train: 0.4935, acc_train: 0.7667\n",
      "[Train] batch: 087, loss_train: 0.3075, acc_train: 0.8667\n",
      "[Train] batch: 088, loss_train: 0.1749, acc_train: 1.0000\n",
      "[Train] batch: 089, loss_train: 0.4580, acc_train: 0.7571\n",
      "[Train] batch: 090, loss_train: 0.2818, acc_train: 0.8667\n",
      "[Train] batch: 091, loss_train: 0.2800, acc_train: 0.8667\n",
      "[Train] batch: 092, loss_train: 0.3559, acc_train: 0.9000\n",
      "[Train] batch: 093, loss_train: 0.6068, acc_train: 0.5867\n",
      "[Train] batch: 094, loss_train: 0.4227, acc_train: 0.7250\n",
      "[Train] batch: 095, loss_train: 0.1731, acc_train: 1.0000\n",
      "[Train] batch: 096, loss_train: 0.4030, acc_train: 0.7267\n",
      "[Train] batch: 097, loss_train: 0.1823, acc_train: 0.8667\n",
      "[Train] batch: 098, loss_train: 0.7308, acc_train: 0.6556\n",
      "[Train] batch: 099, loss_train: 0.2472, acc_train: 0.8667\n",
      "[Train] batch: 100, loss_train: 0.2382, acc_train: 0.8667\n",
      "[Train] batch: 101, loss_train: 0.2397, acc_train: 1.0000\n",
      "[Train] batch: 102, loss_train: 0.9431, acc_train: 0.6071\n",
      "[Train] batch: 103, loss_train: 0.2381, acc_train: 0.9000\n",
      "[Train] batch: 104, loss_train: 0.2481, acc_train: 0.8600\n",
      "[Train] batch: 105, loss_train: 0.4538, acc_train: 0.8933\n",
      "[Train] batch: 106, loss_train: 0.1979, acc_train: 1.0000\n",
      "[Train] batch: 107, loss_train: 0.2397, acc_train: 0.8500\n",
      "[Train] batch: 108, loss_train: 0.3131, acc_train: 0.8667\n",
      "[Train] batch: 109, loss_train: 0.1702, acc_train: 1.0000\n",
      "[Train] batch: 110, loss_train: 0.2561, acc_train: 0.9000\n",
      "[Train] batch: 111, loss_train: 0.4065, acc_train: 0.8667\n",
      "[Train] batch: 112, loss_train: 0.2836, acc_train: 0.8600\n",
      "[Train] batch: 113, loss_train: 0.5098, acc_train: 0.7333\n",
      "[Train] batch: 114, loss_train: 0.4795, acc_train: 0.6267\n",
      "[Train] batch: 115, loss_train: 0.4229, acc_train: 0.7600\n",
      "[Train] batch: 116, loss_train: 0.3138, acc_train: 0.8545\n",
      "[Train] batch: 117, loss_train: 1.0137, acc_train: 0.6200\n",
      "[Train] batch: 118, loss_train: 0.2770, acc_train: 0.8600\n",
      "[Train] batch: 119, loss_train: 0.3618, acc_train: 0.6071\n",
      "[Train] batch: 120, loss_train: 1.2980, acc_train: 0.6933\n",
      "[Train] batch: 121, loss_train: 1.0121, acc_train: 0.6833\n",
      "[Train] batch: 122, loss_train: 0.2529, acc_train: 0.9400\n",
      "[Train] batch: 123, loss_train: 0.9697, acc_train: 0.4433\n",
      "[Train] batch: 124, loss_train: 0.4812, acc_train: 0.6667\n",
      "[Train] batch: 125, loss_train: 0.7181, acc_train: 0.8067\n",
      "[Train] batch: 126, loss_train: 0.3772, acc_train: 0.8000\n",
      "[Train] batch: 127, loss_train: 0.3187, acc_train: 0.8667\n",
      "[Train] batch: 128, loss_train: 0.5682, acc_train: 0.6881\n",
      "[Train] batch: 129, loss_train: 0.2492, acc_train: 0.9000\n",
      "[Train] batch: 130, loss_train: 0.3323, acc_train: 0.8933\n",
      "[Train] batch: 131, loss_train: 0.7168, acc_train: 0.5600\n",
      "[Train] batch: 132, loss_train: 0.6437, acc_train: 0.5545\n",
      "[Train] batch: 133, loss_train: 0.3836, acc_train: 0.8667\n",
      "[Train] batch: 134, loss_train: 0.3852, acc_train: 0.8667\n",
      "[Val] loss= 0.3782, accuracy= 0.7927, label acc= [0.         0.94117647 0.88888889 0.92307692 1.         0.92307692\n",
      " 0.87272727]\n",
      "Epoch: 3\n",
      "[Train] batch: 001, loss_train: 0.4815, acc_train: 0.7267\n",
      "[Train] batch: 002, loss_train: 0.3284, acc_train: 0.8600\n",
      "[Train] batch: 003, loss_train: 0.6512, acc_train: 0.7333\n",
      "[Train] batch: 004, loss_train: 0.6747, acc_train: 0.5333\n",
      "[Train] batch: 005, loss_train: 0.3932, acc_train: 0.8600\n",
      "[Train] batch: 006, loss_train: 0.3418, acc_train: 0.8667\n",
      "[Train] batch: 007, loss_train: 0.4867, acc_train: 0.8600\n",
      "[Train] batch: 008, loss_train: 0.7368, acc_train: 0.5333\n",
      "[Train] batch: 009, loss_train: 0.4581, acc_train: 0.9000\n",
      "[Train] batch: 010, loss_train: 0.1606, acc_train: 1.0000\n",
      "[Train] batch: 011, loss_train: 0.2844, acc_train: 0.8667\n",
      "[Train] batch: 012, loss_train: 0.1538, acc_train: 1.0000\n",
      "[Train] batch: 013, loss_train: 0.1419, acc_train: 1.0000\n",
      "[Train] batch: 014, loss_train: 0.7183, acc_train: 0.4700\n",
      "[Train] batch: 015, loss_train: 0.5550, acc_train: 0.7267\n",
      "[Train] batch: 016, loss_train: 0.4667, acc_train: 0.7333\n",
      "[Train] batch: 017, loss_train: 0.3497, acc_train: 0.9000\n",
      "[Train] batch: 018, loss_train: 0.2568, acc_train: 0.8600\n",
      "[Train] batch: 019, loss_train: 0.4096, acc_train: 0.7171\n",
      "[Train] batch: 020, loss_train: 0.2758, acc_train: 0.8667\n",
      "[Train] batch: 021, loss_train: 0.3911, acc_train: 0.7333\n",
      "[Train] batch: 022, loss_train: 0.5550, acc_train: 0.7267\n",
      "[Train] batch: 023, loss_train: 0.3739, acc_train: 0.8571\n",
      "[Train] batch: 024, loss_train: 0.4318, acc_train: 0.8667\n",
      "[Train] batch: 025, loss_train: 0.3416, acc_train: 0.8600\n",
      "[Train] batch: 026, loss_train: 0.2526, acc_train: 0.8905\n",
      "[Train] batch: 027, loss_train: 0.2940, acc_train: 0.8571\n",
      "[Train] batch: 028, loss_train: 0.3647, acc_train: 0.7333\n",
      "[Train] batch: 029, loss_train: 0.2614, acc_train: 0.8571\n",
      "[Train] batch: 030, loss_train: 0.8055, acc_train: 0.6571\n",
      "[Train] batch: 031, loss_train: 0.1134, acc_train: 1.0000\n",
      "[Train] batch: 032, loss_train: 0.2616, acc_train: 0.9000\n",
      "[Train] batch: 033, loss_train: 0.1177, acc_train: 1.0000\n",
      "[Train] batch: 034, loss_train: 0.3514, acc_train: 0.8600\n",
      "[Train] batch: 035, loss_train: 0.4089, acc_train: 0.8600\n",
      "[Train] batch: 036, loss_train: 0.1429, acc_train: 1.0000\n",
      "[Train] batch: 037, loss_train: 0.1111, acc_train: 1.0000\n",
      "[Train] batch: 038, loss_train: 0.7233, acc_train: 0.7667\n",
      "[Train] batch: 039, loss_train: 0.0546, acc_train: 1.0000\n",
      "[Train] batch: 040, loss_train: 0.6512, acc_train: 0.7600\n",
      "[Train] batch: 041, loss_train: 0.7147, acc_train: 0.7333\n",
      "[Train] batch: 042, loss_train: 0.3516, acc_train: 0.8667\n",
      "[Train] batch: 043, loss_train: 0.5671, acc_train: 0.7333\n",
      "[Train] batch: 044, loss_train: 0.3494, acc_train: 0.8667\n",
      "[Train] batch: 045, loss_train: 0.0887, acc_train: 1.0000\n",
      "[Train] batch: 046, loss_train: 0.3214, acc_train: 0.8600\n",
      "[Train] batch: 047, loss_train: 0.3509, acc_train: 0.7333\n",
      "[Train] batch: 048, loss_train: 0.9868, acc_train: 0.4833\n",
      "[Train] batch: 049, loss_train: 0.2285, acc_train: 0.9400\n",
      "[Train] batch: 050, loss_train: 0.1673, acc_train: 1.0000\n",
      "[Train] batch: 051, loss_train: 0.3631, acc_train: 0.8000\n",
      "[Train] batch: 052, loss_train: 0.2218, acc_train: 1.0000\n",
      "[Train] batch: 053, loss_train: 0.1818, acc_train: 0.8600\n",
      "[Train] batch: 054, loss_train: 0.1860, acc_train: 0.8667\n",
      "[Train] batch: 055, loss_train: 0.4576, acc_train: 0.7333\n",
      "[Train] batch: 056, loss_train: 0.1163, acc_train: 1.0000\n",
      "[Train] batch: 057, loss_train: 0.1856, acc_train: 0.9000\n",
      "[Train] batch: 058, loss_train: 0.1119, acc_train: 1.0000\n",
      "[Train] batch: 059, loss_train: 0.2748, acc_train: 0.8667\n",
      "[Train] batch: 060, loss_train: 0.8789, acc_train: 0.5917\n",
      "[Train] batch: 061, loss_train: 0.6737, acc_train: 0.7556\n",
      "[Train] batch: 062, loss_train: 0.1208, acc_train: 1.0000\n",
      "[Train] batch: 063, loss_train: 0.0609, acc_train: 1.0000\n",
      "[Train] batch: 064, loss_train: 0.4411, acc_train: 0.8000\n",
      "[Train] batch: 065, loss_train: 0.5243, acc_train: 0.7667\n",
      "[Train] batch: 066, loss_train: 0.5942, acc_train: 0.7667\n",
      "[Train] batch: 067, loss_train: 0.5159, acc_train: 0.6000\n",
      "[Train] batch: 068, loss_train: 0.2019, acc_train: 0.8600\n",
      "[Train] batch: 069, loss_train: 0.2740, acc_train: 0.8571\n",
      "[Train] batch: 070, loss_train: 0.8171, acc_train: 0.8000\n",
      "[Train] batch: 071, loss_train: 0.4325, acc_train: 0.8067\n",
      "[Train] batch: 072, loss_train: 0.6288, acc_train: 0.4750\n",
      "[Train] batch: 073, loss_train: 0.1353, acc_train: 1.0000\n",
      "[Train] batch: 074, loss_train: 0.3881, acc_train: 0.8667\n",
      "[Train] batch: 075, loss_train: 0.1900, acc_train: 1.0000\n",
      "[Train] batch: 076, loss_train: 0.4387, acc_train: 0.8667\n",
      "[Train] batch: 077, loss_train: 0.4883, acc_train: 0.7127\n",
      "[Train] batch: 078, loss_train: 0.3455, acc_train: 0.8600\n",
      "[Train] batch: 079, loss_train: 0.1253, acc_train: 1.0000\n",
      "[Train] batch: 080, loss_train: 0.6375, acc_train: 0.6333\n",
      "[Train] batch: 081, loss_train: 0.1096, acc_train: 1.0000\n",
      "[Train] batch: 082, loss_train: 0.2842, acc_train: 0.8600\n",
      "[Train] batch: 083, loss_train: 0.1317, acc_train: 1.0000\n",
      "[Train] batch: 084, loss_train: 0.1179, acc_train: 1.0000\n",
      "[Train] batch: 085, loss_train: 0.2922, acc_train: 0.7267\n",
      "[Train] batch: 086, loss_train: 0.7051, acc_train: 0.6238\n",
      "[Train] batch: 087, loss_train: 0.5360, acc_train: 0.8000\n",
      "[Train] batch: 088, loss_train: 0.0569, acc_train: 1.0000\n",
      "[Train] batch: 089, loss_train: 0.3407, acc_train: 0.8667\n",
      "[Train] batch: 090, loss_train: 0.3704, acc_train: 0.8600\n",
      "[Train] batch: 091, loss_train: 0.4501, acc_train: 0.9000\n",
      "[Train] batch: 092, loss_train: 0.6365, acc_train: 0.9000\n",
      "[Train] batch: 093, loss_train: 0.6860, acc_train: 0.7267\n",
      "[Train] batch: 094, loss_train: 0.2741, acc_train: 0.8571\n",
      "[Train] batch: 095, loss_train: 0.5620, acc_train: 0.7171\n",
      "[Train] batch: 096, loss_train: 0.0666, acc_train: 1.0000\n",
      "[Train] batch: 097, loss_train: 0.4157, acc_train: 0.8667\n",
      "[Train] batch: 098, loss_train: 0.2703, acc_train: 0.8600\n",
      "[Train] batch: 099, loss_train: 0.7265, acc_train: 0.6571\n",
      "[Train] batch: 100, loss_train: 0.3982, acc_train: 0.7500\n",
      "[Train] batch: 101, loss_train: 0.2784, acc_train: 0.8600\n",
      "[Train] batch: 102, loss_train: 0.2146, acc_train: 0.9000\n",
      "[Train] batch: 103, loss_train: 0.1974, acc_train: 0.8667\n",
      "[Train] batch: 104, loss_train: 0.5171, acc_train: 0.6600\n",
      "[Train] batch: 105, loss_train: 0.2897, acc_train: 0.8000\n",
      "[Train] batch: 106, loss_train: 0.3645, acc_train: 0.7600\n",
      "[Train] batch: 107, loss_train: 0.4855, acc_train: 0.8933\n",
      "[Train] batch: 108, loss_train: 0.4596, acc_train: 0.7000\n",
      "[Train] batch: 109, loss_train: 0.5636, acc_train: 0.6171\n",
      "[Train] batch: 110, loss_train: 0.3538, acc_train: 0.9000\n",
      "[Train] batch: 111, loss_train: 0.4345, acc_train: 0.8600\n",
      "[Train] batch: 112, loss_train: 0.8016, acc_train: 0.7000\n",
      "[Train] batch: 113, loss_train: 0.8241, acc_train: 0.5850\n",
      "[Train] batch: 114, loss_train: 0.3075, acc_train: 0.7267\n",
      "[Train] batch: 115, loss_train: 0.2969, acc_train: 0.8667\n",
      "[Train] batch: 116, loss_train: 0.2553, acc_train: 1.0000\n",
      "[Train] batch: 117, loss_train: 0.2774, acc_train: 0.9000\n",
      "[Train] batch: 118, loss_train: 0.7928, acc_train: 0.6933\n",
      "[Train] batch: 119, loss_train: 0.7103, acc_train: 0.8000\n",
      "[Train] batch: 120, loss_train: 0.3427, acc_train: 0.8500\n",
      "[Train] batch: 121, loss_train: 0.5091, acc_train: 0.6600\n",
      "[Train] batch: 122, loss_train: 1.0874, acc_train: 0.4000\n",
      "[Train] batch: 123, loss_train: 0.4328, acc_train: 0.7600\n",
      "[Train] batch: 124, loss_train: 0.1720, acc_train: 1.0000\n",
      "[Train] batch: 125, loss_train: 0.1705, acc_train: 1.0000\n",
      "[Train] batch: 126, loss_train: 0.3838, acc_train: 0.8571\n",
      "[Train] batch: 127, loss_train: 0.7434, acc_train: 0.6100\n",
      "[Train] batch: 128, loss_train: 0.9648, acc_train: 0.4867\n",
      "[Train] batch: 129, loss_train: 0.3624, acc_train: 0.8600\n",
      "[Train] batch: 130, loss_train: 0.1819, acc_train: 0.8667\n",
      "[Train] batch: 131, loss_train: 0.1737, acc_train: 0.9333\n",
      "[Train] batch: 132, loss_train: 0.4221, acc_train: 0.9000\n",
      "[Train] batch: 133, loss_train: 0.6094, acc_train: 0.6700\n",
      "[Train] batch: 134, loss_train: 1.4371, acc_train: 0.4714\n",
      "[Val] loss= 0.4736, accuracy= 0.7095, label acc= [0.22641509 0.94117647 0.08       0.92307692 1.         0.92307692\n",
      " 0.87272727]\n",
      "Epoch: 4\n",
      "[Train] batch: 001, loss_train: 0.2111, acc_train: 0.8600\n",
      "[Train] batch: 002, loss_train: 0.2786, acc_train: 0.8000\n",
      "[Train] batch: 003, loss_train: 0.4000, acc_train: 0.6667\n",
      "[Train] batch: 004, loss_train: 0.3832, acc_train: 0.8933\n",
      "[Train] batch: 005, loss_train: 0.3125, acc_train: 0.8933\n",
      "[Train] batch: 006, loss_train: 0.2691, acc_train: 0.9000\n",
      "[Train] batch: 007, loss_train: 0.6105, acc_train: 0.7000\n",
      "[Train] batch: 008, loss_train: 0.5269, acc_train: 0.6400\n",
      "[Train] batch: 009, loss_train: 0.4535, acc_train: 0.6667\n",
      "[Train] batch: 010, loss_train: 0.6566, acc_train: 0.7200\n",
      "[Train] batch: 011, loss_train: 0.4002, acc_train: 0.8571\n",
      "[Train] batch: 012, loss_train: 0.3583, acc_train: 0.8600\n",
      "[Train] batch: 013, loss_train: 0.3324, acc_train: 0.8571\n",
      "[Train] batch: 014, loss_train: 0.5392, acc_train: 0.7267\n",
      "[Train] batch: 015, loss_train: 0.0669, acc_train: 1.0000\n",
      "[Train] batch: 016, loss_train: 0.4819, acc_train: 0.7267\n",
      "[Train] batch: 017, loss_train: 0.4045, acc_train: 0.7333\n",
      "[Train] batch: 018, loss_train: 0.3910, acc_train: 0.8667\n",
      "[Train] batch: 019, loss_train: 0.4875, acc_train: 0.7600\n",
      "[Train] batch: 020, loss_train: 0.6104, acc_train: 0.8000\n",
      "[Train] batch: 021, loss_train: 0.0915, acc_train: 1.0000\n",
      "[Train] batch: 022, loss_train: 0.2032, acc_train: 0.9400\n",
      "[Train] batch: 023, loss_train: 0.1401, acc_train: 1.0000\n",
      "[Train] batch: 024, loss_train: 0.8329, acc_train: 0.5857\n",
      "[Train] batch: 025, loss_train: 0.2542, acc_train: 0.8600\n",
      "[Train] batch: 026, loss_train: 0.8329, acc_train: 0.5667\n",
      "[Train] batch: 027, loss_train: 0.2868, acc_train: 0.8600\n",
      "[Train] batch: 028, loss_train: 0.3999, acc_train: 0.9444\n",
      "[Train] batch: 029, loss_train: 0.3327, acc_train: 0.7267\n",
      "[Train] batch: 030, loss_train: 0.2687, acc_train: 0.8667\n",
      "[Train] batch: 031, loss_train: 0.3968, acc_train: 0.7333\n",
      "[Train] batch: 032, loss_train: 0.3629, acc_train: 0.8600\n",
      "[Train] batch: 033, loss_train: 0.3824, acc_train: 0.7571\n",
      "[Train] batch: 034, loss_train: 0.2449, acc_train: 0.8571\n",
      "[Train] batch: 035, loss_train: 0.1226, acc_train: 1.0000\n",
      "[Train] batch: 036, loss_train: 0.2622, acc_train: 0.8600\n",
      "[Train] batch: 037, loss_train: 0.4492, acc_train: 0.7238\n",
      "[Train] batch: 038, loss_train: 0.5767, acc_train: 0.7667\n",
      "[Train] batch: 039, loss_train: 0.5113, acc_train: 0.7238\n",
      "[Train] batch: 040, loss_train: 0.9506, acc_train: 0.5238\n",
      "[Train] batch: 041, loss_train: 0.1691, acc_train: 1.0000\n",
      "[Train] batch: 042, loss_train: 0.3827, acc_train: 0.7333\n",
      "[Train] batch: 043, loss_train: 0.4278, acc_train: 0.7333\n",
      "[Train] batch: 044, loss_train: 0.8822, acc_train: 0.4571\n",
      "[Train] batch: 045, loss_train: 0.3049, acc_train: 0.8556\n",
      "[Train] batch: 046, loss_train: 0.2663, acc_train: 0.8667\n",
      "[Train] batch: 047, loss_train: 0.7858, acc_train: 0.5333\n",
      "[Train] batch: 048, loss_train: 0.2384, acc_train: 1.0000\n",
      "[Train] batch: 049, loss_train: 0.7112, acc_train: 0.4667\n",
      "[Train] batch: 050, loss_train: 0.1799, acc_train: 0.8667\n",
      "[Train] batch: 051, loss_train: 0.2962, acc_train: 0.7600\n",
      "[Train] batch: 052, loss_train: 0.3697, acc_train: 0.8600\n",
      "[Train] batch: 053, loss_train: 0.5327, acc_train: 0.8429\n",
      "[Train] batch: 054, loss_train: 0.3535, acc_train: 0.8095\n",
      "[Train] batch: 055, loss_train: 0.4710, acc_train: 0.9000\n",
      "[Train] batch: 056, loss_train: 0.1320, acc_train: 1.0000\n",
      "[Train] batch: 057, loss_train: 0.3082, acc_train: 0.8571\n",
      "[Train] batch: 058, loss_train: 0.3063, acc_train: 0.8000\n",
      "[Train] batch: 059, loss_train: 0.5760, acc_train: 0.7333\n",
      "[Train] batch: 060, loss_train: 0.1347, acc_train: 1.0000\n",
      "[Train] batch: 061, loss_train: 0.6153, acc_train: 0.8167\n",
      "[Train] batch: 062, loss_train: 0.0618, acc_train: 1.0000\n",
      "[Train] batch: 063, loss_train: 0.2702, acc_train: 0.9000\n",
      "[Train] batch: 064, loss_train: 0.1907, acc_train: 1.0000\n",
      "[Train] batch: 065, loss_train: 0.2341, acc_train: 0.9000\n",
      "[Train] batch: 066, loss_train: 0.0324, acc_train: 1.0000\n",
      "[Train] batch: 067, loss_train: 1.1707, acc_train: 0.5905\n",
      "[Train] batch: 068, loss_train: 0.8321, acc_train: 0.7267\n",
      "[Train] batch: 069, loss_train: 1.0500, acc_train: 0.5100\n",
      "[Train] batch: 070, loss_train: 0.5326, acc_train: 0.6000\n",
      "[Train] batch: 071, loss_train: 0.3937, acc_train: 0.7200\n",
      "[Train] batch: 072, loss_train: 0.5508, acc_train: 0.9000\n",
      "[Train] batch: 073, loss_train: 0.3845, acc_train: 0.8667\n",
      "[Train] batch: 074, loss_train: 0.4176, acc_train: 0.7333\n",
      "[Train] batch: 075, loss_train: 0.3637, acc_train: 0.8667\n",
      "[Train] batch: 076, loss_train: 0.4853, acc_train: 0.9000\n",
      "[Train] batch: 077, loss_train: 0.3309, acc_train: 0.7000\n",
      "[Train] batch: 078, loss_train: 0.3733, acc_train: 0.7833\n",
      "[Train] batch: 079, loss_train: 0.9759, acc_train: 0.5333\n",
      "[Train] batch: 080, loss_train: 0.4961, acc_train: 0.6156\n",
      "[Train] batch: 081, loss_train: 0.2696, acc_train: 0.9333\n",
      "[Train] batch: 082, loss_train: 0.1944, acc_train: 0.9000\n",
      "[Train] batch: 083, loss_train: 0.4697, acc_train: 0.7200\n",
      "[Train] batch: 084, loss_train: 0.5435, acc_train: 0.6500\n",
      "[Train] batch: 085, loss_train: 0.3604, acc_train: 0.8571\n",
      "[Train] batch: 086, loss_train: 0.4382, acc_train: 0.7333\n",
      "[Train] batch: 087, loss_train: 0.3810, acc_train: 0.8571\n",
      "[Train] batch: 088, loss_train: 0.4797, acc_train: 0.8067\n",
      "[Train] batch: 089, loss_train: 0.7308, acc_train: 0.6333\n",
      "[Train] batch: 090, loss_train: 0.7409, acc_train: 0.6833\n",
      "[Train] batch: 091, loss_train: 0.3037, acc_train: 0.8667\n",
      "[Train] batch: 092, loss_train: 0.2474, acc_train: 0.9000\n",
      "[Train] batch: 093, loss_train: 0.5324, acc_train: 0.9000\n",
      "[Train] batch: 094, loss_train: 0.2173, acc_train: 0.8571\n",
      "[Train] batch: 095, loss_train: 0.5697, acc_train: 0.5838\n",
      "[Train] batch: 096, loss_train: 0.3880, acc_train: 0.8571\n",
      "[Train] batch: 097, loss_train: 0.3245, acc_train: 0.9000\n",
      "[Train] batch: 098, loss_train: 0.5676, acc_train: 0.5867\n",
      "[Train] batch: 099, loss_train: 0.2000, acc_train: 0.9444\n",
      "[Train] batch: 100, loss_train: 0.1401, acc_train: 1.0000\n",
      "[Train] batch: 101, loss_train: 0.2874, acc_train: 0.8667\n",
      "[Train] batch: 102, loss_train: 0.1093, acc_train: 1.0000\n",
      "[Train] batch: 103, loss_train: 0.3569, acc_train: 0.7333\n",
      "[Train] batch: 104, loss_train: 0.3956, acc_train: 0.8667\n",
      "[Train] batch: 105, loss_train: 0.1224, acc_train: 1.0000\n",
      "[Train] batch: 106, loss_train: 0.1915, acc_train: 0.8667\n",
      "[Train] batch: 107, loss_train: 0.6951, acc_train: 0.6333\n",
      "[Train] batch: 108, loss_train: 0.1802, acc_train: 0.8600\n",
      "[Train] batch: 109, loss_train: 0.2847, acc_train: 0.8571\n",
      "[Train] batch: 110, loss_train: 0.2590, acc_train: 0.8600\n",
      "[Train] batch: 111, loss_train: 0.5867, acc_train: 0.8667\n",
      "[Train] batch: 112, loss_train: 0.4598, acc_train: 0.8667\n",
      "[Train] batch: 113, loss_train: 0.0592, acc_train: 1.0000\n",
      "[Train] batch: 114, loss_train: 0.7026, acc_train: 0.7333\n",
      "[Train] batch: 115, loss_train: 0.4732, acc_train: 0.9000\n",
      "[Train] batch: 116, loss_train: 0.5219, acc_train: 0.7333\n",
      "[Train] batch: 117, loss_train: 0.1921, acc_train: 0.8667\n",
      "[Train] batch: 118, loss_train: 0.4392, acc_train: 0.9000\n",
      "[Train] batch: 119, loss_train: 0.4805, acc_train: 0.7667\n",
      "[Train] batch: 120, loss_train: 0.2204, acc_train: 0.9400\n",
      "[Train] batch: 121, loss_train: 0.3056, acc_train: 0.9000\n",
      "[Train] batch: 122, loss_train: 0.3246, acc_train: 0.8667\n",
      "[Train] batch: 123, loss_train: 0.5806, acc_train: 0.7333\n",
      "[Train] batch: 124, loss_train: 0.5721, acc_train: 0.7571\n",
      "[Train] batch: 125, loss_train: 0.5294, acc_train: 0.8000\n",
      "[Train] batch: 126, loss_train: 0.2235, acc_train: 1.0000\n",
      "[Train] batch: 127, loss_train: 0.3799, acc_train: 0.8667\n",
      "[Train] batch: 128, loss_train: 0.2173, acc_train: 0.9400\n",
      "[Train] batch: 129, loss_train: 0.5909, acc_train: 0.6867\n",
      "[Train] batch: 130, loss_train: 0.4085, acc_train: 0.7333\n",
      "[Train] batch: 131, loss_train: 0.2764, acc_train: 0.8600\n",
      "[Train] batch: 132, loss_train: 0.6248, acc_train: 0.7400\n",
      "[Train] batch: 133, loss_train: 0.4406, acc_train: 0.6800\n",
      "[Train] batch: 134, loss_train: 0.1836, acc_train: 0.9000\n",
      "[Val] loss= 0.4306, accuracy= 0.7965, label acc= [0.17142857 0.94117647 0.74418605 0.92307692 1.         0.92307692\n",
      " 0.87272727]\n",
      "Epoch: 5\n",
      "[Train] batch: 001, loss_train: 0.3633, acc_train: 0.8100\n",
      "[Train] batch: 002, loss_train: 0.3129, acc_train: 0.8600\n",
      "[Train] batch: 003, loss_train: 0.3190, acc_train: 0.8000\n",
      "[Train] batch: 004, loss_train: 0.7723, acc_train: 0.6167\n",
      "[Train] batch: 005, loss_train: 0.6248, acc_train: 0.6667\n",
      "[Train] batch: 006, loss_train: 0.5911, acc_train: 0.6222\n",
      "[Train] batch: 007, loss_train: 0.1478, acc_train: 1.0000\n",
      "[Train] batch: 008, loss_train: 0.6658, acc_train: 0.6000\n",
      "[Train] batch: 009, loss_train: 0.2305, acc_train: 1.0000\n",
      "[Train] batch: 010, loss_train: 0.7125, acc_train: 0.6667\n",
      "[Train] batch: 011, loss_train: 0.4427, acc_train: 0.9429\n",
      "[Train] batch: 012, loss_train: 0.4079, acc_train: 0.7600\n",
      "[Train] batch: 013, loss_train: 0.4436, acc_train: 0.7333\n",
      "[Train] batch: 014, loss_train: 0.3588, acc_train: 0.9000\n",
      "[Train] batch: 015, loss_train: 0.5186, acc_train: 0.8000\n",
      "[Train] batch: 016, loss_train: 0.2123, acc_train: 1.0000\n",
      "[Train] batch: 017, loss_train: 0.1223, acc_train: 1.0000\n",
      "[Train] batch: 018, loss_train: 0.6827, acc_train: 0.7156\n",
      "[Train] batch: 019, loss_train: 0.1388, acc_train: 1.0000\n",
      "[Train] batch: 020, loss_train: 0.0590, acc_train: 1.0000\n",
      "[Train] batch: 021, loss_train: 0.3170, acc_train: 0.8600\n",
      "[Train] batch: 022, loss_train: 0.5712, acc_train: 0.7667\n",
      "[Train] batch: 023, loss_train: 0.7680, acc_train: 0.6100\n",
      "[Train] batch: 024, loss_train: 0.6026, acc_train: 0.7267\n",
      "[Train] batch: 025, loss_train: 0.0771, acc_train: 1.0000\n",
      "[Train] batch: 026, loss_train: 0.4494, acc_train: 0.7238\n",
      "[Train] batch: 027, loss_train: 0.0914, acc_train: 1.0000\n",
      "[Train] batch: 028, loss_train: 0.1930, acc_train: 0.9429\n",
      "[Train] batch: 029, loss_train: 0.0764, acc_train: 1.0000\n",
      "[Train] batch: 030, loss_train: 0.2743, acc_train: 0.8600\n",
      "[Train] batch: 031, loss_train: 0.7612, acc_train: 0.7333\n",
      "[Train] batch: 032, loss_train: 0.4184, acc_train: 0.8600\n",
      "[Train] batch: 033, loss_train: 0.3431, acc_train: 0.8556\n",
      "[Train] batch: 034, loss_train: 0.8878, acc_train: 0.7500\n",
      "[Train] batch: 035, loss_train: 0.2200, acc_train: 0.8667\n",
      "[Train] batch: 036, loss_train: 0.0457, acc_train: 1.0000\n",
      "[Train] batch: 037, loss_train: 0.4557, acc_train: 0.6400\n",
      "[Train] batch: 038, loss_train: 0.1450, acc_train: 1.0000\n",
      "[Train] batch: 039, loss_train: 0.0924, acc_train: 1.0000\n",
      "[Train] batch: 040, loss_train: 0.1715, acc_train: 1.0000\n",
      "[Train] batch: 041, loss_train: 0.3986, acc_train: 0.7545\n",
      "[Train] batch: 042, loss_train: 0.5837, acc_train: 0.7333\n",
      "[Train] batch: 043, loss_train: 0.1106, acc_train: 1.0000\n",
      "[Train] batch: 044, loss_train: 0.2173, acc_train: 0.9000\n",
      "[Train] batch: 045, loss_train: 0.1151, acc_train: 1.0000\n",
      "[Train] batch: 046, loss_train: 0.2544, acc_train: 0.8667\n",
      "[Train] batch: 047, loss_train: 0.2023, acc_train: 0.9333\n",
      "[Train] batch: 048, loss_train: 0.8204, acc_train: 0.6000\n",
      "[Train] batch: 049, loss_train: 0.4113, acc_train: 0.7333\n",
      "[Train] batch: 050, loss_train: 0.2149, acc_train: 0.8600\n",
      "[Train] batch: 051, loss_train: 0.3280, acc_train: 0.7667\n",
      "[Train] batch: 052, loss_train: 0.3002, acc_train: 0.8762\n",
      "[Train] batch: 053, loss_train: 0.7982, acc_train: 0.6600\n",
      "[Train] batch: 054, loss_train: 0.7566, acc_train: 0.7667\n",
      "[Train] batch: 055, loss_train: 0.5557, acc_train: 0.9000\n",
      "[Train] batch: 056, loss_train: 0.1982, acc_train: 0.8667\n",
      "[Train] batch: 057, loss_train: 0.5158, acc_train: 0.8667\n",
      "[Train] batch: 058, loss_train: 1.0518, acc_train: 0.5167\n",
      "[Train] batch: 059, loss_train: 0.2911, acc_train: 0.8556\n",
      "[Train] batch: 060, loss_train: 0.1597, acc_train: 1.0000\n",
      "[Train] batch: 061, loss_train: 0.6283, acc_train: 0.7238\n",
      "[Train] batch: 062, loss_train: 0.0886, acc_train: 1.0000\n",
      "[Train] batch: 063, loss_train: 0.1995, acc_train: 0.9000\n",
      "[Train] batch: 064, loss_train: 0.5017, acc_train: 0.7833\n",
      "[Train] batch: 065, loss_train: 0.5052, acc_train: 0.8000\n",
      "[Train] batch: 066, loss_train: 0.2168, acc_train: 1.0000\n",
      "[Train] batch: 067, loss_train: 0.2730, acc_train: 0.7000\n",
      "[Train] batch: 068, loss_train: 0.4240, acc_train: 0.8571\n",
      "[Train] batch: 069, loss_train: 0.8534, acc_train: 0.5738\n",
      "[Train] batch: 070, loss_train: 0.4364, acc_train: 0.6250\n",
      "[Train] batch: 071, loss_train: 0.1169, acc_train: 1.0000\n",
      "[Train] batch: 072, loss_train: 0.7517, acc_train: 0.6000\n",
      "[Train] batch: 073, loss_train: 0.5919, acc_train: 0.6600\n",
      "[Train] batch: 074, loss_train: 0.1425, acc_train: 1.0000\n",
      "[Train] batch: 075, loss_train: 0.5449, acc_train: 0.7600\n",
      "[Train] batch: 076, loss_train: 0.2972, acc_train: 0.9000\n",
      "[Train] batch: 077, loss_train: 0.2956, acc_train: 0.9000\n",
      "[Train] batch: 078, loss_train: 0.2450, acc_train: 1.0000\n",
      "[Train] batch: 079, loss_train: 0.1371, acc_train: 1.0000\n",
      "[Train] batch: 080, loss_train: 0.4025, acc_train: 0.8571\n",
      "[Train] batch: 081, loss_train: 0.2572, acc_train: 0.8667\n",
      "[Train] batch: 082, loss_train: 0.7138, acc_train: 0.8500\n",
      "[Train] batch: 083, loss_train: 0.2604, acc_train: 0.8667\n",
      "[Train] batch: 084, loss_train: 0.2938, acc_train: 0.8571\n",
      "[Train] batch: 085, loss_train: 0.4518, acc_train: 0.7222\n",
      "[Train] batch: 086, loss_train: 0.3519, acc_train: 0.7333\n",
      "[Train] batch: 087, loss_train: 0.3857, acc_train: 0.8600\n",
      "[Train] batch: 088, loss_train: 0.2637, acc_train: 0.7333\n",
      "[Train] batch: 089, loss_train: 0.7870, acc_train: 0.6600\n",
      "[Train] batch: 090, loss_train: 0.3676, acc_train: 1.0000\n",
      "[Train] batch: 091, loss_train: 0.4880, acc_train: 0.6667\n",
      "[Train] batch: 092, loss_train: 0.6043, acc_train: 0.7156\n",
      "[Train] batch: 093, loss_train: 0.9475, acc_train: 0.6000\n",
      "[Train] batch: 094, loss_train: 0.1397, acc_train: 1.0000\n",
      "[Train] batch: 095, loss_train: 0.6114, acc_train: 0.8000\n",
      "[Train] batch: 096, loss_train: 0.4317, acc_train: 0.7000\n",
      "[Train] batch: 097, loss_train: 0.7018, acc_train: 0.4867\n",
      "[Train] batch: 098, loss_train: 0.4920, acc_train: 0.7667\n",
      "[Train] batch: 099, loss_train: 0.1689, acc_train: 1.0000\n",
      "[Train] batch: 100, loss_train: 0.3088, acc_train: 0.9400\n",
      "[Train] batch: 101, loss_train: 0.2191, acc_train: 0.9000\n",
      "[Train] batch: 102, loss_train: 0.1570, acc_train: 1.0000\n",
      "[Train] batch: 103, loss_train: 0.2791, acc_train: 0.8571\n",
      "[Train] batch: 104, loss_train: 1.0449, acc_train: 0.3333\n",
      "[Train] batch: 105, loss_train: 0.4741, acc_train: 0.8400\n",
      "[Train] batch: 106, loss_train: 0.1973, acc_train: 0.8538\n",
      "[Train] batch: 107, loss_train: 0.9572, acc_train: 0.4667\n",
      "[Train] batch: 108, loss_train: 0.2053, acc_train: 0.9000\n",
      "[Train] batch: 109, loss_train: 0.4403, acc_train: 0.8100\n",
      "[Train] batch: 110, loss_train: 0.2782, acc_train: 1.0000\n",
      "[Train] batch: 111, loss_train: 0.0777, acc_train: 1.0000\n",
      "[Train] batch: 112, loss_train: 1.0669, acc_train: 0.5838\n",
      "[Train] batch: 113, loss_train: 0.3437, acc_train: 0.7667\n",
      "[Train] batch: 114, loss_train: 1.1745, acc_train: 0.5905\n",
      "[Train] batch: 115, loss_train: 0.4088, acc_train: 0.7667\n",
      "[Train] batch: 116, loss_train: 0.1267, acc_train: 1.0000\n",
      "[Train] batch: 117, loss_train: 0.3292, acc_train: 0.8667\n",
      "[Train] batch: 118, loss_train: 0.4344, acc_train: 0.7171\n",
      "[Train] batch: 119, loss_train: 0.3502, acc_train: 0.8095\n",
      "[Train] batch: 120, loss_train: 0.2422, acc_train: 0.9000\n",
      "[Train] batch: 121, loss_train: 0.5099, acc_train: 0.7667\n",
      "[Train] batch: 122, loss_train: 0.3957, acc_train: 0.8000\n",
      "[Train] batch: 123, loss_train: 0.1493, acc_train: 1.0000\n",
      "[Train] batch: 124, loss_train: 0.3930, acc_train: 0.8000\n",
      "[Train] batch: 125, loss_train: 0.1421, acc_train: 1.0000\n",
      "[Train] batch: 126, loss_train: 0.4916, acc_train: 0.7000\n",
      "[Train] batch: 127, loss_train: 0.1811, acc_train: 1.0000\n",
      "[Train] batch: 128, loss_train: 0.2759, acc_train: 0.9000\n",
      "[Train] batch: 129, loss_train: 0.8401, acc_train: 0.5867\n",
      "[Train] batch: 130, loss_train: 0.6539, acc_train: 0.7571\n",
      "[Train] batch: 131, loss_train: 0.4225, acc_train: 0.8667\n",
      "[Train] batch: 132, loss_train: 0.4217, acc_train: 0.7267\n",
      "[Train] batch: 133, loss_train: 0.4481, acc_train: 0.8667\n",
      "[Train] batch: 134, loss_train: 0.3581, acc_train: 0.8667\n",
      "[Val] loss= 0.3852, accuracy= 0.7927, label acc= [0.         0.94117647 0.88888889 0.92307692 1.         0.92307692\n",
      " 0.87272727]\n",
      "Epoch: 6\n",
      "[Train] batch: 001, loss_train: 0.1707, acc_train: 1.0000\n",
      "[Train] batch: 002, loss_train: 0.4244, acc_train: 0.8933\n",
      "[Train] batch: 003, loss_train: 0.0377, acc_train: 1.0000\n",
      "[Train] batch: 004, loss_train: 0.4258, acc_train: 0.7238\n",
      "[Train] batch: 005, loss_train: 0.5885, acc_train: 0.7667\n",
      "[Train] batch: 006, loss_train: 0.3424, acc_train: 0.7250\n",
      "[Train] batch: 007, loss_train: 0.0952, acc_train: 1.0000\n",
      "[Train] batch: 008, loss_train: 0.4285, acc_train: 0.7700\n",
      "[Train] batch: 009, loss_train: 0.2570, acc_train: 0.8889\n",
      "[Train] batch: 010, loss_train: 0.1228, acc_train: 1.0000\n",
      "[Train] batch: 011, loss_train: 0.2828, acc_train: 0.8667\n",
      "[Train] batch: 012, loss_train: 0.1375, acc_train: 1.0000\n",
      "[Train] batch: 013, loss_train: 0.2336, acc_train: 0.9000\n",
      "[Train] batch: 014, loss_train: 0.3152, acc_train: 0.8545\n",
      "[Train] batch: 015, loss_train: 0.5134, acc_train: 0.6200\n",
      "[Train] batch: 016, loss_train: 0.2940, acc_train: 0.8667\n",
      "[Train] batch: 017, loss_train: 0.2815, acc_train: 0.8571\n",
      "[Train] batch: 018, loss_train: 0.0713, acc_train: 1.0000\n",
      "[Train] batch: 019, loss_train: 0.1365, acc_train: 1.0000\n",
      "[Train] batch: 020, loss_train: 0.2733, acc_train: 0.8600\n",
      "[Train] batch: 021, loss_train: 0.1103, acc_train: 1.0000\n",
      "[Train] batch: 022, loss_train: 0.9947, acc_train: 0.4500\n",
      "[Train] batch: 023, loss_train: 0.4786, acc_train: 0.7333\n",
      "[Train] batch: 024, loss_train: 0.4735, acc_train: 0.8000\n",
      "[Train] batch: 025, loss_train: 0.2919, acc_train: 0.8000\n",
      "[Train] batch: 026, loss_train: 0.1225, acc_train: 1.0000\n",
      "[Train] batch: 027, loss_train: 0.2161, acc_train: 0.9333\n",
      "[Train] batch: 028, loss_train: 0.3679, acc_train: 0.8667\n",
      "[Train] batch: 029, loss_train: 0.3888, acc_train: 0.7333\n",
      "[Train] batch: 030, loss_train: 0.2693, acc_train: 0.8667\n",
      "[Train] batch: 031, loss_train: 0.2243, acc_train: 1.0000\n",
      "[Train] batch: 032, loss_train: 0.1554, acc_train: 1.0000\n",
      "[Train] batch: 033, loss_train: 0.2808, acc_train: 0.8600\n",
      "[Train] batch: 034, loss_train: 0.5136, acc_train: 0.9000\n",
      "[Train] batch: 035, loss_train: 0.2468, acc_train: 0.9000\n",
      "[Train] batch: 036, loss_train: 0.8299, acc_train: 0.6000\n",
      "[Train] batch: 037, loss_train: 0.5222, acc_train: 0.7500\n",
      "[Train] batch: 038, loss_train: 0.4299, acc_train: 0.8667\n",
      "[Train] batch: 039, loss_train: 0.6589, acc_train: 0.7333\n",
      "[Train] batch: 040, loss_train: 0.3441, acc_train: 0.8600\n",
      "[Train] batch: 041, loss_train: 0.6548, acc_train: 0.7500\n",
      "[Train] batch: 042, loss_train: 0.0868, acc_train: 1.0000\n",
      "[Train] batch: 043, loss_train: 0.1301, acc_train: 1.0000\n",
      "[Train] batch: 044, loss_train: 0.4669, acc_train: 0.7250\n",
      "[Train] batch: 045, loss_train: 0.3329, acc_train: 0.8000\n",
      "[Train] batch: 046, loss_train: 0.2716, acc_train: 0.7600\n",
      "[Train] batch: 047, loss_train: 0.6372, acc_train: 0.8100\n",
      "[Train] batch: 048, loss_train: 0.1719, acc_train: 1.0000\n",
      "[Train] batch: 049, loss_train: 0.9273, acc_train: 0.5333\n",
      "[Train] batch: 050, loss_train: 0.1719, acc_train: 1.0000\n",
      "[Train] batch: 051, loss_train: 0.0728, acc_train: 1.0000\n",
      "[Train] batch: 052, loss_train: 0.0679, acc_train: 1.0000\n",
      "[Train] batch: 053, loss_train: 0.5450, acc_train: 0.7238\n",
      "[Train] batch: 054, loss_train: 0.2575, acc_train: 1.0000\n",
      "[Train] batch: 055, loss_train: 0.1968, acc_train: 0.9000\n",
      "[Train] batch: 056, loss_train: 0.2971, acc_train: 0.8933\n",
      "[Train] batch: 057, loss_train: 0.0975, acc_train: 1.0000\n",
      "[Train] batch: 058, loss_train: 0.3052, acc_train: 0.8556\n",
      "[Train] batch: 059, loss_train: 0.0627, acc_train: 1.0000\n",
      "[Train] batch: 060, loss_train: 0.2679, acc_train: 0.8600\n",
      "[Train] batch: 061, loss_train: 0.7620, acc_train: 0.7000\n",
      "[Train] batch: 062, loss_train: 0.2617, acc_train: 0.8667\n",
      "[Train] batch: 063, loss_train: 0.6110, acc_train: 0.7267\n",
      "[Train] batch: 064, loss_train: 0.7329, acc_train: 0.6167\n",
      "[Train] batch: 065, loss_train: 0.5257, acc_train: 0.6238\n",
      "[Train] batch: 066, loss_train: 0.5327, acc_train: 0.7600\n",
      "[Train] batch: 067, loss_train: 0.4471, acc_train: 0.8000\n",
      "[Train] batch: 068, loss_train: 0.2822, acc_train: 0.7500\n",
      "[Train] batch: 069, loss_train: 0.4215, acc_train: 0.7600\n",
      "[Train] batch: 070, loss_train: 0.3560, acc_train: 0.7571\n",
      "[Train] batch: 071, loss_train: 0.4894, acc_train: 0.6381\n",
      "[Train] batch: 072, loss_train: 0.4943, acc_train: 0.8400\n",
      "[Train] batch: 073, loss_train: 0.3862, acc_train: 0.7000\n",
      "[Train] batch: 074, loss_train: 0.5786, acc_train: 0.5917\n",
      "[Train] batch: 075, loss_train: 0.2445, acc_train: 1.0000\n",
      "[Train] batch: 076, loss_train: 0.3894, acc_train: 0.9400\n",
      "[Train] batch: 077, loss_train: 0.2426, acc_train: 1.0000\n",
      "[Train] batch: 078, loss_train: 0.9877, acc_train: 0.7267\n",
      "[Train] batch: 079, loss_train: 0.3538, acc_train: 0.7333\n",
      "[Train] batch: 080, loss_train: 0.2508, acc_train: 0.9000\n",
      "[Train] batch: 081, loss_train: 0.1576, acc_train: 0.8571\n",
      "[Train] batch: 082, loss_train: 0.4936, acc_train: 0.6267\n",
      "[Train] batch: 083, loss_train: 0.5470, acc_train: 0.7500\n",
      "[Train] batch: 084, loss_train: 0.3992, acc_train: 0.8667\n",
      "[Train] batch: 085, loss_train: 0.2483, acc_train: 1.0000\n",
      "[Train] batch: 086, loss_train: 0.5103, acc_train: 0.6767\n",
      "[Train] batch: 087, loss_train: 0.4564, acc_train: 0.8100\n",
      "[Train] batch: 088, loss_train: 0.3277, acc_train: 0.7238\n",
      "[Train] batch: 089, loss_train: 0.2917, acc_train: 0.8667\n",
      "[Train] batch: 090, loss_train: 0.4359, acc_train: 0.8100\n",
      "[Train] batch: 091, loss_train: 0.2566, acc_train: 1.0000\n",
      "[Train] batch: 092, loss_train: 0.5173, acc_train: 0.7333\n",
      "[Train] batch: 093, loss_train: 0.1744, acc_train: 0.8600\n",
      "[Train] batch: 094, loss_train: 0.2027, acc_train: 1.0000\n",
      "[Train] batch: 095, loss_train: 0.3929, acc_train: 0.8600\n",
      "[Train] batch: 096, loss_train: 0.4857, acc_train: 0.7171\n",
      "[Train] batch: 097, loss_train: 0.3839, acc_train: 0.7600\n",
      "[Train] batch: 098, loss_train: 0.2521, acc_train: 0.8571\n",
      "[Train] batch: 099, loss_train: 0.3060, acc_train: 0.8667\n",
      "[Train] batch: 100, loss_train: 0.4641, acc_train: 0.7200\n",
      "[Train] batch: 101, loss_train: 0.0960, acc_train: 1.0000\n",
      "[Train] batch: 102, loss_train: 0.0703, acc_train: 1.0000\n",
      "[Train] batch: 103, loss_train: 0.1645, acc_train: 1.0000\n",
      "[Train] batch: 104, loss_train: 0.3480, acc_train: 0.8667\n",
      "[Train] batch: 105, loss_train: 0.7217, acc_train: 0.6867\n",
      "[Train] batch: 106, loss_train: 0.4288, acc_train: 0.8400\n",
      "[Train] batch: 107, loss_train: 0.5527, acc_train: 0.7600\n",
      "[Train] batch: 108, loss_train: 0.1216, acc_train: 1.0000\n",
      "[Train] batch: 109, loss_train: 0.2689, acc_train: 0.8667\n",
      "[Train] batch: 110, loss_train: 0.2247, acc_train: 0.8556\n",
      "[Train] batch: 111, loss_train: 0.7042, acc_train: 0.6667\n",
      "[Train] batch: 112, loss_train: 0.2254, acc_train: 0.7500\n",
      "[Train] batch: 113, loss_train: 0.4228, acc_train: 0.8933\n",
      "[Train] batch: 114, loss_train: 0.1916, acc_train: 0.8667\n",
      "[Train] batch: 115, loss_train: 0.2170, acc_train: 0.8571\n",
      "[Train] batch: 116, loss_train: 0.3175, acc_train: 0.7000\n",
      "[Train] batch: 117, loss_train: 0.3084, acc_train: 0.8500\n",
      "[Train] batch: 118, loss_train: 0.2829, acc_train: 0.8600\n",
      "[Train] batch: 119, loss_train: 0.7566, acc_train: 0.6600\n",
      "[Train] batch: 120, loss_train: 0.9059, acc_train: 0.8000\n",
      "[Train] batch: 121, loss_train: 0.5195, acc_train: 0.7333\n",
      "[Train] batch: 122, loss_train: 1.0033, acc_train: 0.4333\n",
      "[Train] batch: 123, loss_train: 0.3448, acc_train: 0.7238\n",
      "[Train] batch: 124, loss_train: 0.3473, acc_train: 0.8667\n",
      "[Train] batch: 125, loss_train: 0.2222, acc_train: 0.9000\n",
      "[Train] batch: 126, loss_train: 0.3617, acc_train: 0.7000\n",
      "[Train] batch: 127, loss_train: 0.5712, acc_train: 0.5333\n",
      "[Train] batch: 128, loss_train: 0.0335, acc_train: 1.0000\n",
      "[Train] batch: 129, loss_train: 0.3564, acc_train: 0.7222\n",
      "[Train] batch: 130, loss_train: 0.4835, acc_train: 0.8000\n",
      "[Train] batch: 131, loss_train: 0.0853, acc_train: 1.0000\n",
      "[Train] batch: 132, loss_train: 1.0761, acc_train: 0.3167\n",
      "[Train] batch: 133, loss_train: 0.3351, acc_train: 0.8500\n",
      "[Train] batch: 134, loss_train: 0.1480, acc_train: 1.0000\n",
      "[Val] loss= 0.4366, accuracy= 0.7961, label acc= [0.07142857 0.89795918 0.90566038 0.92307692 1.         0.90196078\n",
      " 0.87272727]\n",
      "Epoch: 7\n",
      "[Train] batch: 001, loss_train: 0.5160, acc_train: 0.6467\n",
      "[Train] batch: 002, loss_train: 0.0976, acc_train: 1.0000\n",
      "[Train] batch: 003, loss_train: 0.5721, acc_train: 0.8000\n",
      "[Train] batch: 004, loss_train: 0.2423, acc_train: 0.8600\n",
      "[Train] batch: 005, loss_train: 0.3346, acc_train: 0.8600\n",
      "[Train] batch: 006, loss_train: 1.0161, acc_train: 0.6267\n",
      "[Train] batch: 007, loss_train: 0.8965, acc_train: 0.5167\n",
      "[Train] batch: 008, loss_train: 0.3539, acc_train: 0.8600\n",
      "[Train] batch: 009, loss_train: 0.2818, acc_train: 0.8889\n",
      "[Train] batch: 010, loss_train: 0.6955, acc_train: 0.9333\n",
      "[Train] batch: 011, loss_train: 0.5523, acc_train: 0.6233\n",
      "[Train] batch: 012, loss_train: 0.2168, acc_train: 0.8600\n",
      "[Train] batch: 013, loss_train: 0.2013, acc_train: 1.0000\n",
      "[Train] batch: 014, loss_train: 0.2242, acc_train: 0.9000\n",
      "[Train] batch: 015, loss_train: 0.4187, acc_train: 0.6771\n",
      "[Train] batch: 016, loss_train: 0.2714, acc_train: 0.9000\n",
      "[Train] batch: 017, loss_train: 0.2885, acc_train: 0.8667\n",
      "[Train] batch: 018, loss_train: 0.3955, acc_train: 0.6800\n",
      "[Train] batch: 019, loss_train: 0.1702, acc_train: 1.0000\n",
      "[Train] batch: 020, loss_train: 0.3259, acc_train: 0.7500\n",
      "[Train] batch: 021, loss_train: 0.3997, acc_train: 0.7500\n",
      "[Train] batch: 022, loss_train: 0.4247, acc_train: 0.7267\n",
      "[Train] batch: 023, loss_train: 0.3618, acc_train: 0.8667\n",
      "[Train] batch: 024, loss_train: 0.6650, acc_train: 0.6100\n",
      "[Train] batch: 025, loss_train: 0.1198, acc_train: 1.0000\n",
      "[Train] batch: 026, loss_train: 0.2355, acc_train: 0.8600\n",
      "[Train] batch: 027, loss_train: 0.2244, acc_train: 0.8600\n",
      "[Train] batch: 028, loss_train: 0.1524, acc_train: 1.0000\n",
      "[Train] batch: 029, loss_train: 0.5774, acc_train: 0.7500\n",
      "[Train] batch: 030, loss_train: 0.2914, acc_train: 0.8667\n",
      "[Train] batch: 031, loss_train: 0.3883, acc_train: 0.7250\n",
      "[Train] batch: 032, loss_train: 0.1995, acc_train: 0.8600\n",
      "[Train] batch: 033, loss_train: 0.1544, acc_train: 1.0000\n",
      "[Train] batch: 034, loss_train: 0.0971, acc_train: 1.0000\n",
      "[Train] batch: 035, loss_train: 0.2698, acc_train: 0.8600\n",
      "[Train] batch: 036, loss_train: 0.0547, acc_train: 1.0000\n",
      "[Train] batch: 037, loss_train: 0.0768, acc_train: 1.0000\n",
      "[Train] batch: 038, loss_train: 0.6484, acc_train: 0.7238\n",
      "[Train] batch: 039, loss_train: 0.9275, acc_train: 0.5933\n",
      "[Train] batch: 040, loss_train: 0.3692, acc_train: 0.9000\n",
      "[Train] batch: 041, loss_train: 0.0826, acc_train: 1.0000\n",
      "[Train] batch: 042, loss_train: 0.5710, acc_train: 0.6000\n",
      "[Train] batch: 043, loss_train: 0.0620, acc_train: 1.0000\n",
      "[Train] batch: 044, loss_train: 0.2010, acc_train: 0.9400\n",
      "[Train] batch: 045, loss_train: 0.3480, acc_train: 0.9000\n",
      "[Train] batch: 046, loss_train: 0.1361, acc_train: 1.0000\n",
      "[Train] batch: 047, loss_train: 0.4472, acc_train: 0.8733\n",
      "[Train] batch: 048, loss_train: 0.2367, acc_train: 0.9000\n",
      "[Train] batch: 049, loss_train: 0.5884, acc_train: 0.7200\n",
      "[Train] batch: 050, loss_train: 0.3103, acc_train: 0.8029\n",
      "[Train] batch: 051, loss_train: 0.0697, acc_train: 1.0000\n",
      "[Train] batch: 052, loss_train: 0.2703, acc_train: 0.8000\n",
      "[Train] batch: 053, loss_train: 0.3735, acc_train: 0.8667\n",
      "[Train] batch: 054, loss_train: 0.3364, acc_train: 0.7500\n",
      "[Train] batch: 055, loss_train: 0.3179, acc_train: 0.8667\n",
      "[Train] batch: 056, loss_train: 0.1419, acc_train: 0.9000\n",
      "[Train] batch: 057, loss_train: 0.2759, acc_train: 0.9429\n",
      "[Train] batch: 058, loss_train: 0.6183, acc_train: 0.7667\n",
      "[Train] batch: 059, loss_train: 0.2979, acc_train: 0.9333\n",
      "[Train] batch: 060, loss_train: 0.2769, acc_train: 0.8667\n",
      "[Train] batch: 061, loss_train: 0.2323, acc_train: 0.8556\n",
      "[Train] batch: 062, loss_train: 0.1335, acc_train: 1.0000\n",
      "[Train] batch: 063, loss_train: 0.3128, acc_train: 0.7667\n",
      "[Train] batch: 064, loss_train: 0.5575, acc_train: 0.6267\n",
      "[Train] batch: 065, loss_train: 0.0867, acc_train: 1.0000\n",
      "[Train] batch: 066, loss_train: 0.5067, acc_train: 0.7171\n",
      "[Train] batch: 067, loss_train: 0.3133, acc_train: 0.8600\n",
      "[Train] batch: 068, loss_train: 1.0363, acc_train: 0.6267\n",
      "[Train] batch: 069, loss_train: 0.2307, acc_train: 0.9000\n",
      "[Train] batch: 070, loss_train: 1.0028, acc_train: 0.6238\n",
      "[Train] batch: 071, loss_train: 0.6665, acc_train: 0.6533\n",
      "[Train] batch: 072, loss_train: 0.4130, acc_train: 0.7250\n",
      "[Train] batch: 073, loss_train: 0.7093, acc_train: 0.7600\n",
      "[Train] batch: 074, loss_train: 0.2067, acc_train: 0.9333\n",
      "[Train] batch: 075, loss_train: 0.7918, acc_train: 0.6095\n",
      "[Train] batch: 076, loss_train: 0.4032, acc_train: 0.8500\n",
      "[Train] batch: 077, loss_train: 0.5261, acc_train: 0.7933\n",
      "[Train] batch: 078, loss_train: 0.2724, acc_train: 0.8600\n",
      "[Train] batch: 079, loss_train: 0.5504, acc_train: 0.7571\n",
      "[Train] batch: 080, loss_train: 0.5212, acc_train: 0.6267\n",
      "[Train] batch: 081, loss_train: 0.1541, acc_train: 1.0000\n",
      "[Train] batch: 082, loss_train: 0.6128, acc_train: 0.6400\n",
      "[Train] batch: 083, loss_train: 0.8781, acc_train: 0.7767\n",
      "[Train] batch: 084, loss_train: 0.8078, acc_train: 0.7500\n",
      "[Train] batch: 085, loss_train: 0.2808, acc_train: 1.0000\n",
      "[Train] batch: 086, loss_train: 0.0777, acc_train: 1.0000\n",
      "[Train] batch: 087, loss_train: 0.3776, acc_train: 0.8667\n",
      "[Train] batch: 088, loss_train: 0.3517, acc_train: 0.9067\n",
      "[Train] batch: 089, loss_train: 0.5051, acc_train: 0.8556\n",
      "[Train] batch: 090, loss_train: 0.0752, acc_train: 1.0000\n",
      "[Train] batch: 091, loss_train: 0.6615, acc_train: 0.7267\n",
      "[Train] batch: 092, loss_train: 0.1617, acc_train: 1.0000\n",
      "[Train] batch: 093, loss_train: 0.9737, acc_train: 0.4000\n",
      "[Train] batch: 094, loss_train: 0.1097, acc_train: 1.0000\n",
      "[Train] batch: 095, loss_train: 0.3398, acc_train: 0.9000\n",
      "[Train] batch: 096, loss_train: 0.4620, acc_train: 0.8600\n",
      "[Train] batch: 097, loss_train: 0.6255, acc_train: 0.8000\n",
      "[Train] batch: 098, loss_train: 0.4030, acc_train: 0.8000\n",
      "[Train] batch: 099, loss_train: 0.6181, acc_train: 0.6600\n",
      "[Train] batch: 100, loss_train: 0.2696, acc_train: 0.8667\n",
      "[Train] batch: 101, loss_train: 1.0308, acc_train: 0.4750\n",
      "[Train] batch: 102, loss_train: 0.0972, acc_train: 1.0000\n",
      "[Train] batch: 103, loss_train: 0.3759, acc_train: 0.6600\n",
      "[Train] batch: 104, loss_train: 0.3369, acc_train: 0.7333\n",
      "[Train] batch: 105, loss_train: 0.4663, acc_train: 0.7500\n",
      "[Train] batch: 106, loss_train: 0.5179, acc_train: 0.6400\n",
      "[Train] batch: 107, loss_train: 0.3630, acc_train: 0.8167\n",
      "[Train] batch: 108, loss_train: 0.3386, acc_train: 0.8000\n",
      "[Train] batch: 109, loss_train: 0.1771, acc_train: 1.0000\n",
      "[Train] batch: 110, loss_train: 0.2891, acc_train: 0.9400\n",
      "[Train] batch: 111, loss_train: 0.4511, acc_train: 0.6733\n",
      "[Train] batch: 112, loss_train: 0.5311, acc_train: 0.9000\n",
      "[Train] batch: 113, loss_train: 0.1745, acc_train: 1.0000\n",
      "[Train] batch: 114, loss_train: 0.1772, acc_train: 0.9000\n",
      "[Train] batch: 115, loss_train: 0.8511, acc_train: 0.6100\n",
      "[Train] batch: 116, loss_train: 0.7229, acc_train: 0.5267\n",
      "[Train] batch: 117, loss_train: 0.5227, acc_train: 0.8933\n",
      "[Train] batch: 118, loss_train: 0.4786, acc_train: 0.5667\n",
      "[Train] batch: 119, loss_train: 0.3441, acc_train: 0.9000\n",
      "[Train] batch: 120, loss_train: 0.2647, acc_train: 1.0000\n",
      "[Train] batch: 121, loss_train: 0.5844, acc_train: 0.7267\n",
      "[Train] batch: 122, loss_train: 0.1583, acc_train: 1.0000\n",
      "[Train] batch: 123, loss_train: 0.1945, acc_train: 0.8667\n",
      "[Train] batch: 124, loss_train: 0.0623, acc_train: 1.0000\n",
      "[Train] batch: 125, loss_train: 0.9993, acc_train: 0.6500\n",
      "[Train] batch: 126, loss_train: 0.4928, acc_train: 0.8400\n",
      "[Train] batch: 127, loss_train: 0.2093, acc_train: 0.8667\n",
      "[Train] batch: 128, loss_train: 0.7200, acc_train: 0.7700\n",
      "[Train] batch: 129, loss_train: 0.1999, acc_train: 1.0000\n",
      "[Train] batch: 130, loss_train: 0.6476, acc_train: 0.8600\n",
      "[Train] batch: 131, loss_train: 0.1945, acc_train: 1.0000\n",
      "[Train] batch: 132, loss_train: 0.7918, acc_train: 0.7556\n",
      "[Train] batch: 133, loss_train: 0.3858, acc_train: 0.9000\n",
      "[Train] batch: 134, loss_train: 0.4630, acc_train: 0.8000\n",
      "[Val] loss= 0.4368, accuracy= 0.7885, label acc= [0.07407407 0.89795918 0.8627451  0.92307692 1.         0.88888889\n",
      " 0.87272727]\n",
      "Epoch: 8\n",
      "[Train] batch: 001, loss_train: 0.5130, acc_train: 0.6667\n",
      "[Train] batch: 002, loss_train: 0.7097, acc_train: 0.6500\n",
      "[Train] batch: 003, loss_train: 0.3678, acc_train: 0.9000\n",
      "[Train] batch: 004, loss_train: 0.5617, acc_train: 0.5571\n",
      "[Train] batch: 005, loss_train: 0.7505, acc_train: 0.7000\n",
      "[Train] batch: 006, loss_train: 0.0720, acc_train: 1.0000\n",
      "[Train] batch: 007, loss_train: 0.2315, acc_train: 0.8571\n",
      "[Train] batch: 008, loss_train: 0.4968, acc_train: 0.7267\n",
      "[Train] batch: 009, loss_train: 0.4918, acc_train: 0.7222\n",
      "[Train] batch: 010, loss_train: 0.5600, acc_train: 0.7267\n",
      "[Train] batch: 011, loss_train: 0.3283, acc_train: 0.8667\n",
      "[Train] batch: 012, loss_train: 0.2668, acc_train: 0.9333\n",
      "[Train] batch: 013, loss_train: 0.4383, acc_train: 0.7767\n",
      "[Train] batch: 014, loss_train: 0.2471, acc_train: 1.0000\n",
      "[Train] batch: 015, loss_train: 0.3082, acc_train: 0.8571\n",
      "[Train] batch: 016, loss_train: 0.3219, acc_train: 0.8571\n",
      "[Train] batch: 017, loss_train: 0.1912, acc_train: 1.0000\n",
      "[Train] batch: 018, loss_train: 0.3263, acc_train: 0.8667\n",
      "[Train] batch: 019, loss_train: 0.1905, acc_train: 0.9000\n",
      "[Train] batch: 020, loss_train: 0.3447, acc_train: 0.7833\n",
      "[Train] batch: 021, loss_train: 0.5072, acc_train: 0.7400\n",
      "[Train] batch: 022, loss_train: 0.2859, acc_train: 0.9400\n",
      "[Train] batch: 023, loss_train: 0.1980, acc_train: 0.9000\n",
      "[Train] batch: 024, loss_train: 0.4018, acc_train: 0.7267\n",
      "[Train] batch: 025, loss_train: 0.4225, acc_train: 0.7500\n",
      "[Train] batch: 026, loss_train: 0.1758, acc_train: 0.9000\n",
      "[Train] batch: 027, loss_train: 0.8312, acc_train: 0.7200\n",
      "[Train] batch: 028, loss_train: 0.0946, acc_train: 0.9000\n",
      "[Train] batch: 029, loss_train: 0.7294, acc_train: 0.6600\n",
      "[Train] batch: 030, loss_train: 0.4193, acc_train: 0.8667\n",
      "[Train] batch: 031, loss_train: 0.3109, acc_train: 0.8667\n",
      "[Train] batch: 032, loss_train: 0.1363, acc_train: 0.9000\n",
      "[Train] batch: 033, loss_train: 0.0896, acc_train: 0.9000\n",
      "[Train] batch: 034, loss_train: 0.0517, acc_train: 1.0000\n",
      "[Train] batch: 035, loss_train: 0.4059, acc_train: 0.7267\n",
      "[Train] batch: 036, loss_train: 0.4854, acc_train: 0.7571\n",
      "[Train] batch: 037, loss_train: 0.6651, acc_train: 0.5917\n",
      "[Train] batch: 038, loss_train: 0.2914, acc_train: 0.8933\n",
      "[Train] batch: 039, loss_train: 0.4667, acc_train: 0.6667\n",
      "[Train] batch: 040, loss_train: 0.5153, acc_train: 0.9000\n",
      "[Train] batch: 041, loss_train: 0.2457, acc_train: 1.0000\n",
      "[Train] batch: 042, loss_train: 0.1776, acc_train: 0.9333\n",
      "[Train] batch: 043, loss_train: 0.3160, acc_train: 0.8600\n",
      "[Train] batch: 044, loss_train: 0.3354, acc_train: 0.7500\n",
      "[Train] batch: 045, loss_train: 0.1901, acc_train: 0.9333\n",
      "[Train] batch: 046, loss_train: 0.9659, acc_train: 0.6333\n",
      "[Train] batch: 047, loss_train: 0.4673, acc_train: 0.6733\n",
      "[Train] batch: 048, loss_train: 0.3233, acc_train: 0.7500\n",
      "[Train] batch: 049, loss_train: 0.2134, acc_train: 0.9400\n",
      "[Train] batch: 050, loss_train: 0.2487, acc_train: 0.9000\n",
      "[Train] batch: 051, loss_train: 0.2028, acc_train: 0.8600\n",
      "[Train] batch: 052, loss_train: 0.1877, acc_train: 0.7833\n",
      "[Train] batch: 053, loss_train: 0.3417, acc_train: 0.9333\n",
      "[Train] batch: 054, loss_train: 0.5744, acc_train: 0.6667\n",
      "[Train] batch: 055, loss_train: 0.5103, acc_train: 0.7571\n",
      "[Train] batch: 056, loss_train: 0.5558, acc_train: 0.8100\n",
      "[Train] batch: 057, loss_train: 0.5164, acc_train: 0.6267\n",
      "[Train] batch: 058, loss_train: 0.2595, acc_train: 1.0000\n",
      "[Train] batch: 059, loss_train: 0.2336, acc_train: 0.8556\n",
      "[Train] batch: 060, loss_train: 0.3233, acc_train: 0.8667\n",
      "[Train] batch: 061, loss_train: 0.2478, acc_train: 0.8667\n",
      "[Train] batch: 062, loss_train: 0.2463, acc_train: 0.9000\n",
      "[Train] batch: 063, loss_train: 0.9033, acc_train: 0.5867\n",
      "[Train] batch: 064, loss_train: 0.1622, acc_train: 0.9333\n",
      "[Train] batch: 065, loss_train: 0.5284, acc_train: 0.8600\n",
      "[Train] batch: 066, loss_train: 1.2199, acc_train: 0.4767\n",
      "[Train] batch: 067, loss_train: 0.8929, acc_train: 0.7667\n",
      "[Train] batch: 068, loss_train: 0.4685, acc_train: 0.7267\n",
      "[Train] batch: 069, loss_train: 0.1897, acc_train: 0.8600\n",
      "[Train] batch: 070, loss_train: 0.3320, acc_train: 0.8667\n",
      "[Train] batch: 071, loss_train: 0.3277, acc_train: 0.8571\n",
      "[Train] batch: 072, loss_train: 0.3079, acc_train: 0.8600\n",
      "[Train] batch: 073, loss_train: 0.1196, acc_train: 1.0000\n",
      "[Train] batch: 074, loss_train: 0.8490, acc_train: 0.6100\n",
      "[Train] batch: 075, loss_train: 0.5133, acc_train: 0.6267\n",
      "[Train] batch: 076, loss_train: 0.1808, acc_train: 1.0000\n",
      "[Train] batch: 077, loss_train: 0.3516, acc_train: 0.7267\n",
      "[Train] batch: 078, loss_train: 0.5507, acc_train: 0.4333\n",
      "[Train] batch: 079, loss_train: 0.2440, acc_train: 0.8000\n",
      "[Train] batch: 080, loss_train: 0.3393, acc_train: 0.8000\n",
      "[Train] batch: 081, loss_train: 0.5086, acc_train: 0.6000\n",
      "[Train] batch: 082, loss_train: 0.3513, acc_train: 0.9000\n",
      "[Train] batch: 083, loss_train: 0.5253, acc_train: 0.7333\n",
      "[Train] batch: 084, loss_train: 1.1252, acc_train: 0.4500\n",
      "[Train] batch: 085, loss_train: 0.1959, acc_train: 1.0000\n",
      "[Train] batch: 086, loss_train: 0.4978, acc_train: 0.8667\n",
      "[Train] batch: 087, loss_train: 0.1946, acc_train: 1.0000\n",
      "[Train] batch: 088, loss_train: 0.6332, acc_train: 0.6400\n",
      "[Train] batch: 089, loss_train: 0.6593, acc_train: 0.6667\n",
      "[Train] batch: 090, loss_train: 0.2171, acc_train: 1.0000\n",
      "[Train] batch: 091, loss_train: 0.1046, acc_train: 1.0000\n",
      "[Train] batch: 092, loss_train: 0.3121, acc_train: 0.7556\n",
      "[Train] batch: 093, loss_train: 0.6397, acc_train: 0.7333\n",
      "[Train] batch: 094, loss_train: 0.0853, acc_train: 1.0000\n",
      "[Train] batch: 095, loss_train: 0.7699, acc_train: 0.5267\n",
      "[Train] batch: 096, loss_train: 0.8417, acc_train: 0.7571\n",
      "[Train] batch: 097, loss_train: 0.4126, acc_train: 0.7600\n",
      "[Train] batch: 098, loss_train: 0.5788, acc_train: 0.8333\n",
      "[Train] batch: 099, loss_train: 0.2767, acc_train: 0.8600\n",
      "[Train] batch: 100, loss_train: 0.9147, acc_train: 0.4767\n",
      "[Train] batch: 101, loss_train: 0.2305, acc_train: 0.9333\n",
      "[Train] batch: 102, loss_train: 0.4576, acc_train: 0.8933\n",
      "[Train] batch: 103, loss_train: 0.3122, acc_train: 0.9333\n",
      "[Train] batch: 104, loss_train: 0.4759, acc_train: 0.9000\n",
      "[Train] batch: 105, loss_train: 0.5540, acc_train: 0.7000\n",
      "[Train] batch: 106, loss_train: 0.1085, acc_train: 1.0000\n",
      "[Train] batch: 107, loss_train: 0.2764, acc_train: 0.8600\n",
      "[Train] batch: 108, loss_train: 0.4524, acc_train: 0.7333\n",
      "[Train] batch: 109, loss_train: 0.6066, acc_train: 0.6056\n",
      "[Train] batch: 110, loss_train: 0.1513, acc_train: 1.0000\n",
      "[Train] batch: 111, loss_train: 0.1156, acc_train: 1.0000\n",
      "[Train] batch: 112, loss_train: 0.4486, acc_train: 0.7667\n",
      "[Train] batch: 113, loss_train: 0.1705, acc_train: 0.8556\n",
      "[Train] batch: 114, loss_train: 0.1130, acc_train: 1.0000\n",
      "[Train] batch: 115, loss_train: 0.2656, acc_train: 0.8667\n",
      "[Train] batch: 116, loss_train: 0.1964, acc_train: 0.8556\n",
      "[Train] batch: 117, loss_train: 0.3224, acc_train: 0.7333\n",
      "[Train] batch: 118, loss_train: 0.1124, acc_train: 1.0000\n",
      "[Train] batch: 119, loss_train: 0.6217, acc_train: 0.9333\n",
      "[Train] batch: 120, loss_train: 0.8248, acc_train: 0.5267\n",
      "[Train] batch: 121, loss_train: 0.4855, acc_train: 0.7200\n",
      "[Train] batch: 122, loss_train: 0.7033, acc_train: 0.7429\n",
      "[Train] batch: 123, loss_train: 0.4423, acc_train: 0.7667\n",
      "[Train] batch: 124, loss_train: 0.1717, acc_train: 1.0000\n",
      "[Train] batch: 125, loss_train: 0.3125, acc_train: 0.8571\n",
      "[Train] batch: 126, loss_train: 0.4722, acc_train: 0.8000\n",
      "[Train] batch: 127, loss_train: 0.2852, acc_train: 1.0000\n",
      "[Train] batch: 128, loss_train: 0.2768, acc_train: 0.8571\n",
      "[Train] batch: 129, loss_train: 0.3382, acc_train: 0.8667\n",
      "[Train] batch: 130, loss_train: 0.1521, acc_train: 1.0000\n",
      "[Train] batch: 131, loss_train: 0.4088, acc_train: 0.8571\n",
      "[Train] batch: 132, loss_train: 0.4120, acc_train: 0.8067\n",
      "[Train] batch: 133, loss_train: 0.0624, acc_train: 1.0000\n",
      "[Train] batch: 134, loss_train: 0.3268, acc_train: 0.8667\n",
      "[Val] loss= 0.4374, accuracy= 0.7927, label acc= [0.         0.94117647 0.88888889 0.92307692 1.         0.92307692\n",
      " 0.87272727]\n",
      "Epoch: 9\n",
      "[Train] batch: 001, loss_train: 0.2886, acc_train: 0.8600\n",
      "[Train] batch: 002, loss_train: 0.1611, acc_train: 0.8600\n",
      "[Train] batch: 003, loss_train: 0.7196, acc_train: 0.8000\n",
      "[Train] batch: 004, loss_train: 0.0625, acc_train: 1.0000\n",
      "[Train] batch: 005, loss_train: 0.4461, acc_train: 0.7600\n",
      "[Train] batch: 006, loss_train: 0.4029, acc_train: 0.8571\n",
      "[Train] batch: 007, loss_train: 0.8669, acc_train: 0.6333\n",
      "[Train] batch: 008, loss_train: 0.3875, acc_train: 0.8600\n",
      "[Train] batch: 009, loss_train: 0.1406, acc_train: 1.0000\n",
      "[Train] batch: 010, loss_train: 0.1671, acc_train: 0.9000\n",
      "[Train] batch: 011, loss_train: 0.2761, acc_train: 0.9000\n",
      "[Train] batch: 012, loss_train: 0.5097, acc_train: 0.6500\n",
      "[Train] batch: 013, loss_train: 0.5430, acc_train: 0.7833\n",
      "[Train] batch: 014, loss_train: 0.2240, acc_train: 1.0000\n",
      "[Train] batch: 015, loss_train: 0.4735, acc_train: 0.7267\n",
      "[Train] batch: 016, loss_train: 0.5352, acc_train: 0.9000\n",
      "[Train] batch: 017, loss_train: 0.2848, acc_train: 0.8600\n",
      "[Train] batch: 018, loss_train: 0.3164, acc_train: 0.8600\n",
      "[Train] batch: 019, loss_train: 0.4642, acc_train: 0.7500\n",
      "[Train] batch: 020, loss_train: 0.2926, acc_train: 0.7000\n",
      "[Train] batch: 021, loss_train: 0.2294, acc_train: 0.8000\n",
      "[Train] batch: 022, loss_train: 0.2711, acc_train: 0.8571\n",
      "[Train] batch: 023, loss_train: 0.6030, acc_train: 0.4833\n",
      "[Train] batch: 024, loss_train: 0.7426, acc_train: 0.4833\n",
      "[Train] batch: 025, loss_train: 0.2140, acc_train: 0.8600\n",
      "[Train] batch: 026, loss_train: 0.2300, acc_train: 0.8571\n",
      "[Train] batch: 027, loss_train: 0.2329, acc_train: 0.8667\n",
      "[Train] batch: 028, loss_train: 0.2442, acc_train: 0.9333\n",
      "[Train] batch: 029, loss_train: 0.4926, acc_train: 0.6167\n",
      "[Train] batch: 030, loss_train: 0.2527, acc_train: 0.9333\n",
      "[Train] batch: 031, loss_train: 0.2887, acc_train: 0.8571\n",
      "[Train] batch: 032, loss_train: 0.4485, acc_train: 0.8667\n",
      "[Train] batch: 033, loss_train: 0.3401, acc_train: 0.8667\n",
      "[Train] batch: 034, loss_train: 0.1856, acc_train: 1.0000\n",
      "[Train] batch: 035, loss_train: 0.3978, acc_train: 0.7833\n",
      "[Train] batch: 036, loss_train: 0.2436, acc_train: 0.8000\n",
      "[Train] batch: 037, loss_train: 0.3943, acc_train: 0.7267\n",
      "[Train] batch: 038, loss_train: 0.4578, acc_train: 0.7238\n",
      "[Train] batch: 039, loss_train: 0.6668, acc_train: 0.8000\n",
      "[Train] batch: 040, loss_train: 0.1472, acc_train: 1.0000\n",
      "[Train] batch: 041, loss_train: 0.3916, acc_train: 0.7267\n",
      "[Train] batch: 042, loss_train: 0.2302, acc_train: 0.8667\n",
      "[Train] batch: 043, loss_train: 0.0984, acc_train: 1.0000\n",
      "[Train] batch: 044, loss_train: 0.0893, acc_train: 1.0000\n",
      "[Train] batch: 045, loss_train: 0.5515, acc_train: 0.7571\n",
      "[Train] batch: 046, loss_train: 0.0980, acc_train: 1.0000\n",
      "[Train] batch: 047, loss_train: 0.3892, acc_train: 0.7333\n",
      "[Train] batch: 048, loss_train: 0.4362, acc_train: 0.7267\n",
      "[Train] batch: 049, loss_train: 0.3274, acc_train: 0.9000\n",
      "[Train] batch: 050, loss_train: 0.1261, acc_train: 0.8667\n",
      "[Train] batch: 051, loss_train: 0.1505, acc_train: 0.9000\n",
      "[Train] batch: 052, loss_train: 0.9505, acc_train: 0.4267\n",
      "[Train] batch: 053, loss_train: 0.2462, acc_train: 0.8667\n",
      "[Train] batch: 054, loss_train: 0.4045, acc_train: 0.8600\n",
      "[Train] batch: 055, loss_train: 0.5280, acc_train: 0.8100\n",
      "[Train] batch: 056, loss_train: 0.4877, acc_train: 0.6505\n",
      "[Train] batch: 057, loss_train: 0.2776, acc_train: 0.9333\n",
      "[Train] batch: 058, loss_train: 0.2767, acc_train: 0.8667\n",
      "[Train] batch: 059, loss_train: 0.5358, acc_train: 0.6600\n",
      "[Train] batch: 060, loss_train: 0.5966, acc_train: 0.8167\n",
      "[Train] batch: 061, loss_train: 0.3618, acc_train: 0.9333\n",
      "[Train] batch: 062, loss_train: 0.3510, acc_train: 0.8571\n",
      "[Train] batch: 063, loss_train: 0.5554, acc_train: 0.6500\n",
      "[Train] batch: 064, loss_train: 0.2749, acc_train: 1.0000\n",
      "[Train] batch: 065, loss_train: 0.3604, acc_train: 0.9400\n",
      "[Train] batch: 066, loss_train: 0.2387, acc_train: 1.0000\n",
      "[Train] batch: 067, loss_train: 0.4594, acc_train: 0.8667\n",
      "[Train] batch: 068, loss_train: 0.3978, acc_train: 0.7333\n",
      "[Train] batch: 069, loss_train: 0.3269, acc_train: 0.8600\n",
      "[Train] batch: 070, loss_train: 0.3428, acc_train: 0.7238\n",
      "[Train] batch: 071, loss_train: 0.4715, acc_train: 0.8029\n",
      "[Train] batch: 072, loss_train: 0.3541, acc_train: 0.8600\n",
      "[Train] batch: 073, loss_train: 0.0859, acc_train: 1.0000\n",
      "[Train] batch: 074, loss_train: 1.1981, acc_train: 0.2643\n",
      "[Train] batch: 075, loss_train: 0.9993, acc_train: 0.5267\n",
      "[Train] batch: 076, loss_train: 0.3750, acc_train: 0.8571\n",
      "[Train] batch: 077, loss_train: 0.3009, acc_train: 0.9000\n",
      "[Train] batch: 078, loss_train: 0.2471, acc_train: 0.9400\n",
      "[Train] batch: 079, loss_train: 0.2920, acc_train: 0.9400\n",
      "[Train] batch: 080, loss_train: 0.2851, acc_train: 0.9000\n",
      "[Train] batch: 081, loss_train: 0.3028, acc_train: 0.8667\n",
      "[Train] batch: 082, loss_train: 0.6029, acc_train: 0.4767\n",
      "[Train] batch: 083, loss_train: 0.2974, acc_train: 0.9000\n",
      "[Train] batch: 084, loss_train: 0.2438, acc_train: 1.0000\n",
      "[Train] batch: 085, loss_train: 0.1094, acc_train: 1.0000\n",
      "[Train] batch: 086, loss_train: 0.1295, acc_train: 1.0000\n",
      "[Train] batch: 087, loss_train: 0.1759, acc_train: 1.0000\n",
      "[Train] batch: 088, loss_train: 0.5626, acc_train: 0.6667\n",
      "[Train] batch: 089, loss_train: 0.6055, acc_train: 0.6333\n",
      "[Train] batch: 090, loss_train: 0.2410, acc_train: 0.8600\n",
      "[Train] batch: 091, loss_train: 0.7845, acc_train: 0.7233\n",
      "[Train] batch: 092, loss_train: 0.2216, acc_train: 0.8667\n",
      "[Train] batch: 093, loss_train: 0.1338, acc_train: 0.8667\n",
      "[Train] batch: 094, loss_train: 0.0429, acc_train: 1.0000\n",
      "[Train] batch: 095, loss_train: 0.2464, acc_train: 0.8600\n",
      "[Train] batch: 096, loss_train: 0.5934, acc_train: 0.7600\n",
      "[Train] batch: 097, loss_train: 0.2442, acc_train: 0.8600\n",
      "[Train] batch: 098, loss_train: 0.2599, acc_train: 0.9000\n",
      "[Train] batch: 099, loss_train: 0.2800, acc_train: 0.8000\n",
      "[Train] batch: 100, loss_train: 0.6259, acc_train: 0.8095\n",
      "[Train] batch: 101, loss_train: 0.4472, acc_train: 0.7667\n",
      "[Train] batch: 102, loss_train: 0.5153, acc_train: 0.7667\n",
      "[Train] batch: 103, loss_train: 0.2289, acc_train: 0.8000\n",
      "[Train] batch: 104, loss_train: 0.7006, acc_train: 0.5667\n",
      "[Train] batch: 105, loss_train: 0.1415, acc_train: 0.9000\n",
      "[Train] batch: 106, loss_train: 0.4186, acc_train: 0.6400\n",
      "[Train] batch: 107, loss_train: 0.1560, acc_train: 0.8667\n",
      "[Train] batch: 108, loss_train: 0.5447, acc_train: 0.7333\n",
      "[Train] batch: 109, loss_train: 0.1775, acc_train: 0.8600\n",
      "[Train] batch: 110, loss_train: 1.2266, acc_train: 0.4733\n",
      "[Train] batch: 111, loss_train: 0.4648, acc_train: 0.7250\n",
      "[Train] batch: 112, loss_train: 0.4358, acc_train: 0.8000\n",
      "[Train] batch: 113, loss_train: 0.4868, acc_train: 0.8000\n",
      "[Train] batch: 114, loss_train: 0.4162, acc_train: 0.8000\n",
      "[Train] batch: 115, loss_train: 0.3175, acc_train: 0.8667\n",
      "[Train] batch: 116, loss_train: 0.5769, acc_train: 0.7333\n",
      "[Train] batch: 117, loss_train: 0.6470, acc_train: 0.7000\n",
      "[Train] batch: 118, loss_train: 0.2369, acc_train: 0.9095\n",
      "[Train] batch: 119, loss_train: 0.1294, acc_train: 1.0000\n",
      "[Train] batch: 120, loss_train: 0.7296, acc_train: 0.6100\n",
      "[Train] batch: 121, loss_train: 0.3615, acc_train: 0.8556\n",
      "[Train] batch: 122, loss_train: 0.2295, acc_train: 1.0000\n",
      "[Train] batch: 123, loss_train: 0.1597, acc_train: 1.0000\n",
      "[Train] batch: 124, loss_train: 0.2834, acc_train: 0.8600\n",
      "[Train] batch: 125, loss_train: 0.6680, acc_train: 0.5200\n",
      "[Train] batch: 126, loss_train: 0.0905, acc_train: 1.0000\n",
      "[Train] batch: 127, loss_train: 0.1858, acc_train: 0.9000\n",
      "[Train] batch: 128, loss_train: 0.3333, acc_train: 0.8667\n",
      "[Train] batch: 129, loss_train: 0.2743, acc_train: 0.9000\n",
      "[Train] batch: 130, loss_train: 0.3935, acc_train: 0.8571\n",
      "[Train] batch: 131, loss_train: 0.1346, acc_train: 1.0000\n",
      "[Train] batch: 132, loss_train: 0.2663, acc_train: 0.8333\n",
      "[Train] batch: 133, loss_train: 0.6544, acc_train: 0.7333\n",
      "[Train] batch: 134, loss_train: 0.2669, acc_train: 0.8600\n",
      "[Val] loss= 0.3847, accuracy= 0.7927, label acc= [0.         0.94117647 0.88888889 0.92307692 1.         0.92307692\n",
      " 0.87272727]\n",
      "[Test] loss= 0.4575, accuracy= 0.7868, label acc= [0.         0.89795918 0.87272727 0.94117647 1.         0.92307692\n",
      " 0.87272727]\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model and optimizer\n",
    "def load_model(model, optimizer, path, device='cpu'):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(f\"Model loaded from {path}, epoch {epoch}\")\n",
    "    return model, optimizer, epoch\n",
    "data = \"CSE-CIC\"\n",
    "\n",
    "# Update the path to use ../cyber_gnn/ instead of datasets/\n",
    "path = \"datasets/\" + data\n",
    "if not path.endswith('/'):\n",
    "    path += '/'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the data manually (edge_feat, label, adj, adj_lists, config)\n",
    "edge_feat = np.load(path + \"edge_feat_scaled.npy\", allow_pickle=True)\n",
    "edge_feat = torch.tensor(edge_feat, dtype=torch.float, device=device)\n",
    "\n",
    "# Load the label for multiclass classification\n",
    "label = np.load(path + \"label_mul.npy\", allow_pickle=True)\n",
    "label = torch.tensor(label, dtype=torch.long, device=device)\n",
    "adj = np.load(path + \"adj_random.npy\", allow_pickle=True)\n",
    "with open(path + 'adj_random_list.dict', 'rb') as file:\n",
    "    adj_lists = pickle.load(file)\n",
    "# Initialize the model and optimizer\n",
    "model = LLMGraphTransformer(model_name=\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\", device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "model, optimizer, start_epoch = load_model(model, optimizer, path=\"model/20241113-181010_llm_w_edgefeat.pth\", device=device)\n",
    "labels = ['Benign', 'BruteForce', 'DoS', 'DDoS', 'Web', 'Bot', 'Infilteration']\n",
    "\n",
    "# Define all indices in the dataset\n",
    "label_cpu = label.cpu().numpy()  # Assume 'label' is the tensor of actual labels\n",
    "\n",
    "# Define the indices of the entire dataset\n",
    "all_data_indices = np.arange(len(label_cpu))\n",
    "\n",
    "# Find unused data indices\n",
    "train, val, test, train_labels, val_labels, test_labels = load_data_splits()\n",
    "used_data_indices = np.array(train.tolist() + val.tolist() + test.tolist())\n",
    "unused_data_indices = np.setdiff1d(all_data_indices, used_data_indices)\n",
    "\n",
    "# Sample balanced data and labels from unused data only\n",
    "balanced_data, balanced_labels = balance_data(unused_data_indices, label_cpu[unused_data_indices], n_samples_per_label=240)\n",
    "\n",
    "# Split unused balanced data into train, val, and test sets\n",
    "unused_train, unused_temp, unused_train_labels, unused_temp_labels = train_test_split(\n",
    "    balanced_data, balanced_labels, test_size=0.2, stratify=balanced_labels, random_state=42\n",
    ")\n",
    "unused_val, unused_test, unused_val_labels, unused_test_labels = train_test_split(\n",
    "    unused_temp, unused_temp_labels, test_size=0.5, stratify=unused_temp_labels, random_state=42\n",
    ")\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(unused_train_labels), y=unused_train_labels)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Update CrossEntropyLoss with class weights\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "# Display split results\n",
    "print(f\"Unused Train Data: {len(unused_train)}, Validation: {len(unused_val)}, Test: {len(unused_test)}\")\n",
    "\n",
    "print(\"Label distribution in Unused Train Set:\", dict(zip(*np.unique(unused_train_labels, return_counts=True))))\n",
    "print(\"Label distribution in Unused Validation Set:\", dict(zip(*np.unique(unused_val_labels, return_counts=True))))\n",
    "print(\"Label distribution in Unused Test Set:\", dict(zip(*np.unique(unused_test_labels, return_counts=True))))\n",
    "\n",
    "# Training loop on unused data\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    print(\"Epoch:\", epoch)\n",
    "    unused_train, unused_train_labels = shuffle(unused_train, unused_train_labels, random_state=42)  # Shuffle together\n",
    "    \n",
    "    for batch in range(int(len(unused_train) / 10)):  # Batch size set to 10\n",
    "        batch_edges = unused_train[10 * batch:10 * (batch + 1)]\n",
    "        batch_labels = unused_train_labels[10 * batch:10 * (batch + 1)]\n",
    "        \n",
    "        # Generate predictions using model\n",
    "        batch_text = model.generate_text(batch_edges, labels, max_new_tokens=10)\n",
    "        edge_batch = edge_feat[batch_edges]\n",
    "        logits = model(batch_text, edge_batch).to(device)\n",
    "        \n",
    "        batch_labels_tensor = torch.tensor(batch_labels, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Calculate loss and backpropagate\n",
    "        loss = loss_fn(logits, batch_labels_tensor)\n",
    "        optimizer.zero_grad()  # Zero gradients before backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and print training accuracy\n",
    "        predicted_labels = torch.argmax(logits, dim=-1)\n",
    "        acc_train = f1_score(batch_labels, predicted_labels.cpu().numpy(), average=\"weighted\")\n",
    "        print(f'[Train] batch: {batch + 1:03d}, loss_train: {loss.item():.4f}, acc_train: {acc_train:.4f}')\n",
    "\n",
    "    # Validation after each epoch\n",
    "    model.eval()  # Set model to evaluation mode for validation\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        val_acc, val_loss, val_output = predict_(model, label, loss_fn, unused_val, device, edge_feat)\n",
    "        print(f\"[Val] loss= {val_loss:.4f}, accuracy= {val_acc:.4f}, label acc= {f1_score(unused_val_labels, val_output, average=None)}\")\n",
    "\n",
    "    \n",
    "\n",
    "# Final test evaluation\n",
    "model.eval()  # Set model to evaluation mode for testing\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    acc_test, loss_test, predict_output = predict_(model, label, loss_fn, unused_test, device, edge_feat)\n",
    "    print(f\"[Test] loss= {loss_test:.4f}, accuracy= {acc_test:.4f}, label acc= {f1_score(unused_test_labels, predict_output, average=None)}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splits and labels saved successfully.\n"
     ]
    }
   ],
   "source": [
    "def con1_save_data_splits(train, val, test, train_labels, val_labels, test_labels, path=\"data_splits/cse-cic\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    with open(os.path.join(path, \"con1_train.pkl\"), \"wb\") as f:\n",
    "        pickle.dump((train, train_labels), f)\n",
    "    with open(os.path.join(path, \"con1_val.pkl\"), \"wb\") as f:\n",
    "        pickle.dump((val, val_labels), f)\n",
    "    with open(os.path.join(path, \"con1_test.pkl\"), \"wb\") as f:\n",
    "        pickle.dump((test, test_labels), f)\n",
    "    print(\"Data splits and labels saved successfully.\")\n",
    "con1_save_data_splits(unused_train, unused_val, unused_test, unused_train_labels, unused_val_labels, unused_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1_load_data_splits(path=\"data_splits/cse-cic\"):\n",
    "    with open(os.path.join(path, \"con1_train.pkl\"), \"rb\") as f:\n",
    "        train, train_labels = pickle.load(f)\n",
    "    with open(os.path.join(path, \"con1_val.pkl\"), \"rb\") as f:\n",
    "        val, val_labels = pickle.load(f)\n",
    "    with open(os.path.join(path, \"con1_test.pkl\"), \"rb\") as f:\n",
    "        test, test_labels = pickle.load(f)\n",
    "    print(\"Data splits and labels loaded successfully.\")\n",
    "    return train, val, test, train_labels, val_labels, test_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to model/20241113-194017_llm_w_edgefeat.pth\n"
     ]
    }
   ],
   "source": [
    "save_model(model, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the model - phase 1 \n",
    "- training with 1360 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splits and labels loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_172663/3458605403.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model/20241113-181010_llm_w_edgefeat.pth, epoch 9\n",
      "Test set results: loss= 0.6059, accuracy= 0.8512, f1_score(weighted)= 0.7906\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       Benign       0.00      0.00      0.00        24\n",
      "   BruteForce       0.92      1.00      0.96        24\n",
      "          DoS       0.73      1.00      0.84        24\n",
      "         DDoS       0.89      1.00      0.94        24\n",
      "          Web       1.00      1.00      1.00        24\n",
      "          Bot       0.85      0.96      0.90        24\n",
      "Infilteration       0.80      1.00      0.89        24\n",
      "\n",
      "     accuracy                           0.85       168\n",
      "    macro avg       0.74      0.85      0.79       168\n",
      " weighted avg       0.74      0.85      0.79       168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "\n",
    "def load_model(model, optimizer, path, device='cpu'):\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    \n",
    "    # Load the state dictionaries for the model and optimizer\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # Retrieve the saved epoch number\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(f\"Model loaded from {path}, epoch {epoch}\")\n",
    "    \n",
    "    return model, optimizer, epoch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LLMGraphTransformer(model_name=\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\", device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "train, val, test, train_labels, val_labels, test_labels = load_data_splits()\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "\n",
    "\n",
    "# Specify the path to the saved model\n",
    "model_path = \"model/20241113-181010_llm_w_edgefeat.pth\"\n",
    "model, optimizer, start_epoch = load_model(model, optimizer, path=model_path, device=device)\n",
    "\n",
    "# Set device to CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Path setup\n",
    "path = \"datasets/CSE-CIC/\"\n",
    "if not path.endswith('/'):\n",
    "    path += '/'\n",
    "\n",
    "edge_feat = torch.tensor(np.load(path + \"edge_feat_scaled.npy\", allow_pickle=True), dtype=torch.float, device=device)\n",
    "label = torch.tensor(np.load(path + \"label_mul.npy\", allow_pickle=True), dtype=torch.long, device=device)\n",
    "\n",
    "# Labels for relationship types\n",
    "labels = ['Benign', 'BruteForce', 'DoS', 'DDoS', 'Web', 'Bot', 'Infilteration']\n",
    "\n",
    "# Test the model\n",
    "label_cpu = label.cpu().numpy()\n",
    "f1_test, loss_test, predict_output = predict_(model, label, loss_fn, test, device, edge_feat)\n",
    "\n",
    "# Calculate accuracy and F1 score for weighted average\n",
    "accuracy = accuracy_score(label_cpu[test], predict_output)\n",
    "f1_weighted = f1_score(label_cpu[test], predict_output, average=\"weighted\")\n",
    "report = classification_report(label_cpu[test], predict_output, target_names=labels)\n",
    "\n",
    "# Print test set results\n",
    "print(f\"Test set results: loss= {loss_test:.4f}, accuracy= {accuracy:.4f}, f1_score(weighted)= {f1_weighted:.4f}\")\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the model - phase 2\n",
    "- training with another 1360 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splits and labels loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_172663/1332990643.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model/20241113-194017_llm_w_edgefeat.pth, epoch 9\n",
      "Test set results: loss= 0.3917, accuracy= 0.8571, f1_score(weighted)= 0.7936\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       Benign       0.00      0.00      0.00        24\n",
      "   BruteForce       0.92      1.00      0.96        24\n",
      "          DoS       0.73      1.00      0.84        24\n",
      "         DDoS       0.89      1.00      0.94        24\n",
      "          Web       1.00      1.00      1.00        24\n",
      "          Bot       0.86      1.00      0.92        24\n",
      "Infilteration       0.80      1.00      0.89        24\n",
      "\n",
      "     accuracy                           0.86       168\n",
      "    macro avg       0.74      0.86      0.79       168\n",
      " weighted avg       0.74      0.86      0.79       168\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "\n",
    "def load_model(model, optimizer, path, device='cpu'):\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    \n",
    "    # Load the state dictionaries for the model and optimizer\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # Retrieve the saved epoch number\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(f\"Model loaded from {path}, epoch {epoch}\")\n",
    "    \n",
    "    return model, optimizer, epoch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LLMGraphTransformer(model_name=\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\", device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "train, val, test, train_labels, val_labels, test_labels = load_data_splits()\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "\n",
    "\n",
    "# Specify the path to the saved model\n",
    "model_path = \"model/20241113-194017_llm_w_edgefeat.pth\"\n",
    "model, optimizer, start_epoch = load_model(model, optimizer, path=model_path, device=device)\n",
    "\n",
    "# Set device to CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Path setup\n",
    "path = \"datasets/CSE-CIC/\"\n",
    "if not path.endswith('/'):\n",
    "    path += '/'\n",
    "\n",
    "edge_feat = torch.tensor(np.load(path + \"edge_feat_scaled.npy\", allow_pickle=True), dtype=torch.float, device=device)\n",
    "label = torch.tensor(np.load(path + \"label_mul.npy\", allow_pickle=True), dtype=torch.long, device=device)\n",
    "\n",
    "# Labels for relationship types\n",
    "labels = ['Benign', 'BruteForce', 'DoS', 'DDoS', 'Web', 'Bot', 'Infilteration']\n",
    "\n",
    "# Test the model\n",
    "label_cpu = label.cpu().numpy()\n",
    "f1_test, loss_test, predict_output = predict_(model, label, loss_fn, test, device, edge_feat)\n",
    "\n",
    "# Calculate accuracy and F1 score for weighted average\n",
    "accuracy = accuracy_score(label_cpu[test], predict_output)\n",
    "f1_weighted = f1_score(label_cpu[test], predict_output, average=\"weighted\")\n",
    "report = classification_report(label_cpu[test], predict_output, target_names=labels)\n",
    "\n",
    "# Print test set results\n",
    "print(f\"Test set results: loss= {loss_test:.4f}, accuracy= {accuracy:.4f}, f1_score(weighted)= {f1_weighted:.4f}\")\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splits and labels loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_172663/2946164101.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model/20241113-194017_llm_w_edgefeat.pth, epoch 9\n",
      "Test set results: loss= 0.4598, accuracy= 0.8452, f1_score(weighted)= 0.7868\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       Benign       0.00      0.00      0.00        24\n",
      "   BruteForce       0.88      0.92      0.90        24\n",
      "          DoS       0.77      1.00      0.87        24\n",
      "         DDoS       0.89      1.00      0.94        24\n",
      "          Web       1.00      1.00      1.00        24\n",
      "          Bot       0.86      1.00      0.92        24\n",
      "Infilteration       0.77      1.00      0.87        24\n",
      "\n",
      "     accuracy                           0.85       168\n",
      "    macro avg       0.74      0.85      0.79       168\n",
      " weighted avg       0.74      0.85      0.79       168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "\n",
    "def load_model(model, optimizer, path, device='cpu'):\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    \n",
    "    # Load the state dictionaries for the model and optimizer\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # Retrieve the saved epoch number\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(f\"Model loaded from {path}, epoch {epoch}\")\n",
    "    \n",
    "    return model, optimizer, epoch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LLMGraphTransformer(model_name=\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\", device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "train, val, test, train_labels, val_labels, test_labels = conv1_load_data_splits()\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "\n",
    "\n",
    "# Specify the path to the saved model\n",
    "model_path = \"model/20241113-194017_llm_w_edgefeat.pth\"\n",
    "model, optimizer, start_epoch = load_model(model, optimizer, path=model_path, device=device)\n",
    "\n",
    "# Set device to CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Path setup\n",
    "path = \"datasets/CSE-CIC/\"\n",
    "if not path.endswith('/'):\n",
    "    path += '/'\n",
    "\n",
    "edge_feat = torch.tensor(np.load(path + \"edge_feat_scaled.npy\", allow_pickle=True), dtype=torch.float, device=device)\n",
    "label = torch.tensor(np.load(path + \"label_mul.npy\", allow_pickle=True), dtype=torch.long, device=device)\n",
    "\n",
    "# Labels for relationship types\n",
    "labels = ['Benign', 'BruteForce', 'DoS', 'DDoS', 'Web', 'Bot', 'Infilteration']\n",
    "\n",
    "# Test the model\n",
    "label_cpu = label.cpu().numpy()\n",
    "f1_test, loss_test, predict_output = predict_(model, label, loss_fn, test, device, edge_feat)\n",
    "\n",
    "# Calculate accuracy and F1 score for weighted average\n",
    "accuracy = accuracy_score(label_cpu[test], predict_output)\n",
    "f1_weighted = f1_score(label_cpu[test], predict_output, average=\"weighted\")\n",
    "report = classification_report(label_cpu[test], predict_output, target_names=labels)\n",
    "\n",
    "# Print test set results\n",
    "print(f\"Test set results: loss= {loss_test:.4f}, accuracy= {accuracy:.4f}, f1_score(weighted)= {f1_weighted:.4f}\")\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
