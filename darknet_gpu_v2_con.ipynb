{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ab4dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a0fc3e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9fd020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10321290",
   "metadata": {},
   "source": [
    "# Continuous Learning | Phase 1 \n",
    "## 120 Sampling + Pre-Fixed Seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38548ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splits and labels saved successfully.\n",
      "874 98 108\n",
      "Label distribution in Train Set:\n",
      "{0: 97, 1: 97, 2: 97, 3: 97, 4: 97, 5: 97, 6: 97, 7: 98, 8: 97}\n",
      "Label distribution in Validation Set:\n",
      "{0: 11, 1: 11, 2: 11, 3: 11, 4: 11, 5: 11, 6: 11, 7: 10, 8: 11}\n",
      "Label distribution in Test Set:\n",
      "{0: 12, 1: 12, 2: 12, 3: 12, 4: 12, 5: 12, 6: 12, 7: 12, 8: 12}\n",
      "Epoch:  0\n",
      "Training data size: 874\n",
      "Number of batches: 87\n",
      "batch: 001, loss_train: 23.7233, acc_train: 0.0000, time: 0.5825s\n",
      "batch: 002, loss_train: 19.9106, acc_train: 0.0000, time: 0.5758s\n",
      "batch: 003, loss_train: 18.7829, acc_train: 0.0000, time: 0.5776s\n",
      "batch: 004, loss_train: 16.1703, acc_train: 0.0000, time: 0.5767s\n",
      "batch: 005, loss_train: 10.5653, acc_train: 0.0000, time: 0.5665s\n",
      "batch: 006, loss_train: 11.0256, acc_train: 0.1000, time: 0.5771s\n",
      "batch: 007, loss_train: 5.4044, acc_train: 0.1000, time: 0.5768s\n",
      "batch: 008, loss_train: 3.7000, acc_train: 0.0500, time: 0.5665s\n",
      "batch: 009, loss_train: 6.5287, acc_train: 0.0667, time: 0.5765s\n",
      "batch: 010, loss_train: 6.1517, acc_train: 0.1000, time: 0.5770s\n",
      "batch: 011, loss_train: 4.4068, acc_train: 0.2636, time: 0.4837s\n",
      "batch: 012, loss_train: 7.8864, acc_train: 0.1000, time: 0.4611s\n",
      "batch: 013, loss_train: 4.3096, acc_train: 0.1000, time: 0.4255s\n",
      "batch: 014, loss_train: 3.1851, acc_train: 0.0200, time: 0.5773s\n",
      "batch: 015, loss_train: 3.3833, acc_train: 0.4762, time: 0.5806s\n",
      "batch: 016, loss_train: 3.2972, acc_train: 0.1600, time: 0.5682s\n",
      "batch: 017, loss_train: 1.5918, acc_train: 0.2643, time: 0.5681s\n",
      "batch: 018, loss_train: 3.6097, acc_train: 0.3500, time: 0.5680s\n",
      "batch: 019, loss_train: 2.9521, acc_train: 0.3333, time: 0.5680s\n",
      "batch: 020, loss_train: 3.5371, acc_train: 0.0250, time: 0.5762s\n",
      "batch: 021, loss_train: 1.7804, acc_train: 0.7600, time: 0.5678s\n",
      "batch: 022, loss_train: 2.7685, acc_train: 0.3500, time: 0.5775s\n",
      "batch: 023, loss_train: 2.1145, acc_train: 0.6267, time: 0.5772s\n",
      "batch: 024, loss_train: 2.2631, acc_train: 0.5167, time: 0.5770s\n",
      "batch: 025, loss_train: 2.6311, acc_train: 0.4767, time: 0.5772s\n",
      "batch: 026, loss_train: 2.3411, acc_train: 0.5500, time: 0.5692s\n",
      "batch: 027, loss_train: 2.4578, acc_train: 0.4167, time: 0.5682s\n",
      "batch: 028, loss_train: 1.3731, acc_train: 0.7000, time: 0.5662s\n",
      "batch: 029, loss_train: 4.0335, acc_train: 0.0727, time: 0.5764s\n",
      "batch: 030, loss_train: 2.0045, acc_train: 0.4000, time: 0.5663s\n",
      "batch: 031, loss_train: 1.3313, acc_train: 0.9095, time: 0.5682s\n",
      "batch: 032, loss_train: 1.7024, acc_train: 0.6667, time: 0.5689s\n",
      "batch: 033, loss_train: 1.7551, acc_train: 0.7500, time: 0.5678s\n",
      "batch: 034, loss_train: 1.3553, acc_train: 0.4333, time: 0.5661s\n",
      "batch: 035, loss_train: 1.6029, acc_train: 0.3333, time: 0.5673s\n",
      "batch: 036, loss_train: 1.8895, acc_train: 0.6000, time: 0.5680s\n",
      "batch: 037, loss_train: 1.1769, acc_train: 0.7000, time: 0.5772s\n",
      "batch: 038, loss_train: 2.5064, acc_train: 0.5333, time: 0.5772s\n",
      "batch: 039, loss_train: 1.4472, acc_train: 0.4743, time: 0.5776s\n",
      "batch: 040, loss_train: 1.6254, acc_train: 0.6567, time: 0.5699s\n",
      "batch: 041, loss_train: 1.6216, acc_train: 0.6333, time: 0.5790s\n",
      "batch: 042, loss_train: 1.9611, acc_train: 0.7333, time: 0.5671s\n",
      "batch: 043, loss_train: 1.0296, acc_train: 0.9000, time: 0.5762s\n",
      "batch: 044, loss_train: 1.2314, acc_train: 0.9000, time: 0.5679s\n",
      "batch: 045, loss_train: 1.8155, acc_train: 0.4119, time: 0.5797s\n",
      "batch: 046, loss_train: 1.6730, acc_train: 0.5000, time: 0.5767s\n",
      "batch: 047, loss_train: 2.0852, acc_train: 0.5833, time: 0.5772s\n",
      "batch: 048, loss_train: 0.0889, acc_train: 1.0000, time: 0.5666s\n",
      "batch: 049, loss_train: 0.9584, acc_train: 0.6833, time: 0.5765s\n",
      "batch: 050, loss_train: 0.9868, acc_train: 0.6500, time: 0.5777s\n",
      "batch: 051, loss_train: 1.8418, acc_train: 0.5400, time: 0.5789s\n",
      "batch: 052, loss_train: 1.3229, acc_train: 0.6100, time: 0.5770s\n",
      "batch: 053, loss_train: 1.8625, acc_train: 0.5133, time: 0.5768s\n",
      "batch: 054, loss_train: 1.3673, acc_train: 0.7679, time: 0.5684s\n",
      "batch: 055, loss_train: 1.4338, acc_train: 0.7238, time: 0.5775s\n",
      "batch: 056, loss_train: 1.9891, acc_train: 0.6143, time: 0.5680s\n",
      "batch: 057, loss_train: 1.0834, acc_train: 0.6500, time: 0.5766s\n",
      "batch: 058, loss_train: 1.3650, acc_train: 0.6100, time: 0.5761s\n",
      "batch: 059, loss_train: 1.9685, acc_train: 0.3833, time: 0.5767s\n",
      "batch: 060, loss_train: 1.6264, acc_train: 0.6000, time: 0.5488s\n",
      "batch: 061, loss_train: 0.7797, acc_train: 0.8333, time: 0.5781s\n",
      "batch: 062, loss_train: 1.4658, acc_train: 0.3286, time: 0.5794s\n",
      "batch: 063, loss_train: 1.4399, acc_train: 0.5333, time: 0.5817s\n",
      "batch: 064, loss_train: 1.0411, acc_train: 0.6905, time: 0.5713s\n",
      "batch: 065, loss_train: 1.3519, acc_train: 0.6000, time: 0.5826s\n",
      "batch: 066, loss_train: 1.4715, acc_train: 0.5667, time: 0.5803s\n",
      "batch: 067, loss_train: 1.1870, acc_train: 0.6095, time: 0.5813s\n",
      "batch: 068, loss_train: 1.4110, acc_train: 0.7467, time: 0.5797s\n",
      "batch: 069, loss_train: 2.2760, acc_train: 0.3933, time: 0.5803s\n",
      "batch: 070, loss_train: 0.5459, acc_train: 0.8571, time: 0.5713s\n",
      "batch: 071, loss_train: 0.7812, acc_train: 0.7500, time: 0.5790s\n",
      "batch: 072, loss_train: 0.6078, acc_train: 0.8600, time: 0.5782s\n",
      "batch: 073, loss_train: 0.4347, acc_train: 0.9029, time: 0.5691s\n",
      "batch: 074, loss_train: 1.0575, acc_train: 0.6267, time: 0.5773s\n",
      "batch: 075, loss_train: 1.0084, acc_train: 0.6400, time: 0.5770s\n",
      "batch: 076, loss_train: 0.4438, acc_train: 0.9000, time: 0.5700s\n",
      "batch: 077, loss_train: 1.5374, acc_train: 0.3810, time: 0.5802s\n",
      "batch: 078, loss_train: 0.1744, acc_train: 1.0000, time: 0.5712s\n",
      "batch: 079, loss_train: 0.8436, acc_train: 0.6600, time: 0.5801s\n",
      "batch: 080, loss_train: 0.4677, acc_train: 0.8000, time: 0.5699s\n",
      "batch: 081, loss_train: 0.6902, acc_train: 0.7533, time: 0.5695s\n",
      "batch: 082, loss_train: 0.8579, acc_train: 0.7333, time: 0.5691s\n",
      "batch: 083, loss_train: 1.0728, acc_train: 0.7333, time: 0.5691s\n",
      "batch: 084, loss_train: 0.6723, acc_train: 0.7400, time: 0.5777s\n",
      "batch: 085, loss_train: 0.2648, acc_train: 0.9333, time: 0.5684s\n",
      "batch: 086, loss_train: 0.5519, acc_train: 0.8095, time: 0.5770s\n",
      "batch: 087, loss_train: 0.8567, acc_train: 0.7000, time: 0.5772s\n",
      "Validation after epoch 0:\n",
      "Validation set results: loss= 0.7140, accuracy= 0.7701, label acc= [0.         0.58064516 0.91666667 0.90909091 0.81818182 0.95238095\n",
      " 0.90909091 0.9        0.95652174]\n",
      "Epoch:  1\n",
      "Training data size: 874\n",
      "Number of batches: 87\n",
      "batch: 001, loss_train: 0.2553, acc_train: 0.9000, time: 0.5762s\n",
      "batch: 002, loss_train: 0.1709, acc_train: 1.0000, time: 0.5677s\n",
      "batch: 003, loss_train: 0.5417, acc_train: 0.7500, time: 0.5777s\n",
      "batch: 004, loss_train: 0.8962, acc_train: 0.6400, time: 0.5779s\n",
      "batch: 005, loss_train: 1.7847, acc_train: 0.4067, time: 0.5716s\n",
      "batch: 006, loss_train: 0.7114, acc_train: 0.6167, time: 0.5806s\n",
      "batch: 007, loss_train: 1.5756, acc_train: 0.5833, time: 0.5803s\n",
      "batch: 008, loss_train: 2.1742, acc_train: 0.3333, time: 0.5812s\n",
      "batch: 009, loss_train: 0.6752, acc_train: 0.7600, time: 0.5809s\n",
      "batch: 010, loss_train: 0.7770, acc_train: 0.7400, time: 0.5809s\n",
      "batch: 011, loss_train: 1.4346, acc_train: 0.4233, time: 0.5693s\n",
      "batch: 012, loss_train: 0.8426, acc_train: 0.6000, time: 0.5784s\n",
      "batch: 013, loss_train: 0.7059, acc_train: 0.7667, time: 0.5777s\n",
      "batch: 014, loss_train: 0.9220, acc_train: 0.5833, time: 0.5775s\n",
      "batch: 015, loss_train: 0.5607, acc_train: 0.7000, time: 0.5777s\n",
      "batch: 016, loss_train: 0.7549, acc_train: 0.7467, time: 0.5776s\n",
      "batch: 017, loss_train: 0.4738, acc_train: 0.7000, time: 0.5685s\n",
      "batch: 018, loss_train: 1.0674, acc_train: 0.5733, time: 0.5772s\n",
      "batch: 019, loss_train: 0.8429, acc_train: 0.7333, time: 0.5771s\n",
      "batch: 020, loss_train: 0.4300, acc_train: 0.7333, time: 0.5686s\n",
      "batch: 021, loss_train: 0.4187, acc_train: 0.7500, time: 0.5768s\n",
      "batch: 022, loss_train: 0.5249, acc_train: 0.7500, time: 0.5779s\n",
      "batch: 023, loss_train: 0.1808, acc_train: 1.0000, time: 0.5689s\n",
      "batch: 024, loss_train: 0.8331, acc_train: 0.8933, time: 0.5685s\n",
      "batch: 025, loss_train: 0.1693, acc_train: 0.9000, time: 0.5769s\n",
      "batch: 026, loss_train: 0.3928, acc_train: 0.8600, time: 0.5787s\n",
      "batch: 027, loss_train: 0.4011, acc_train: 1.0000, time: 0.5688s\n",
      "batch: 028, loss_train: 0.5433, acc_train: 0.6833, time: 0.5777s\n",
      "batch: 029, loss_train: 0.7140, acc_train: 0.8167, time: 0.5683s\n",
      "batch: 030, loss_train: 0.9073, acc_train: 0.7333, time: 0.5688s\n",
      "batch: 031, loss_train: 0.5749, acc_train: 0.7267, time: 0.5776s\n",
      "batch: 032, loss_train: 0.1904, acc_train: 1.0000, time: 0.5691s\n",
      "batch: 033, loss_train: 0.2687, acc_train: 1.0000, time: 0.5686s\n",
      "batch: 034, loss_train: 0.6730, acc_train: 0.6250, time: 0.5778s\n",
      "batch: 035, loss_train: 1.0630, acc_train: 0.5600, time: 0.5772s\n",
      "batch: 036, loss_train: 0.5327, acc_train: 0.6400, time: 0.5772s\n",
      "batch: 037, loss_train: 0.1322, acc_train: 0.9429, time: 0.5684s\n",
      "batch: 038, loss_train: 0.1756, acc_train: 0.9000, time: 0.5683s\n",
      "batch: 039, loss_train: 0.1305, acc_train: 1.0000, time: 0.5678s\n",
      "batch: 040, loss_train: 0.9458, acc_train: 0.7600, time: 0.5770s\n",
      "batch: 041, loss_train: 0.2616, acc_train: 1.0000, time: 0.5683s\n",
      "batch: 042, loss_train: 0.3186, acc_train: 0.8600, time: 0.5680s\n",
      "batch: 043, loss_train: 0.4269, acc_train: 0.7500, time: 0.5768s\n",
      "batch: 044, loss_train: 0.4253, acc_train: 0.7500, time: 0.5767s\n",
      "batch: 045, loss_train: 0.4536, acc_train: 0.7905, time: 0.5766s\n",
      "batch: 046, loss_train: 0.3236, acc_train: 0.8600, time: 0.5683s\n",
      "batch: 047, loss_train: 0.1480, acc_train: 0.9000, time: 0.5763s\n",
      "batch: 048, loss_train: 0.0865, acc_train: 1.0000, time: 0.5682s\n",
      "batch: 049, loss_train: 0.4376, acc_train: 0.9067, time: 0.5678s\n",
      "batch: 050, loss_train: 0.5245, acc_train: 0.8571, time: 0.5692s\n",
      "batch: 051, loss_train: 0.7069, acc_train: 0.7200, time: 0.5802s\n",
      "batch: 052, loss_train: 0.4566, acc_train: 0.7000, time: 0.5763s\n",
      "batch: 053, loss_train: 0.0746, acc_train: 1.0000, time: 0.5685s\n",
      "batch: 054, loss_train: 0.1079, acc_train: 1.0000, time: 0.5685s\n",
      "batch: 055, loss_train: 0.0982, acc_train: 1.0000, time: 0.5682s\n",
      "batch: 056, loss_train: 0.5004, acc_train: 0.7250, time: 0.5774s\n",
      "batch: 057, loss_train: 0.2043, acc_train: 0.9000, time: 0.5798s\n",
      "batch: 058, loss_train: 1.1427, acc_train: 0.8524, time: 0.5677s\n",
      "batch: 059, loss_train: 0.6230, acc_train: 0.9067, time: 0.5684s\n",
      "batch: 060, loss_train: 0.3602, acc_train: 0.8400, time: 0.5682s\n",
      "batch: 061, loss_train: 1.8953, acc_train: 0.5067, time: 0.5765s\n",
      "batch: 062, loss_train: 0.9237, acc_train: 0.5733, time: 0.5767s\n",
      "batch: 063, loss_train: 0.5008, acc_train: 0.8000, time: 0.5767s\n",
      "batch: 064, loss_train: 0.6611, acc_train: 0.7000, time: 0.5772s\n",
      "batch: 065, loss_train: 0.6042, acc_train: 0.7200, time: 0.5775s\n",
      "batch: 066, loss_train: 0.1039, acc_train: 1.0000, time: 0.5689s\n",
      "batch: 067, loss_train: 0.3106, acc_train: 0.8000, time: 0.5787s\n",
      "batch: 068, loss_train: 0.6713, acc_train: 0.6833, time: 0.5807s\n",
      "batch: 069, loss_train: 1.4266, acc_train: 0.4667, time: 0.5802s\n",
      "batch: 070, loss_train: 0.2973, acc_train: 0.8667, time: 0.5707s\n",
      "batch: 071, loss_train: 0.3135, acc_train: 0.8000, time: 0.5799s\n",
      "batch: 072, loss_train: 0.3202, acc_train: 0.8600, time: 0.5795s\n",
      "batch: 073, loss_train: 0.2924, acc_train: 0.7933, time: 0.5772s\n",
      "batch: 074, loss_train: 0.6982, acc_train: 0.6000, time: 0.5768s\n",
      "batch: 075, loss_train: 0.4564, acc_train: 0.8029, time: 0.5773s\n",
      "batch: 076, loss_train: 0.1813, acc_train: 0.8667, time: 0.5763s\n",
      "batch: 077, loss_train: 0.2896, acc_train: 0.7000, time: 0.5741s\n",
      "batch: 078, loss_train: 2.2419, acc_train: 0.3000, time: 0.5749s\n",
      "batch: 079, loss_train: 0.7169, acc_train: 0.6667, time: 0.5684s\n",
      "batch: 080, loss_train: 0.2148, acc_train: 1.0000, time: 0.5698s\n",
      "batch: 081, loss_train: 1.1886, acc_train: 0.7333, time: 0.5812s\n",
      "batch: 082, loss_train: 0.1435, acc_train: 1.0000, time: 0.5712s\n",
      "batch: 083, loss_train: 0.4576, acc_train: 0.8667, time: 0.5776s\n",
      "batch: 084, loss_train: 1.0032, acc_train: 0.8600, time: 0.5803s\n",
      "batch: 085, loss_train: 0.3129, acc_train: 0.7000, time: 0.5694s\n",
      "batch: 086, loss_train: 0.4226, acc_train: 0.9333, time: 0.5826s\n",
      "batch: 087, loss_train: 0.2089, acc_train: 0.8667, time: 0.5798s\n",
      "Validation after epoch 1:\n",
      "Validation set results: loss= 0.6331, accuracy= 0.7881, label acc= [0.70967742 0.11764706 0.95238095 0.625      0.90909091 0.95238095\n",
      " 0.84615385 1.         1.        ]\n",
      "Epoch:  2\n",
      "Training data size: 874\n",
      "Number of batches: 87\n",
      "batch: 001, loss_train: 0.6230, acc_train: 0.5667, time: 0.5658s\n",
      "batch: 002, loss_train: 1.1415, acc_train: 0.6667, time: 0.5660s\n",
      "batch: 003, loss_train: 0.8769, acc_train: 0.7267, time: 0.5777s\n",
      "batch: 004, loss_train: 0.9159, acc_train: 0.7238, time: 0.5700s\n",
      "batch: 005, loss_train: 0.5123, acc_train: 0.7000, time: 0.5803s\n",
      "batch: 006, loss_train: 0.1682, acc_train: 1.0000, time: 0.5773s\n",
      "batch: 007, loss_train: 0.3070, acc_train: 0.8600, time: 0.5770s\n",
      "batch: 008, loss_train: 0.6912, acc_train: 0.5433, time: 0.5686s\n",
      "batch: 009, loss_train: 0.3387, acc_train: 0.9000, time: 0.5773s\n",
      "batch: 010, loss_train: 0.1028, acc_train: 1.0000, time: 0.5785s\n",
      "batch: 011, loss_train: 0.3908, acc_train: 0.6456, time: 0.5688s\n",
      "batch: 012, loss_train: 0.6850, acc_train: 0.8029, time: 0.5779s\n",
      "batch: 013, loss_train: 0.2544, acc_train: 0.8600, time: 0.5778s\n",
      "batch: 014, loss_train: 0.4685, acc_train: 0.9400, time: 0.5699s\n",
      "batch: 015, loss_train: 0.0186, acc_train: 1.0000, time: 0.5709s\n",
      "batch: 016, loss_train: 0.7120, acc_train: 0.7833, time: 0.5691s\n",
      "batch: 017, loss_train: 0.2186, acc_train: 0.8600, time: 0.5782s\n",
      "batch: 018, loss_train: 0.2699, acc_train: 1.0000, time: 0.5822s\n",
      "batch: 019, loss_train: 0.3182, acc_train: 0.7333, time: 0.5779s\n",
      "batch: 020, loss_train: 0.6456, acc_train: 0.7333, time: 0.5680s\n",
      "batch: 021, loss_train: 0.0848, acc_train: 1.0000, time: 0.5649s\n",
      "batch: 022, loss_train: 0.1386, acc_train: 0.9333, time: 0.5683s\n",
      "batch: 023, loss_train: 0.2523, acc_train: 0.7500, time: 0.5683s\n",
      "batch: 024, loss_train: 0.3568, acc_train: 0.8500, time: 0.5681s\n",
      "batch: 025, loss_train: 0.1497, acc_train: 0.9000, time: 0.5680s\n",
      "batch: 026, loss_train: 0.2995, acc_train: 1.0000, time: 0.5777s\n",
      "batch: 027, loss_train: 0.4917, acc_train: 0.8667, time: 0.5688s\n",
      "batch: 028, loss_train: 0.3087, acc_train: 1.0000, time: 0.5771s\n",
      "batch: 029, loss_train: 0.2915, acc_train: 1.0000, time: 0.5773s\n",
      "batch: 030, loss_train: 0.0899, acc_train: 1.0000, time: 0.5685s\n",
      "batch: 031, loss_train: 0.2961, acc_train: 0.8333, time: 0.5772s\n",
      "batch: 032, loss_train: 0.4314, acc_train: 0.9000, time: 0.5682s\n",
      "batch: 033, loss_train: 0.2737, acc_train: 1.0000, time: 0.5770s\n",
      "batch: 034, loss_train: 0.4317, acc_train: 0.8667, time: 0.5774s\n",
      "batch: 035, loss_train: 0.4815, acc_train: 0.7333, time: 0.5773s\n",
      "batch: 036, loss_train: 0.2296, acc_train: 1.0000, time: 0.5761s\n",
      "batch: 037, loss_train: 0.2199, acc_train: 1.0000, time: 0.5748s\n",
      "batch: 038, loss_train: 0.6763, acc_train: 0.9400, time: 0.5769s\n",
      "batch: 039, loss_train: 0.1448, acc_train: 1.0000, time: 0.5768s\n",
      "batch: 040, loss_train: 0.4791, acc_train: 0.8905, time: 0.5771s\n",
      "batch: 041, loss_train: 0.5217, acc_train: 0.8667, time: 0.5683s\n",
      "batch: 042, loss_train: 0.1508, acc_train: 1.0000, time: 0.5682s\n",
      "batch: 043, loss_train: 0.4397, acc_train: 0.8933, time: 0.5690s\n",
      "batch: 044, loss_train: 0.9777, acc_train: 0.8667, time: 0.5769s\n",
      "batch: 045, loss_train: 1.3236, acc_train: 0.7667, time: 0.5764s\n",
      "batch: 046, loss_train: 0.0160, acc_train: 1.0000, time: 0.5682s\n",
      "batch: 047, loss_train: 0.1978, acc_train: 1.0000, time: 0.5771s\n",
      "batch: 048, loss_train: 0.1465, acc_train: 0.8600, time: 0.5683s\n",
      "batch: 049, loss_train: 0.2535, acc_train: 1.0000, time: 0.5774s\n",
      "batch: 050, loss_train: 0.2588, acc_train: 0.9000, time: 0.5685s\n",
      "batch: 051, loss_train: 0.1912, acc_train: 1.0000, time: 0.5687s\n",
      "batch: 052, loss_train: 0.8783, acc_train: 0.6267, time: 0.5683s\n",
      "batch: 053, loss_train: 1.4988, acc_train: 0.7571, time: 0.5686s\n",
      "batch: 054, loss_train: 0.4504, acc_train: 0.7600, time: 0.5691s\n",
      "batch: 055, loss_train: 0.9228, acc_train: 0.9000, time: 0.5688s\n",
      "batch: 056, loss_train: 0.7462, acc_train: 0.6533, time: 0.5681s\n",
      "batch: 057, loss_train: 0.9551, acc_train: 0.6381, time: 0.5768s\n",
      "batch: 058, loss_train: 0.3017, acc_train: 0.9000, time: 0.5685s\n",
      "batch: 059, loss_train: 0.7190, acc_train: 0.8400, time: 0.5688s\n",
      "batch: 060, loss_train: 0.2596, acc_train: 0.8667, time: 0.5767s\n",
      "batch: 061, loss_train: 0.2550, acc_train: 0.9000, time: 0.5684s\n",
      "batch: 062, loss_train: 0.4585, acc_train: 0.9095, time: 0.5769s\n",
      "batch: 063, loss_train: 0.7543, acc_train: 0.5238, time: 0.5771s\n",
      "batch: 064, loss_train: 0.6548, acc_train: 0.8600, time: 0.5761s\n",
      "batch: 065, loss_train: 0.2513, acc_train: 0.8600, time: 0.5662s\n",
      "batch: 066, loss_train: 0.2883, acc_train: 1.0000, time: 0.5752s\n",
      "batch: 067, loss_train: 0.2826, acc_train: 1.0000, time: 0.5767s\n",
      "batch: 068, loss_train: 0.1724, acc_train: 1.0000, time: 0.5703s\n",
      "batch: 069, loss_train: 0.2745, acc_train: 0.8905, time: 0.5807s\n",
      "batch: 070, loss_train: 0.2506, acc_train: 0.9000, time: 0.5787s\n",
      "batch: 071, loss_train: 0.1769, acc_train: 1.0000, time: 0.5792s\n",
      "batch: 072, loss_train: 0.0900, acc_train: 1.0000, time: 0.5681s\n",
      "batch: 073, loss_train: 0.2800, acc_train: 0.8600, time: 0.5688s\n",
      "batch: 074, loss_train: 0.2487, acc_train: 1.0000, time: 0.5762s\n",
      "batch: 075, loss_train: 0.2089, acc_train: 0.8667, time: 0.5770s\n",
      "batch: 076, loss_train: 0.4138, acc_train: 0.9067, time: 0.5775s\n",
      "batch: 077, loss_train: 1.0838, acc_train: 0.8733, time: 0.5705s\n",
      "batch: 078, loss_train: 0.6968, acc_train: 0.8095, time: 0.5779s\n",
      "batch: 079, loss_train: 0.4518, acc_train: 0.9095, time: 0.5788s\n",
      "batch: 080, loss_train: 0.1757, acc_train: 1.0000, time: 0.5771s\n",
      "batch: 081, loss_train: 0.1003, acc_train: 1.0000, time: 0.5691s\n",
      "batch: 082, loss_train: 0.2102, acc_train: 1.0000, time: 0.5776s\n",
      "batch: 083, loss_train: 0.9611, acc_train: 0.8905, time: 0.5816s\n",
      "batch: 084, loss_train: 0.1898, acc_train: 1.0000, time: 0.5791s\n",
      "batch: 085, loss_train: 0.1657, acc_train: 1.0000, time: 0.5772s\n",
      "batch: 086, loss_train: 0.5984, acc_train: 0.8000, time: 0.5805s\n",
      "batch: 087, loss_train: 0.2149, acc_train: 0.8600, time: 0.5777s\n",
      "Validation after epoch 2:\n",
      "Validation set results: loss= 0.3171, accuracy= 0.9492, label acc= [1.         0.9        0.91666667 1.         0.86956522 0.95238095\n",
      " 1.         0.94736842 0.95652174]\n",
      "Epoch:  3\n",
      "Training data size: 874\n",
      "Number of batches: 87\n",
      "batch: 001, loss_train: 0.1214, acc_train: 1.0000, time: 0.5661s\n",
      "batch: 002, loss_train: 0.0705, acc_train: 1.0000, time: 0.5659s\n",
      "batch: 003, loss_train: 0.1798, acc_train: 1.0000, time: 0.5716s\n",
      "batch: 004, loss_train: 0.3340, acc_train: 0.8600, time: 0.5740s\n",
      "batch: 005, loss_train: 0.1361, acc_train: 1.0000, time: 0.5802s\n",
      "batch: 006, loss_train: 0.2231, acc_train: 0.9333, time: 0.5782s\n",
      "batch: 007, loss_train: 0.3477, acc_train: 0.9400, time: 0.5698s\n",
      "batch: 008, loss_train: 0.1630, acc_train: 0.8667, time: 0.5709s\n",
      "batch: 009, loss_train: 0.1538, acc_train: 1.0000, time: 0.5770s\n",
      "batch: 010, loss_train: 0.4528, acc_train: 0.8067, time: 0.5772s\n",
      "batch: 011, loss_train: 0.3221, acc_train: 0.9333, time: 0.5775s\n",
      "batch: 012, loss_train: 0.2221, acc_train: 0.9000, time: 0.5771s\n",
      "batch: 013, loss_train: 0.2622, acc_train: 0.8667, time: 0.5784s\n",
      "batch: 014, loss_train: 0.2121, acc_train: 1.0000, time: 0.5678s\n",
      "batch: 015, loss_train: 0.1496, acc_train: 1.0000, time: 0.5770s\n",
      "batch: 016, loss_train: 0.0736, acc_train: 1.0000, time: 0.5772s\n",
      "batch: 017, loss_train: 0.2745, acc_train: 0.8667, time: 0.5679s\n",
      "batch: 018, loss_train: 0.1526, acc_train: 1.0000, time: 0.5689s\n",
      "batch: 019, loss_train: 0.1258, acc_train: 1.0000, time: 0.5790s\n",
      "batch: 020, loss_train: 0.0052, acc_train: 1.0000, time: 0.5668s\n",
      "batch: 021, loss_train: 0.0401, acc_train: 1.0000, time: 0.5721s\n",
      "batch: 022, loss_train: 0.2164, acc_train: 0.9000, time: 0.5686s\n",
      "batch: 023, loss_train: 0.0664, acc_train: 1.0000, time: 0.5770s\n",
      "batch: 024, loss_train: 0.0656, acc_train: 1.0000, time: 0.5768s\n",
      "batch: 025, loss_train: 0.1697, acc_train: 0.8667, time: 0.5688s\n",
      "batch: 026, loss_train: 0.2013, acc_train: 0.9000, time: 0.5783s\n",
      "batch: 027, loss_train: 0.5036, acc_train: 0.9029, time: 0.5685s\n",
      "batch: 028, loss_train: 0.0471, acc_train: 1.0000, time: 0.5773s\n",
      "batch: 029, loss_train: 0.0493, acc_train: 1.0000, time: 0.5767s\n",
      "batch: 030, loss_train: 0.0480, acc_train: 1.0000, time: 0.5775s\n",
      "batch: 031, loss_train: 0.0937, acc_train: 1.0000, time: 0.5765s\n",
      "batch: 032, loss_train: 0.4495, acc_train: 0.9333, time: 0.5772s\n",
      "batch: 033, loss_train: 0.0733, acc_train: 1.0000, time: 0.5768s\n",
      "batch: 034, loss_train: 0.0458, acc_train: 1.0000, time: 0.5679s\n",
      "batch: 035, loss_train: 0.0387, acc_train: 1.0000, time: 0.5777s\n",
      "batch: 036, loss_train: 0.9089, acc_train: 0.9333, time: 0.5772s\n",
      "batch: 037, loss_train: 0.0386, acc_train: 1.0000, time: 0.5675s\n",
      "batch: 038, loss_train: 0.0433, acc_train: 1.0000, time: 0.5760s\n",
      "batch: 039, loss_train: 0.2415, acc_train: 0.8933, time: 0.5661s\n",
      "batch: 040, loss_train: 0.0319, acc_train: 1.0000, time: 0.5778s\n",
      "batch: 041, loss_train: 0.0580, acc_train: 1.0000, time: 0.5771s\n",
      "batch: 042, loss_train: 0.1711, acc_train: 0.8933, time: 0.5771s\n",
      "batch: 043, loss_train: 0.1079, acc_train: 1.0000, time: 0.5773s\n",
      "batch: 044, loss_train: 0.2658, acc_train: 0.7833, time: 0.5773s\n",
      "batch: 045, loss_train: 0.3170, acc_train: 0.8600, time: 0.5684s\n",
      "batch: 046, loss_train: 0.0623, acc_train: 1.0000, time: 0.5766s\n",
      "batch: 047, loss_train: 1.2483, acc_train: 0.6900, time: 0.5675s\n",
      "batch: 048, loss_train: 0.5018, acc_train: 0.8667, time: 0.5660s\n",
      "batch: 049, loss_train: 0.7417, acc_train: 0.9067, time: 0.5736s\n",
      "batch: 050, loss_train: 0.0740, acc_train: 1.0000, time: 0.5684s\n",
      "batch: 051, loss_train: 0.4647, acc_train: 0.8667, time: 0.5762s\n",
      "batch: 052, loss_train: 0.3624, acc_train: 0.9333, time: 0.5684s\n",
      "batch: 053, loss_train: 0.1333, acc_train: 1.0000, time: 0.5769s\n",
      "batch: 054, loss_train: 0.1632, acc_train: 1.0000, time: 0.5763s\n",
      "batch: 055, loss_train: 0.3620, acc_train: 0.9000, time: 0.5683s\n",
      "batch: 056, loss_train: 0.2585, acc_train: 0.7538, time: 0.5689s\n",
      "batch: 057, loss_train: 0.1325, acc_train: 1.0000, time: 0.5679s\n",
      "batch: 058, loss_train: 0.0234, acc_train: 1.0000, time: 0.5681s\n",
      "batch: 059, loss_train: 0.0406, acc_train: 1.0000, time: 0.5680s\n",
      "batch: 060, loss_train: 0.0425, acc_train: 1.0000, time: 0.5680s\n",
      "batch: 061, loss_train: 0.0644, acc_train: 1.0000, time: 0.5770s\n",
      "batch: 062, loss_train: 0.0034, acc_train: 1.0000, time: 0.5681s\n",
      "batch: 063, loss_train: 0.0313, acc_train: 1.0000, time: 0.5665s\n",
      "batch: 064, loss_train: 0.0196, acc_train: 1.0000, time: 0.5678s\n",
      "batch: 065, loss_train: 0.0165, acc_train: 1.0000, time: 0.5658s\n",
      "batch: 066, loss_train: 0.3406, acc_train: 0.8600, time: 0.5681s\n",
      "batch: 067, loss_train: 0.0419, acc_train: 1.0000, time: 0.5679s\n",
      "batch: 068, loss_train: 0.0565, acc_train: 1.0000, time: 0.5774s\n",
      "batch: 069, loss_train: 0.2540, acc_train: 0.9000, time: 0.5685s\n",
      "batch: 070, loss_train: 0.0451, acc_train: 1.0000, time: 0.5767s\n",
      "batch: 071, loss_train: 0.0248, acc_train: 1.0000, time: 0.5775s\n",
      "batch: 072, loss_train: 0.0460, acc_train: 1.0000, time: 0.5762s\n",
      "batch: 073, loss_train: 0.9929, acc_train: 0.9333, time: 0.5772s\n",
      "batch: 074, loss_train: 0.7096, acc_train: 0.8600, time: 0.5776s\n",
      "batch: 075, loss_train: 0.0374, acc_train: 1.0000, time: 0.5765s\n",
      "batch: 076, loss_train: 0.3751, acc_train: 0.8933, time: 0.5668s\n",
      "batch: 077, loss_train: 0.7950, acc_train: 0.8800, time: 0.5745s\n",
      "batch: 078, loss_train: 0.0778, acc_train: 1.0000, time: 0.5780s\n",
      "batch: 079, loss_train: 0.0315, acc_train: 1.0000, time: 0.5680s\n",
      "batch: 080, loss_train: 0.3004, acc_train: 0.8167, time: 0.5683s\n",
      "batch: 081, loss_train: 1.1309, acc_train: 0.8667, time: 0.5764s\n",
      "batch: 082, loss_train: 0.0212, acc_train: 1.0000, time: 0.5675s\n",
      "batch: 083, loss_train: 0.2094, acc_train: 0.8933, time: 0.5680s\n",
      "batch: 084, loss_train: 0.0510, acc_train: 1.0000, time: 0.5768s\n",
      "batch: 085, loss_train: 0.1004, acc_train: 1.0000, time: 0.5763s\n",
      "batch: 086, loss_train: 0.1331, acc_train: 1.0000, time: 0.5766s\n",
      "batch: 087, loss_train: 0.0439, acc_train: 1.0000, time: 0.5774s\n",
      "Validation after epoch 3:\n",
      "Validation set results: loss= 0.1553, accuracy= 0.9488, label acc= [1.         0.9        1.         0.91666667 0.95652174 0.95238095\n",
      " 0.95652174 0.9        0.95238095]\n",
      "Epoch:  4\n",
      "Training data size: 874\n",
      "Number of batches: 87\n",
      "batch: 001, loss_train: 0.0416, acc_train: 1.0000, time: 0.5742s\n",
      "batch: 002, loss_train: 0.2736, acc_train: 0.9000, time: 0.5660s\n",
      "batch: 003, loss_train: 0.6488, acc_train: 0.7933, time: 0.5765s\n",
      "batch: 004, loss_train: 0.2382, acc_train: 0.9400, time: 0.5766s\n",
      "batch: 005, loss_train: 0.0300, acc_train: 1.0000, time: 0.5774s\n",
      "batch: 006, loss_train: 0.1673, acc_train: 0.9333, time: 0.5781s\n",
      "batch: 007, loss_train: 0.2255, acc_train: 0.8600, time: 0.5690s\n",
      "batch: 008, loss_train: 0.0829, acc_train: 1.0000, time: 0.5681s\n",
      "batch: 009, loss_train: 0.4626, acc_train: 0.7500, time: 0.5677s\n",
      "batch: 010, loss_train: 0.0425, acc_train: 1.0000, time: 0.5765s\n",
      "batch: 011, loss_train: 0.0456, acc_train: 1.0000, time: 0.5683s\n",
      "batch: 012, loss_train: 0.0796, acc_train: 1.0000, time: 0.5776s\n",
      "batch: 013, loss_train: 1.4562, acc_train: 0.7571, time: 0.5686s\n",
      "batch: 014, loss_train: 0.0193, acc_train: 1.0000, time: 0.5682s\n",
      "batch: 015, loss_train: 0.3336, acc_train: 0.9000, time: 0.5771s\n",
      "batch: 016, loss_train: 0.0360, acc_train: 1.0000, time: 0.5766s\n",
      "batch: 017, loss_train: 0.1733, acc_train: 0.8933, time: 0.5686s\n",
      "batch: 018, loss_train: 0.0310, acc_train: 1.0000, time: 0.5699s\n",
      "batch: 019, loss_train: 0.0740, acc_train: 1.0000, time: 0.5687s\n",
      "batch: 020, loss_train: 0.0423, acc_train: 1.0000, time: 0.5726s\n",
      "batch: 021, loss_train: 0.0186, acc_train: 1.0000, time: 0.5817s\n",
      "batch: 022, loss_train: 0.7481, acc_train: 0.9000, time: 0.5790s\n",
      "batch: 023, loss_train: 0.4820, acc_train: 0.9000, time: 0.5805s\n",
      "batch: 024, loss_train: 0.0259, acc_train: 1.0000, time: 0.5823s\n",
      "batch: 025, loss_train: 0.0439, acc_train: 1.0000, time: 0.5709s\n",
      "batch: 026, loss_train: 0.0389, acc_train: 1.0000, time: 0.5718s\n",
      "batch: 027, loss_train: 0.0109, acc_train: 1.0000, time: 0.5809s\n",
      "batch: 028, loss_train: 0.0399, acc_train: 1.0000, time: 0.5796s\n",
      "batch: 029, loss_train: 0.0588, acc_train: 1.0000, time: 0.5787s\n",
      "batch: 030, loss_train: 0.0579, acc_train: 1.0000, time: 0.5853s\n",
      "batch: 031, loss_train: 0.0262, acc_train: 1.0000, time: 0.5770s\n",
      "batch: 032, loss_train: 0.0229, acc_train: 1.0000, time: 0.5777s\n",
      "batch: 033, loss_train: 0.0516, acc_train: 1.0000, time: 0.5785s\n",
      "batch: 034, loss_train: 0.0158, acc_train: 1.0000, time: 0.5701s\n",
      "batch: 035, loss_train: 0.1456, acc_train: 0.8667, time: 0.5777s\n",
      "batch: 036, loss_train: 0.0047, acc_train: 1.0000, time: 0.5654s\n",
      "batch: 037, loss_train: 0.0693, acc_train: 1.0000, time: 0.5747s\n",
      "batch: 038, loss_train: 0.0608, acc_train: 1.0000, time: 0.5761s\n",
      "batch: 039, loss_train: 0.0513, acc_train: 1.0000, time: 0.5768s\n",
      "batch: 040, loss_train: 0.0082, acc_train: 1.0000, time: 0.5677s\n",
      "batch: 041, loss_train: 0.0281, acc_train: 1.0000, time: 0.5773s\n",
      "batch: 042, loss_train: 0.0278, acc_train: 1.0000, time: 0.5770s\n",
      "batch: 043, loss_train: 0.0548, acc_train: 1.0000, time: 0.5680s\n",
      "batch: 044, loss_train: 0.0081, acc_train: 1.0000, time: 0.5682s\n",
      "batch: 045, loss_train: 0.0178, acc_train: 1.0000, time: 0.5769s\n",
      "batch: 046, loss_train: 0.0800, acc_train: 1.0000, time: 0.5768s\n",
      "batch: 047, loss_train: 0.0160, acc_train: 1.0000, time: 0.5683s\n",
      "batch: 048, loss_train: 0.0274, acc_train: 1.0000, time: 0.5762s\n",
      "batch: 049, loss_train: 0.0182, acc_train: 1.0000, time: 0.5770s\n",
      "batch: 050, loss_train: 0.4809, acc_train: 0.8400, time: 0.5766s\n",
      "batch: 051, loss_train: 1.2087, acc_train: 0.9333, time: 0.5683s\n",
      "batch: 052, loss_train: 0.3020, acc_train: 0.9000, time: 0.5662s\n",
      "batch: 053, loss_train: 0.0110, acc_train: 1.0000, time: 0.5767s\n",
      "batch: 054, loss_train: 0.0201, acc_train: 1.0000, time: 0.5767s\n",
      "batch: 055, loss_train: 0.0200, acc_train: 1.0000, time: 0.5689s\n",
      "batch: 056, loss_train: 0.0640, acc_train: 1.0000, time: 0.5779s\n",
      "batch: 057, loss_train: 0.0276, acc_train: 1.0000, time: 0.5774s\n",
      "batch: 058, loss_train: 0.2580, acc_train: 0.9000, time: 0.5764s\n",
      "batch: 059, loss_train: 0.7432, acc_train: 0.7667, time: 0.5767s\n",
      "batch: 060, loss_train: 0.0201, acc_train: 1.0000, time: 0.5659s\n",
      "batch: 061, loss_train: 0.0576, acc_train: 1.0000, time: 0.5656s\n",
      "batch: 062, loss_train: 0.2288, acc_train: 0.9400, time: 0.5682s\n",
      "batch: 063, loss_train: 0.0151, acc_train: 1.0000, time: 0.5771s\n",
      "batch: 064, loss_train: 0.0183, acc_train: 1.0000, time: 0.5767s\n",
      "batch: 065, loss_train: 0.4078, acc_train: 0.8667, time: 0.5774s\n",
      "batch: 066, loss_train: 0.0460, acc_train: 1.0000, time: 0.5682s\n",
      "batch: 067, loss_train: 0.2507, acc_train: 0.9333, time: 0.5769s\n",
      "batch: 068, loss_train: 0.0138, acc_train: 1.0000, time: 0.5770s\n",
      "batch: 069, loss_train: 0.0226, acc_train: 1.0000, time: 0.5767s\n",
      "batch: 070, loss_train: 0.1009, acc_train: 0.9067, time: 0.5662s\n",
      "batch: 071, loss_train: 0.0272, acc_train: 1.0000, time: 0.5764s\n",
      "batch: 072, loss_train: 0.0510, acc_train: 1.0000, time: 0.5684s\n",
      "batch: 073, loss_train: 0.2121, acc_train: 0.9333, time: 0.5780s\n",
      "batch: 074, loss_train: 0.0246, acc_train: 1.0000, time: 0.5774s\n",
      "batch: 075, loss_train: 0.0307, acc_train: 1.0000, time: 0.5771s\n",
      "batch: 076, loss_train: 0.0022, acc_train: 1.0000, time: 0.5662s\n",
      "batch: 077, loss_train: 0.0127, acc_train: 1.0000, time: 0.5775s\n",
      "batch: 078, loss_train: 0.0130, acc_train: 1.0000, time: 0.5683s\n",
      "batch: 079, loss_train: 0.0130, acc_train: 1.0000, time: 0.5772s\n",
      "batch: 080, loss_train: 0.0683, acc_train: 1.0000, time: 0.5766s\n",
      "batch: 081, loss_train: 0.0009, acc_train: 1.0000, time: 0.5676s\n",
      "batch: 082, loss_train: 0.1148, acc_train: 1.0000, time: 0.5688s\n",
      "batch: 083, loss_train: 0.0056, acc_train: 1.0000, time: 0.5689s\n",
      "batch: 084, loss_train: 0.6610, acc_train: 0.9000, time: 0.5776s\n",
      "batch: 085, loss_train: 0.0237, acc_train: 1.0000, time: 0.5766s\n",
      "batch: 086, loss_train: 0.6122, acc_train: 0.9400, time: 0.5689s\n",
      "batch: 087, loss_train: 0.0112, acc_train: 1.0000, time: 0.5778s\n",
      "Validation after epoch 4:\n",
      "Validation set results: loss= 0.1127, accuracy= 0.9693, label acc= [1.         0.95238095 1.         1.         0.95652174 0.90909091\n",
      " 0.95652174 0.94736842 1.        ]\n",
      "Epoch:  5\n",
      "Training data size: 874\n",
      "Number of batches: 87\n",
      "batch: 001, loss_train: 0.0163, acc_train: 1.0000, time: 0.5765s\n",
      "batch: 002, loss_train: 0.3916, acc_train: 0.9333, time: 0.5750s\n",
      "batch: 003, loss_train: 0.1360, acc_train: 0.9000, time: 0.5767s\n",
      "batch: 004, loss_train: 0.0221, acc_train: 1.0000, time: 0.5773s\n",
      "batch: 005, loss_train: 0.0277, acc_train: 1.0000, time: 0.5774s\n",
      "batch: 006, loss_train: 0.0276, acc_train: 1.0000, time: 0.5778s\n",
      "batch: 007, loss_train: 0.0076, acc_train: 1.0000, time: 0.5772s\n",
      "batch: 008, loss_train: 0.3230, acc_train: 0.8000, time: 0.5777s\n",
      "batch: 009, loss_train: 0.0160, acc_train: 1.0000, time: 0.5771s\n",
      "batch: 010, loss_train: 0.0431, acc_train: 1.0000, time: 0.5769s\n",
      "batch: 011, loss_train: 0.0127, acc_train: 1.0000, time: 0.5679s\n",
      "batch: 012, loss_train: 0.0367, acc_train: 1.0000, time: 0.5674s\n",
      "batch: 013, loss_train: 0.0857, acc_train: 1.0000, time: 0.5698s\n",
      "batch: 014, loss_train: 0.1125, acc_train: 1.0000, time: 0.5765s\n",
      "batch: 015, loss_train: 0.0390, acc_train: 1.0000, time: 0.5677s\n",
      "batch: 016, loss_train: 0.0220, acc_train: 1.0000, time: 0.5677s\n",
      "batch: 017, loss_train: 0.3486, acc_train: 0.8667, time: 0.5681s\n",
      "batch: 018, loss_train: 0.0198, acc_train: 1.0000, time: 0.5773s\n",
      "batch: 019, loss_train: 0.0124, acc_train: 1.0000, time: 0.5667s\n",
      "batch: 020, loss_train: 0.4222, acc_train: 0.8933, time: 0.5746s\n",
      "batch: 021, loss_train: 0.0118, acc_train: 1.0000, time: 0.5689s\n",
      "batch: 022, loss_train: 0.3276, acc_train: 0.9333, time: 0.5770s\n",
      "batch: 023, loss_train: 0.0165, acc_train: 1.0000, time: 0.5767s\n",
      "batch: 024, loss_train: 0.0015, acc_train: 1.0000, time: 0.5684s\n",
      "batch: 025, loss_train: 0.1923, acc_train: 0.9400, time: 0.5773s\n",
      "batch: 026, loss_train: 0.0198, acc_train: 1.0000, time: 0.5781s\n",
      "batch: 027, loss_train: 0.0079, acc_train: 1.0000, time: 0.5769s\n",
      "batch: 028, loss_train: 0.0501, acc_train: 1.0000, time: 0.5770s\n",
      "batch: 029, loss_train: 0.0168, acc_train: 1.0000, time: 0.5776s\n",
      "batch: 030, loss_train: 0.0344, acc_train: 1.0000, time: 0.5686s\n",
      "batch: 031, loss_train: 0.0216, acc_train: 1.0000, time: 0.5772s\n",
      "batch: 032, loss_train: 0.0122, acc_train: 1.0000, time: 0.5773s\n",
      "batch: 033, loss_train: 0.0139, acc_train: 1.0000, time: 0.5667s\n",
      "batch: 034, loss_train: 0.0121, acc_train: 1.0000, time: 0.5770s\n",
      "batch: 035, loss_train: 0.1536, acc_train: 0.8600, time: 0.5687s\n",
      "batch: 036, loss_train: 0.0515, acc_train: 1.0000, time: 0.5679s\n",
      "batch: 037, loss_train: 0.0055, acc_train: 1.0000, time: 0.5708s\n",
      "batch: 038, loss_train: 0.0183, acc_train: 1.0000, time: 0.5787s\n",
      "batch: 039, loss_train: 0.2535, acc_train: 0.8600, time: 0.5791s\n",
      "batch: 040, loss_train: 0.0144, acc_train: 1.0000, time: 0.5802s\n",
      "batch: 041, loss_train: 0.0051, acc_train: 1.0000, time: 0.5686s\n",
      "batch: 042, loss_train: 0.0290, acc_train: 1.0000, time: 0.5680s\n",
      "batch: 043, loss_train: 0.0047, acc_train: 1.0000, time: 0.5660s\n",
      "batch: 044, loss_train: 0.5709, acc_train: 0.8167, time: 0.5772s\n",
      "batch: 045, loss_train: 0.0074, acc_train: 1.0000, time: 0.5688s\n",
      "batch: 046, loss_train: 0.2144, acc_train: 0.9000, time: 0.5687s\n",
      "batch: 047, loss_train: 0.0329, acc_train: 1.0000, time: 0.5770s\n",
      "batch: 048, loss_train: 0.0084, acc_train: 1.0000, time: 0.5683s\n",
      "batch: 049, loss_train: 0.0107, acc_train: 1.0000, time: 0.5771s\n",
      "batch: 050, loss_train: 0.0092, acc_train: 1.0000, time: 0.5775s\n",
      "batch: 051, loss_train: 0.4955, acc_train: 0.9067, time: 0.5767s\n",
      "batch: 052, loss_train: 0.0394, acc_train: 1.0000, time: 0.5773s\n",
      "batch: 053, loss_train: 0.9590, acc_train: 0.8333, time: 0.5771s\n",
      "batch: 054, loss_train: 0.0459, acc_train: 1.0000, time: 0.5776s\n",
      "batch: 055, loss_train: 0.0091, acc_train: 1.0000, time: 0.5773s\n",
      "batch: 056, loss_train: 0.0173, acc_train: 1.0000, time: 0.5780s\n",
      "batch: 057, loss_train: 0.0416, acc_train: 1.0000, time: 0.5674s\n",
      "batch: 058, loss_train: 0.0212, acc_train: 1.0000, time: 0.5683s\n",
      "batch: 059, loss_train: 0.2770, acc_train: 0.9000, time: 0.5679s\n",
      "batch: 060, loss_train: 0.0149, acc_train: 1.0000, time: 0.5683s\n",
      "batch: 061, loss_train: 0.0187, acc_train: 1.0000, time: 0.5774s\n",
      "batch: 062, loss_train: 0.5054, acc_train: 0.8933, time: 0.5686s\n",
      "batch: 063, loss_train: 0.0135, acc_train: 1.0000, time: 0.5742s\n",
      "batch: 064, loss_train: 0.0068, acc_train: 1.0000, time: 0.5679s\n",
      "batch: 065, loss_train: 0.0123, acc_train: 1.0000, time: 0.5682s\n",
      "batch: 066, loss_train: 0.5261, acc_train: 0.8667, time: 0.5769s\n",
      "batch: 067, loss_train: 1.3107, acc_train: 0.8600, time: 0.5679s\n",
      "batch: 068, loss_train: 0.2378, acc_train: 0.8933, time: 0.5679s\n",
      "batch: 069, loss_train: 0.0330, acc_train: 1.0000, time: 0.5769s\n",
      "batch: 070, loss_train: 0.0387, acc_train: 1.0000, time: 0.5687s\n",
      "batch: 071, loss_train: 0.0757, acc_train: 1.0000, time: 0.5772s\n",
      "batch: 072, loss_train: 0.0165, acc_train: 1.0000, time: 0.5752s\n",
      "batch: 073, loss_train: 0.2086, acc_train: 0.9000, time: 0.5768s\n",
      "batch: 074, loss_train: 0.0744, acc_train: 1.0000, time: 0.5771s\n",
      "batch: 075, loss_train: 0.0091, acc_train: 1.0000, time: 0.5676s\n",
      "batch: 076, loss_train: 0.0124, acc_train: 1.0000, time: 0.5768s\n",
      "batch: 077, loss_train: 0.1122, acc_train: 0.9400, time: 0.5766s\n",
      "batch: 078, loss_train: 0.1145, acc_train: 0.8667, time: 0.5764s\n",
      "batch: 079, loss_train: 0.0115, acc_train: 1.0000, time: 0.5680s\n",
      "batch: 080, loss_train: 0.0100, acc_train: 1.0000, time: 0.5682s\n",
      "batch: 081, loss_train: 0.3385, acc_train: 0.9400, time: 0.5677s\n",
      "batch: 082, loss_train: 0.0102, acc_train: 1.0000, time: 0.5771s\n",
      "batch: 083, loss_train: 0.1906, acc_train: 0.9333, time: 0.5677s\n",
      "batch: 084, loss_train: 0.1689, acc_train: 0.9095, time: 0.5783s\n",
      "batch: 085, loss_train: 0.0113, acc_train: 1.0000, time: 0.5765s\n",
      "batch: 086, loss_train: 0.0647, acc_train: 1.0000, time: 0.5772s\n",
      "batch: 087, loss_train: 0.0331, acc_train: 1.0000, time: 0.5672s\n",
      "Validation after epoch 5:\n",
      "Validation set results: loss= 0.1104, accuracy= 0.9795, label acc= [1.         1.         0.95652174 1.         0.95652174 0.95238095\n",
      " 1.         0.94736842 1.        ]\n",
      "Epoch:  6\n",
      "Training data size: 874\n",
      "Number of batches: 87\n",
      "batch: 001, loss_train: 0.0149, acc_train: 1.0000, time: 0.5755s\n",
      "batch: 002, loss_train: 0.0204, acc_train: 1.0000, time: 0.5746s\n",
      "batch: 003, loss_train: 0.2685, acc_train: 0.8933, time: 0.5770s\n",
      "batch: 004, loss_train: 0.0215, acc_train: 1.0000, time: 0.5761s\n",
      "batch: 005, loss_train: 0.0114, acc_train: 1.0000, time: 0.5769s\n",
      "batch: 006, loss_train: 0.0256, acc_train: 1.0000, time: 0.5685s\n",
      "batch: 007, loss_train: 0.0287, acc_train: 1.0000, time: 0.5676s\n",
      "batch: 008, loss_train: 0.0140, acc_train: 1.0000, time: 0.5658s\n",
      "batch: 009, loss_train: 0.0144, acc_train: 1.0000, time: 0.5766s\n",
      "batch: 010, loss_train: 0.5747, acc_train: 0.8600, time: 0.5679s\n",
      "batch: 011, loss_train: 0.0151, acc_train: 1.0000, time: 0.5685s\n",
      "batch: 012, loss_train: 0.0355, acc_train: 1.0000, time: 0.5682s\n",
      "batch: 013, loss_train: 0.0423, acc_train: 1.0000, time: 0.5768s\n",
      "batch: 014, loss_train: 0.1702, acc_train: 0.9000, time: 0.5678s\n",
      "batch: 015, loss_train: 0.0299, acc_train: 1.0000, time: 0.5676s\n",
      "batch: 016, loss_train: 0.1416, acc_train: 1.0000, time: 0.5678s\n",
      "batch: 017, loss_train: 0.0126, acc_train: 1.0000, time: 0.5687s\n",
      "batch: 018, loss_train: 0.2305, acc_train: 0.9333, time: 0.5770s\n",
      "batch: 019, loss_train: 0.0141, acc_train: 1.0000, time: 0.5769s\n",
      "batch: 020, loss_train: 0.0113, acc_train: 1.0000, time: 0.5678s\n",
      "batch: 021, loss_train: 1.9116, acc_train: 0.8000, time: 0.5684s\n",
      "batch: 022, loss_train: 0.0065, acc_train: 1.0000, time: 0.5680s\n",
      "batch: 023, loss_train: 0.0069, acc_train: 1.0000, time: 0.5778s\n",
      "batch: 024, loss_train: 0.0076, acc_train: 1.0000, time: 0.5767s\n",
      "batch: 025, loss_train: 0.0524, acc_train: 1.0000, time: 0.5773s\n",
      "batch: 026, loss_train: 0.5643, acc_train: 0.7571, time: 0.5767s\n",
      "batch: 027, loss_train: 0.0484, acc_train: 1.0000, time: 0.5686s\n",
      "batch: 028, loss_train: 0.1118, acc_train: 1.0000, time: 0.5764s\n",
      "batch: 029, loss_train: 0.1080, acc_train: 1.0000, time: 0.5775s\n",
      "batch: 030, loss_train: 0.0317, acc_train: 1.0000, time: 0.5777s\n",
      "batch: 031, loss_train: 0.0441, acc_train: 1.0000, time: 0.5695s\n",
      "batch: 032, loss_train: 0.0021, acc_train: 1.0000, time: 0.5696s\n",
      "batch: 033, loss_train: 0.0805, acc_train: 1.0000, time: 0.5698s\n",
      "batch: 034, loss_train: 0.0131, acc_train: 1.0000, time: 0.5789s\n",
      "batch: 035, loss_train: 0.0175, acc_train: 1.0000, time: 0.5784s\n",
      "batch: 036, loss_train: 0.0508, acc_train: 1.0000, time: 0.5783s\n",
      "batch: 037, loss_train: 0.3968, acc_train: 0.8933, time: 0.5692s\n",
      "batch: 038, loss_train: 0.0539, acc_train: 1.0000, time: 0.5780s\n",
      "batch: 039, loss_train: 0.1423, acc_train: 0.9000, time: 0.5789s\n",
      "batch: 040, loss_train: 0.1806, acc_train: 0.8600, time: 0.5693s\n",
      "batch: 041, loss_train: 0.0184, acc_train: 1.0000, time: 0.5784s\n",
      "batch: 042, loss_train: 0.0184, acc_train: 1.0000, time: 0.5798s\n",
      "batch: 043, loss_train: 0.0064, acc_train: 1.0000, time: 0.5695s\n",
      "batch: 044, loss_train: 0.0192, acc_train: 1.0000, time: 0.5792s\n",
      "batch: 045, loss_train: 0.0278, acc_train: 1.0000, time: 0.5789s\n",
      "batch: 046, loss_train: 0.0132, acc_train: 1.0000, time: 0.5786s\n",
      "batch: 047, loss_train: 0.0081, acc_train: 1.0000, time: 0.5695s\n",
      "batch: 048, loss_train: 0.0207, acc_train: 1.0000, time: 0.5777s\n",
      "batch: 049, loss_train: 0.0054, acc_train: 1.0000, time: 0.5786s\n",
      "batch: 050, loss_train: 0.0165, acc_train: 1.0000, time: 0.5688s\n",
      "batch: 051, loss_train: 0.0100, acc_train: 1.0000, time: 0.5777s\n",
      "batch: 052, loss_train: 0.0147, acc_train: 1.0000, time: 0.5786s\n",
      "batch: 053, loss_train: 0.2200, acc_train: 0.9000, time: 0.5708s\n",
      "batch: 054, loss_train: 0.0429, acc_train: 1.0000, time: 0.5816s\n",
      "batch: 055, loss_train: 0.0138, acc_train: 1.0000, time: 0.5828s\n",
      "batch: 056, loss_train: 0.0101, acc_train: 1.0000, time: 0.5824s\n",
      "batch: 057, loss_train: 0.0045, acc_train: 1.0000, time: 0.5701s\n",
      "batch: 058, loss_train: 0.0101, acc_train: 1.0000, time: 0.5731s\n",
      "batch: 059, loss_train: 0.0006, acc_train: 1.0000, time: 0.5699s\n",
      "batch: 060, loss_train: 0.0327, acc_train: 1.0000, time: 0.5697s\n",
      "batch: 061, loss_train: 0.0754, acc_train: 1.0000, time: 0.5813s\n",
      "batch: 062, loss_train: 0.0148, acc_train: 1.0000, time: 0.5780s\n",
      "batch: 063, loss_train: 0.0147, acc_train: 1.0000, time: 0.5774s\n",
      "batch: 064, loss_train: 0.5569, acc_train: 0.8600, time: 0.5772s\n",
      "batch: 065, loss_train: 0.0100, acc_train: 1.0000, time: 0.5693s\n",
      "batch: 066, loss_train: 0.0156, acc_train: 1.0000, time: 0.5696s\n",
      "batch: 067, loss_train: 0.1080, acc_train: 0.9095, time: 0.5696s\n",
      "batch: 068, loss_train: 0.0089, acc_train: 1.0000, time: 0.5776s\n",
      "batch: 069, loss_train: 0.0342, acc_train: 1.0000, time: 0.5787s\n",
      "batch: 070, loss_train: 0.0133, acc_train: 1.0000, time: 0.5769s\n",
      "batch: 071, loss_train: 0.1028, acc_train: 0.9000, time: 0.5784s\n",
      "batch: 072, loss_train: 0.0041, acc_train: 1.0000, time: 0.5690s\n",
      "batch: 073, loss_train: 0.4162, acc_train: 0.9333, time: 0.5778s\n",
      "batch: 074, loss_train: 0.0076, acc_train: 1.0000, time: 0.5697s\n",
      "batch: 075, loss_train: 0.3088, acc_train: 0.9400, time: 0.5786s\n",
      "batch: 076, loss_train: 0.0043, acc_train: 1.0000, time: 0.5702s\n",
      "batch: 077, loss_train: 0.0035, acc_train: 1.0000, time: 0.5695s\n",
      "batch: 078, loss_train: 0.0299, acc_train: 1.0000, time: 0.5784s\n",
      "batch: 079, loss_train: 0.0107, acc_train: 1.0000, time: 0.5770s\n",
      "batch: 080, loss_train: 0.0198, acc_train: 1.0000, time: 0.5791s\n",
      "batch: 081, loss_train: 0.0140, acc_train: 1.0000, time: 0.5781s\n",
      "batch: 082, loss_train: 0.0326, acc_train: 1.0000, time: 0.5754s\n",
      "batch: 083, loss_train: 0.1136, acc_train: 0.8667, time: 0.5778s\n",
      "batch: 084, loss_train: 0.0142, acc_train: 1.0000, time: 0.5781s\n",
      "batch: 085, loss_train: 0.0166, acc_train: 1.0000, time: 0.5688s\n",
      "batch: 086, loss_train: 0.0097, acc_train: 1.0000, time: 0.5786s\n",
      "batch: 087, loss_train: 0.0122, acc_train: 1.0000, time: 0.5788s\n",
      "Validation after epoch 6:\n",
      "Validation set results: loss= 0.0758, accuracy= 0.9795, label acc= [1.         0.95238095 1.         0.95652174 1.         0.95238095\n",
      " 0.95652174 1.         1.        ]\n",
      "Epoch:  7\n",
      "Training data size: 874\n",
      "Number of batches: 87\n",
      "batch: 001, loss_train: 0.0095, acc_train: 1.0000, time: 0.5755s\n",
      "batch: 002, loss_train: 0.0641, acc_train: 1.0000, time: 0.5765s\n",
      "batch: 003, loss_train: 0.0089, acc_train: 1.0000, time: 0.5793s\n",
      "batch: 004, loss_train: 0.0038, acc_train: 1.0000, time: 0.5792s\n",
      "batch: 005, loss_train: 0.0013, acc_train: 1.0000, time: 0.5687s\n",
      "batch: 006, loss_train: 0.0239, acc_train: 1.0000, time: 0.5789s\n",
      "batch: 007, loss_train: 0.7160, acc_train: 0.9000, time: 0.5694s\n",
      "batch: 008, loss_train: 0.0135, acc_train: 1.0000, time: 0.5787s\n",
      "batch: 009, loss_train: 0.0073, acc_train: 1.0000, time: 0.5785s\n",
      "batch: 010, loss_train: 0.0651, acc_train: 1.0000, time: 0.5686s\n",
      "batch: 011, loss_train: 0.0124, acc_train: 1.0000, time: 0.5779s\n",
      "batch: 012, loss_train: 0.0061, acc_train: 1.0000, time: 0.5694s\n",
      "batch: 013, loss_train: 0.0330, acc_train: 1.0000, time: 0.5790s\n",
      "batch: 014, loss_train: 0.0082, acc_train: 1.0000, time: 0.5781s\n",
      "batch: 015, loss_train: 0.5004, acc_train: 0.8571, time: 0.5774s\n",
      "batch: 016, loss_train: 0.0103, acc_train: 1.0000, time: 0.5842s\n",
      "batch: 017, loss_train: 0.0549, acc_train: 1.0000, time: 0.5704s\n",
      "batch: 018, loss_train: 0.0107, acc_train: 1.0000, time: 0.5797s\n",
      "batch: 019, loss_train: 0.1479, acc_train: 1.0000, time: 0.5790s\n",
      "batch: 020, loss_train: 0.0101, acc_train: 1.0000, time: 0.5717s\n",
      "batch: 021, loss_train: 0.0014, acc_train: 1.0000, time: 0.5687s\n",
      "batch: 022, loss_train: 0.0187, acc_train: 1.0000, time: 0.5720s\n",
      "batch: 023, loss_train: 0.0139, acc_train: 1.0000, time: 0.5813s\n",
      "batch: 024, loss_train: 0.0099, acc_train: 1.0000, time: 0.5818s\n",
      "batch: 025, loss_train: 0.0146, acc_train: 1.0000, time: 0.5743s\n",
      "batch: 026, loss_train: 0.0124, acc_train: 1.0000, time: 0.5802s\n",
      "batch: 027, loss_train: 0.3385, acc_train: 0.9333, time: 0.5804s\n",
      "batch: 028, loss_train: 0.0030, acc_train: 1.0000, time: 0.5715s\n",
      "batch: 029, loss_train: 0.0087, acc_train: 1.0000, time: 0.5810s\n",
      "batch: 030, loss_train: 0.0058, acc_train: 1.0000, time: 0.5708s\n",
      "batch: 031, loss_train: 0.0122, acc_train: 1.0000, time: 0.5772s\n",
      "batch: 032, loss_train: 0.0026, acc_train: 1.0000, time: 0.5700s\n",
      "batch: 033, loss_train: 0.0052, acc_train: 1.0000, time: 0.5789s\n",
      "batch: 034, loss_train: 0.9185, acc_train: 0.9067, time: 0.5693s\n",
      "batch: 035, loss_train: 0.0168, acc_train: 1.0000, time: 0.5786s\n",
      "batch: 036, loss_train: 0.0180, acc_train: 1.0000, time: 0.5700s\n",
      "batch: 037, loss_train: 0.0065, acc_train: 1.0000, time: 0.5699s\n",
      "batch: 038, loss_train: 0.0136, acc_train: 1.0000, time: 0.5783s\n",
      "batch: 039, loss_train: 0.0073, acc_train: 1.0000, time: 0.5791s\n",
      "batch: 040, loss_train: 0.0104, acc_train: 1.0000, time: 0.5793s\n",
      "batch: 041, loss_train: 0.5391, acc_train: 0.9400, time: 0.5793s\n",
      "batch: 042, loss_train: 0.0182, acc_train: 1.0000, time: 0.5778s\n",
      "batch: 043, loss_train: 0.0249, acc_train: 1.0000, time: 0.5704s\n",
      "batch: 044, loss_train: 0.0061, acc_train: 1.0000, time: 0.5766s\n",
      "batch: 045, loss_train: 0.0063, acc_train: 1.0000, time: 0.5732s\n",
      "batch: 046, loss_train: 0.0082, acc_train: 1.0000, time: 0.5817s\n",
      "batch: 047, loss_train: 0.3502, acc_train: 0.9000, time: 0.5816s\n",
      "batch: 048, loss_train: 0.0072, acc_train: 1.0000, time: 0.5831s\n",
      "batch: 049, loss_train: 0.0246, acc_train: 1.0000, time: 0.5732s\n",
      "batch: 050, loss_train: 0.0135, acc_train: 1.0000, time: 0.5834s\n",
      "batch: 051, loss_train: 0.5371, acc_train: 0.8667, time: 0.5826s\n",
      "batch: 052, loss_train: 0.0598, acc_train: 1.0000, time: 0.5833s\n",
      "batch: 053, loss_train: 0.0608, acc_train: 1.0000, time: 0.5761s\n",
      "batch: 054, loss_train: 0.0990, acc_train: 1.0000, time: 0.5831s\n",
      "batch: 055, loss_train: 0.0137, acc_train: 1.0000, time: 0.5726s\n",
      "batch: 056, loss_train: 0.3358, acc_train: 0.9333, time: 0.5847s\n",
      "batch: 057, loss_train: 0.0932, acc_train: 1.0000, time: 0.5903s\n",
      "batch: 058, loss_train: 0.0152, acc_train: 1.0000, time: 0.5837s\n",
      "batch: 059, loss_train: 0.0884, acc_train: 1.0000, time: 0.5779s\n",
      "batch: 060, loss_train: 0.1436, acc_train: 0.9000, time: 0.5801s\n",
      "batch: 061, loss_train: 0.0966, acc_train: 1.0000, time: 0.5710s\n",
      "batch: 062, loss_train: 0.0124, acc_train: 1.0000, time: 0.5709s\n",
      "batch: 063, loss_train: 0.0082, acc_train: 1.0000, time: 0.5802s\n",
      "batch: 064, loss_train: 0.0046, acc_train: 1.0000, time: 0.5713s\n",
      "batch: 065, loss_train: 0.0011, acc_train: 1.0000, time: 0.5721s\n",
      "batch: 066, loss_train: 0.0214, acc_train: 1.0000, time: 0.5804s\n",
      "batch: 067, loss_train: 0.3401, acc_train: 0.9333, time: 0.5810s\n",
      "batch: 068, loss_train: 0.0095, acc_train: 1.0000, time: 0.5800s\n",
      "batch: 069, loss_train: 0.0055, acc_train: 1.0000, time: 0.5795s\n",
      "batch: 070, loss_train: 0.0007, acc_train: 1.0000, time: 0.5745s\n",
      "batch: 071, loss_train: 0.0614, acc_train: 1.0000, time: 0.5704s\n",
      "batch: 072, loss_train: 0.0091, acc_train: 1.0000, time: 0.5845s\n",
      "batch: 073, loss_train: 0.0020, acc_train: 1.0000, time: 0.5725s\n",
      "batch: 074, loss_train: 0.0096, acc_train: 1.0000, time: 0.5724s\n",
      "batch: 075, loss_train: 0.0089, acc_train: 1.0000, time: 0.5800s\n",
      "batch: 076, loss_train: 0.0162, acc_train: 1.0000, time: 0.5711s\n",
      "batch: 077, loss_train: 0.0297, acc_train: 1.0000, time: 0.5795s\n",
      "batch: 078, loss_train: 0.0124, acc_train: 1.0000, time: 0.5797s\n",
      "batch: 079, loss_train: 0.0010, acc_train: 1.0000, time: 0.5706s\n",
      "batch: 080, loss_train: 0.0144, acc_train: 1.0000, time: 0.5795s\n",
      "batch: 081, loss_train: 0.0096, acc_train: 1.0000, time: 0.5796s\n",
      "batch: 082, loss_train: 0.0403, acc_train: 1.0000, time: 0.5800s\n",
      "batch: 083, loss_train: 0.0048, acc_train: 1.0000, time: 0.5797s\n",
      "batch: 084, loss_train: 0.0098, acc_train: 1.0000, time: 0.5696s\n",
      "batch: 085, loss_train: 0.0068, acc_train: 1.0000, time: 0.5801s\n",
      "batch: 086, loss_train: 0.0129, acc_train: 1.0000, time: 0.5803s\n",
      "batch: 087, loss_train: 0.0214, acc_train: 1.0000, time: 0.5710s\n",
      "Validation after epoch 7:\n",
      "Validation set results: loss= 0.0418, accuracy= 0.9898, label acc= [1.         1.         1.         1.         1.         0.95238095\n",
      " 0.95652174 1.         1.        ]\n",
      "Epoch:  8\n",
      "Training data size: 874\n",
      "Number of batches: 87\n",
      "batch: 001, loss_train: 0.0132, acc_train: 1.0000, time: 0.5670s\n",
      "batch: 002, loss_train: 0.0108, acc_train: 1.0000, time: 0.5774s\n",
      "batch: 003, loss_train: 0.0047, acc_train: 1.0000, time: 0.5711s\n",
      "batch: 004, loss_train: 0.0206, acc_train: 1.0000, time: 0.5727s\n",
      "batch: 005, loss_train: 0.0086, acc_train: 1.0000, time: 0.5795s\n",
      "batch: 006, loss_train: 0.0057, acc_train: 1.0000, time: 0.5705s\n",
      "batch: 007, loss_train: 0.0084, acc_train: 1.0000, time: 0.5793s\n",
      "batch: 008, loss_train: 0.0029, acc_train: 1.0000, time: 0.5792s\n",
      "batch: 009, loss_train: 0.0215, acc_train: 1.0000, time: 0.5793s\n",
      "batch: 010, loss_train: 0.0036, acc_train: 1.0000, time: 0.5712s\n",
      "batch: 011, loss_train: 0.0031, acc_train: 1.0000, time: 0.5698s\n",
      "batch: 012, loss_train: 0.0283, acc_train: 1.0000, time: 0.5794s\n",
      "batch: 013, loss_train: 0.0081, acc_train: 1.0000, time: 0.5797s\n",
      "batch: 014, loss_train: 0.0070, acc_train: 1.0000, time: 0.5702s\n",
      "batch: 015, loss_train: 0.0046, acc_train: 1.0000, time: 0.5795s\n",
      "batch: 016, loss_train: 0.0039, acc_train: 1.0000, time: 0.5795s\n",
      "batch: 017, loss_train: 0.0107, acc_train: 1.0000, time: 0.5797s\n",
      "batch: 018, loss_train: 0.0066, acc_train: 1.0000, time: 0.5790s\n",
      "batch: 019, loss_train: 0.0209, acc_train: 1.0000, time: 0.5706s\n",
      "batch: 020, loss_train: 0.0300, acc_train: 1.0000, time: 0.5821s\n",
      "batch: 021, loss_train: 0.0417, acc_train: 1.0000, time: 0.5707s\n",
      "batch: 022, loss_train: 0.3878, acc_train: 0.8600, time: 0.5795s\n",
      "batch: 023, loss_train: 0.0102, acc_train: 1.0000, time: 0.5795s\n",
      "batch: 024, loss_train: 0.0023, acc_train: 1.0000, time: 0.5795s\n",
      "batch: 025, loss_train: 0.0277, acc_train: 1.0000, time: 0.5707s\n",
      "batch: 026, loss_train: 0.1076, acc_train: 0.9000, time: 0.5774s\n",
      "batch: 027, loss_train: 0.0002, acc_train: 1.0000, time: 0.5703s\n",
      "batch: 028, loss_train: 0.0115, acc_train: 1.0000, time: 0.5696s\n",
      "batch: 029, loss_train: 0.0243, acc_train: 1.0000, time: 0.5703s\n",
      "batch: 030, loss_train: 0.0424, acc_train: 1.0000, time: 0.5791s\n",
      "batch: 031, loss_train: 0.0414, acc_train: 1.0000, time: 0.5696s\n",
      "batch: 032, loss_train: 0.0002, acc_train: 1.0000, time: 0.5700s\n",
      "batch: 033, loss_train: 0.1966, acc_train: 0.9400, time: 0.5706s\n",
      "batch: 034, loss_train: 0.0023, acc_train: 1.0000, time: 0.5703s\n",
      "batch: 035, loss_train: 0.0021, acc_train: 1.0000, time: 0.5788s\n",
      "batch: 036, loss_train: 0.0113, acc_train: 1.0000, time: 0.5788s\n",
      "batch: 037, loss_train: 0.0057, acc_train: 1.0000, time: 0.5783s\n",
      "batch: 038, loss_train: 0.0106, acc_train: 1.0000, time: 0.5805s\n",
      "batch: 039, loss_train: 0.0079, acc_train: 1.0000, time: 0.5787s\n",
      "batch: 040, loss_train: 0.0107, acc_train: 1.0000, time: 0.5789s\n",
      "batch: 041, loss_train: 0.2954, acc_train: 0.8933, time: 0.5824s\n",
      "batch: 042, loss_train: 0.0121, acc_train: 1.0000, time: 0.5791s\n",
      "batch: 043, loss_train: 0.0022, acc_train: 1.0000, time: 0.5698s\n",
      "batch: 044, loss_train: 0.0059, acc_train: 1.0000, time: 0.5794s\n",
      "batch: 045, loss_train: 0.0298, acc_train: 1.0000, time: 0.5710s\n",
      "batch: 046, loss_train: 0.0381, acc_train: 1.0000, time: 0.5665s\n",
      "batch: 047, loss_train: 0.0008, acc_train: 1.0000, time: 0.5683s\n",
      "batch: 048, loss_train: 0.0394, acc_train: 1.0000, time: 0.5792s\n",
      "batch: 049, loss_train: 0.0082, acc_train: 1.0000, time: 0.5790s\n",
      "batch: 050, loss_train: 0.0065, acc_train: 1.0000, time: 0.5788s\n",
      "batch: 051, loss_train: 0.3619, acc_train: 0.9000, time: 0.5794s\n",
      "batch: 052, loss_train: 0.2020, acc_train: 0.9095, time: 0.5792s\n",
      "batch: 053, loss_train: 0.0004, acc_train: 1.0000, time: 0.5690s\n",
      "batch: 054, loss_train: 0.0057, acc_train: 1.0000, time: 0.5779s\n",
      "batch: 055, loss_train: 0.0629, acc_train: 1.0000, time: 0.5725s\n",
      "batch: 056, loss_train: 0.0113, acc_train: 1.0000, time: 0.5812s\n",
      "batch: 057, loss_train: 0.0331, acc_train: 1.0000, time: 0.5730s\n",
      "batch: 058, loss_train: 0.0567, acc_train: 1.0000, time: 0.5825s\n",
      "batch: 059, loss_train: 0.2280, acc_train: 0.9333, time: 0.5823s\n",
      "batch: 060, loss_train: 0.0018, acc_train: 1.0000, time: 0.5863s\n",
      "batch: 061, loss_train: 0.0099, acc_train: 1.0000, time: 0.5817s\n",
      "batch: 062, loss_train: 0.0087, acc_train: 1.0000, time: 0.5712s\n",
      "batch: 063, loss_train: 0.0492, acc_train: 1.0000, time: 0.5817s\n",
      "batch: 064, loss_train: 0.0301, acc_train: 1.0000, time: 0.5797s\n",
      "batch: 065, loss_train: 0.0978, acc_train: 1.0000, time: 0.5724s\n",
      "batch: 066, loss_train: 0.0084, acc_train: 1.0000, time: 0.5714s\n",
      "batch: 067, loss_train: 0.0100, acc_train: 1.0000, time: 0.5716s\n",
      "batch: 068, loss_train: 0.0086, acc_train: 1.0000, time: 0.5809s\n",
      "batch: 069, loss_train: 0.0113, acc_train: 1.0000, time: 0.5839s\n",
      "batch: 070, loss_train: 0.0310, acc_train: 1.0000, time: 0.5851s\n",
      "batch: 071, loss_train: 0.0002, acc_train: 1.0000, time: 0.5757s\n",
      "batch: 072, loss_train: 0.0035, acc_train: 1.0000, time: 0.5836s\n",
      "batch: 073, loss_train: 0.7356, acc_train: 0.8000, time: 0.5756s\n",
      "batch: 074, loss_train: 0.1682, acc_train: 0.9333, time: 0.5740s\n",
      "batch: 075, loss_train: 0.0081, acc_train: 1.0000, time: 0.5852s\n",
      "batch: 076, loss_train: 0.0079, acc_train: 1.0000, time: 0.5836s\n",
      "batch: 077, loss_train: 0.0034, acc_train: 1.0000, time: 0.5807s\n",
      "batch: 078, loss_train: 0.2614, acc_train: 0.9095, time: 0.5710s\n",
      "batch: 079, loss_train: 0.0031, acc_train: 1.0000, time: 0.5729s\n",
      "batch: 080, loss_train: 0.0052, acc_train: 1.0000, time: 0.5833s\n",
      "batch: 081, loss_train: 0.0074, acc_train: 1.0000, time: 0.5844s\n",
      "batch: 082, loss_train: 0.0493, acc_train: 1.0000, time: 0.5845s\n",
      "batch: 083, loss_train: 0.0042, acc_train: 1.0000, time: 0.5824s\n",
      "batch: 084, loss_train: 0.0020, acc_train: 1.0000, time: 0.5728s\n",
      "batch: 085, loss_train: 0.0106, acc_train: 1.0000, time: 0.5814s\n",
      "batch: 086, loss_train: 0.0004, acc_train: 1.0000, time: 0.5769s\n",
      "batch: 087, loss_train: 0.0102, acc_train: 1.0000, time: 0.5854s\n",
      "Validation after epoch 8:\n",
      "Validation set results: loss= 0.0836, accuracy= 0.9692, label acc= [1.         0.9        1.         1.         1.         1.\n",
      " 0.95652174 0.94736842 0.91666667]\n",
      "Epoch:  9\n",
      "Training data size: 874\n",
      "Number of batches: 87\n",
      "batch: 001, loss_train: 0.0032, acc_train: 1.0000, time: 0.5758s\n",
      "batch: 002, loss_train: 0.0021, acc_train: 1.0000, time: 0.5780s\n",
      "batch: 003, loss_train: 0.0052, acc_train: 1.0000, time: 0.5796s\n",
      "batch: 004, loss_train: 0.0052, acc_train: 1.0000, time: 0.5801s\n",
      "batch: 005, loss_train: 0.2449, acc_train: 0.8600, time: 0.5794s\n",
      "batch: 006, loss_train: 0.0003, acc_train: 1.0000, time: 0.5715s\n",
      "batch: 007, loss_train: 0.0002, acc_train: 1.0000, time: 0.5713s\n",
      "batch: 008, loss_train: 0.0041, acc_train: 1.0000, time: 0.5690s\n",
      "batch: 009, loss_train: 0.0040, acc_train: 1.0000, time: 0.5796s\n",
      "batch: 010, loss_train: 0.0093, acc_train: 1.0000, time: 0.5800s\n",
      "batch: 011, loss_train: 0.0326, acc_train: 1.0000, time: 0.5798s\n",
      "batch: 012, loss_train: 0.0053, acc_train: 1.0000, time: 0.5699s\n",
      "batch: 013, loss_train: 0.0071, acc_train: 1.0000, time: 0.5774s\n",
      "batch: 014, loss_train: 0.0001, acc_train: 1.0000, time: 0.5703s\n",
      "batch: 015, loss_train: 0.3552, acc_train: 0.9400, time: 0.5792s\n",
      "batch: 016, loss_train: 0.0062, acc_train: 1.0000, time: 0.5795s\n",
      "batch: 017, loss_train: 0.0052, acc_train: 1.0000, time: 0.5799s\n",
      "batch: 018, loss_train: 0.0053, acc_train: 1.0000, time: 0.5699s\n",
      "batch: 019, loss_train: 0.0086, acc_train: 1.0000, time: 0.5702s\n",
      "batch: 020, loss_train: 0.0091, acc_train: 1.0000, time: 0.5788s\n",
      "batch: 021, loss_train: 0.0241, acc_train: 1.0000, time: 0.5683s\n",
      "batch: 022, loss_train: 0.0081, acc_train: 1.0000, time: 0.5783s\n",
      "batch: 023, loss_train: 0.0091, acc_train: 1.0000, time: 0.5786s\n",
      "batch: 024, loss_train: 0.0028, acc_train: 1.0000, time: 0.5709s\n",
      "batch: 025, loss_train: 0.0070, acc_train: 1.0000, time: 0.5822s\n",
      "batch: 026, loss_train: 0.0049, acc_train: 1.0000, time: 0.5704s\n",
      "batch: 027, loss_train: 0.0280, acc_train: 1.0000, time: 0.5798s\n",
      "batch: 028, loss_train: 0.0100, acc_train: 1.0000, time: 0.5771s\n",
      "batch: 029, loss_train: 1.0519, acc_train: 0.9000, time: 0.5700s\n",
      "batch: 030, loss_train: 0.0016, acc_train: 1.0000, time: 0.5795s\n",
      "batch: 031, loss_train: 0.0002, acc_train: 1.0000, time: 0.5703s\n",
      "batch: 032, loss_train: 0.0082, acc_train: 1.0000, time: 0.5789s\n",
      "batch: 033, loss_train: 0.0051, acc_train: 1.0000, time: 0.5701s\n",
      "batch: 034, loss_train: 0.0016, acc_train: 1.0000, time: 0.5796s\n",
      "batch: 035, loss_train: 0.0069, acc_train: 1.0000, time: 0.5791s\n",
      "batch: 036, loss_train: 0.0172, acc_train: 1.0000, time: 0.5785s\n",
      "batch: 037, loss_train: 0.0027, acc_train: 1.0000, time: 0.5707s\n",
      "batch: 038, loss_train: 0.0062, acc_train: 1.0000, time: 0.5828s\n",
      "batch: 039, loss_train: 0.0064, acc_train: 1.0000, time: 0.5823s\n",
      "batch: 040, loss_train: 0.0382, acc_train: 1.0000, time: 0.5820s\n",
      "batch: 041, loss_train: 0.0065, acc_train: 1.0000, time: 0.5719s\n",
      "batch: 042, loss_train: 0.0480, acc_train: 1.0000, time: 0.5738s\n",
      "batch: 043, loss_train: 0.0071, acc_train: 1.0000, time: 0.5834s\n",
      "batch: 044, loss_train: 0.0059, acc_train: 1.0000, time: 0.5814s\n",
      "batch: 045, loss_train: 0.0047, acc_train: 1.0000, time: 0.5697s\n",
      "batch: 046, loss_train: 0.0015, acc_train: 1.0000, time: 0.5792s\n",
      "batch: 047, loss_train: 0.0030, acc_train: 1.0000, time: 0.5711s\n",
      "batch: 048, loss_train: 0.0031, acc_train: 1.0000, time: 0.5804s\n",
      "batch: 049, loss_train: 0.0195, acc_train: 1.0000, time: 0.5840s\n",
      "batch: 050, loss_train: 0.0038, acc_train: 1.0000, time: 0.5835s\n",
      "batch: 051, loss_train: 0.0035, acc_train: 1.0000, time: 0.5725s\n",
      "batch: 052, loss_train: 0.0034, acc_train: 1.0000, time: 0.5739s\n",
      "batch: 053, loss_train: 0.0241, acc_train: 1.0000, time: 0.5808s\n",
      "batch: 054, loss_train: 0.0049, acc_train: 1.0000, time: 0.5747s\n",
      "batch: 055, loss_train: 0.0001, acc_train: 1.0000, time: 0.5758s\n",
      "batch: 056, loss_train: 0.0055, acc_train: 1.0000, time: 0.5816s\n",
      "batch: 057, loss_train: 0.0054, acc_train: 1.0000, time: 0.5731s\n",
      "batch: 058, loss_train: 0.0047, acc_train: 1.0000, time: 0.5715s\n",
      "batch: 059, loss_train: 0.0119, acc_train: 1.0000, time: 0.5693s\n",
      "batch: 060, loss_train: 0.0041, acc_train: 1.0000, time: 0.5711s\n",
      "batch: 061, loss_train: 0.0006, acc_train: 1.0000, time: 0.5717s\n",
      "batch: 062, loss_train: 0.0046, acc_train: 1.0000, time: 0.5810s\n",
      "batch: 063, loss_train: 0.0029, acc_train: 1.0000, time: 0.5819s\n",
      "batch: 064, loss_train: 0.0018, acc_train: 1.0000, time: 0.5704s\n",
      "batch: 065, loss_train: 0.0133, acc_train: 1.0000, time: 0.5771s\n",
      "batch: 066, loss_train: 0.0044, acc_train: 1.0000, time: 0.5804s\n",
      "batch: 067, loss_train: 0.0141, acc_train: 1.0000, time: 0.5806s\n",
      "batch: 068, loss_train: 0.0235, acc_train: 1.0000, time: 0.5810s\n",
      "batch: 069, loss_train: 0.0153, acc_train: 1.0000, time: 0.5808s\n",
      "batch: 070, loss_train: 0.0036, acc_train: 1.0000, time: 0.5813s\n",
      "batch: 071, loss_train: 0.0264, acc_train: 1.0000, time: 0.5800s\n",
      "batch: 072, loss_train: 0.0015, acc_train: 1.0000, time: 0.5731s\n",
      "batch: 073, loss_train: 0.0037, acc_train: 1.0000, time: 0.5741s\n",
      "batch: 074, loss_train: 0.0060, acc_train: 1.0000, time: 0.5741s\n",
      "batch: 075, loss_train: 0.0085, acc_train: 1.0000, time: 0.5843s\n",
      "batch: 076, loss_train: 0.0028, acc_train: 1.0000, time: 0.5851s\n",
      "batch: 077, loss_train: 0.0042, acc_train: 1.0000, time: 0.5832s\n",
      "batch: 078, loss_train: 0.0050, acc_train: 1.0000, time: 0.5820s\n",
      "batch: 079, loss_train: 0.0122, acc_train: 1.0000, time: 0.5825s\n",
      "batch: 080, loss_train: 0.0015, acc_train: 1.0000, time: 0.5729s\n",
      "batch: 081, loss_train: 1.0346, acc_train: 0.8667, time: 0.5797s\n",
      "batch: 082, loss_train: 0.0001, acc_train: 1.0000, time: 0.5681s\n",
      "batch: 083, loss_train: 0.0039, acc_train: 1.0000, time: 0.5795s\n",
      "batch: 084, loss_train: 0.0043, acc_train: 1.0000, time: 0.5798s\n",
      "batch: 085, loss_train: 0.0029, acc_train: 1.0000, time: 0.5801s\n",
      "batch: 086, loss_train: 0.4330, acc_train: 0.9067, time: 0.5713s\n",
      "batch: 087, loss_train: 0.0060, acc_train: 1.0000, time: 0.5788s\n",
      "Validation after epoch 9:\n",
      "Validation set results: loss= 0.0081, accuracy= 1.0000, label acc= [1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Test set results: loss= 0.5793, accuracy= 0.9191, label acc= [1.         0.66666667 0.92307692 0.91666667 0.91666667 0.88888889\n",
      " 1.         1.         0.96      ]\n",
      "Model saved to model/20241109-162008_llm_w_edgefeat.pth\n"
     ]
    }
   ],
   "source": [
    "### import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pickle\n",
    "import math\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "class LLMGraphTransformer(nn.Module):\n",
    "    def __init__(self, model_name=\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\", device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # Load the tokenizer and model for TinyLlama\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(self.device)\n",
    "\n",
    "        # Ensure padding token is set for TinyLlama\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "        # New layers to process edge features\n",
    "        self.edge_fc = nn.Linear(77, 64).to(self.device)  \n",
    "        self.edge_dropout = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self, batch_text, edge_features):\n",
    "        # Tokenize text\n",
    "        inputs = self.tokenizer(batch_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
    "        outputs = self.model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "        \n",
    "        # Get the logits for the last token in each sequence (for classification purposes)\n",
    "        text_logits = outputs.logits[:, -1, :]  # Only take the last token's logits for classification\n",
    "\n",
    "        # Process edge features through a fully connected layer\n",
    "        edge_emb = self.edge_fc(edge_features)  # Project edge features to a lower-dimensional space\n",
    "        edge_emb = self.edge_dropout(edge_emb)\n",
    "\n",
    "        # Concatenate the text logits and the edge feature embeddings\n",
    "        combined_logits = torch.cat((text_logits, edge_emb), dim=1)\n",
    "        \n",
    "        return combined_logits\n",
    "\n",
    "    def generate_text(self, graph_data, labels, max_new_tokens=50):\n",
    "        # Convert the graph adjacency list to text directly within this method\n",
    "        batch_text = []\n",
    "        for node, neighbors in enumerate(graph_data):\n",
    "            if isinstance(neighbors, (list, set, np.ndarray)):\n",
    "                for neighbor in neighbors:\n",
    "                    question = f\"What is the relationship between Node {node} and Node {neighbor}? Choices: {', '.join(labels)}.\"\n",
    "                    batch_text.append(question)\n",
    "            else:\n",
    "                question = f\"What is the relationship between Node {node} and Node {neighbors}? Choices: {', '.join(labels)}.\"\n",
    "                batch_text.append(question)\n",
    "\n",
    "        # Tokenize and generate predictions\n",
    "        inputs = self.tokenizer(batch_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
    "        outputs = self.model.generate(\n",
    "            inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "        generated_text = [self.tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "        return generated_text\n",
    "\n",
    "\n",
    "def balance_data(data, labels, n_samples_per_label):\n",
    "    # Find unique labels and their counts\n",
    "    random.seed(42)\n",
    "    label_groups = {}\n",
    "    for label in np.unique(labels):\n",
    "        label_indices = np.where(labels == label)[0]\n",
    "        # If the label has fewer samples than the target, we use replace=True to oversample.\n",
    "        sampled_indices = np.random.choice(label_indices, size=n_samples_per_label, replace=(len(label_indices) < n_samples_per_label))\n",
    "        label_groups[label] = sampled_indices\n",
    "\n",
    "    # Concatenate the balanced data\n",
    "    balanced_indices = np.concatenate(list(label_groups.values()))\n",
    "    balanced_data = data[balanced_indices]\n",
    "    balanced_labels = labels[balanced_indices]\n",
    "\n",
    "    return balanced_data, balanced_labels\n",
    "\n",
    "\n",
    "def process_llm_output(llm_output):\n",
    "    llm_output = llm_output.lower().strip()\n",
    "    label_mapping = {\n",
    "        'Normal':0, 'Audio-Streaming':1, 'Browsing':2, 'Chat':3, 'File-Transfer':4,\n",
    "        'Email':5, 'P2P':6, 'Video-Streaming':7, 'VOIP':8\n",
    "    }\n",
    "    for keyword, index in label_mapping.items():\n",
    "        if keyword in llm_output:\n",
    "            return index\n",
    "    return -1\n",
    "\n",
    "\n",
    "def save_data_splits(train, val, test, train_labels, val_labels, test_labels, path=\"data_splits/\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    with open(os.path.join(path, \"train.pkl\"), \"wb\") as f:\n",
    "        pickle.dump((train, train_labels), f)\n",
    "    with open(os.path.join(path, \"val.pkl\"), \"wb\") as f:\n",
    "        pickle.dump((val, val_labels), f)\n",
    "    with open(os.path.join(path, \"test.pkl\"), \"wb\") as f:\n",
    "        pickle.dump((test, test_labels), f)\n",
    "    print(\"Data splits and labels saved successfully.\")\n",
    "\n",
    "def load_data_splits(path=\"data_splits/\"):\n",
    "    with open(os.path.join(path, \"train.pkl\"), \"rb\") as f:\n",
    "        train, train_labels = pickle.load(f)\n",
    "    with open(os.path.join(path, \"val.pkl\"), \"rb\") as f:\n",
    "        val, val_labels = pickle.load(f)\n",
    "    with open(os.path.join(path, \"test.pkl\"), \"rb\") as f:\n",
    "        test, test_labels = pickle.load(f)\n",
    "    print(\"Data splits and labels loaded successfully.\")\n",
    "    return train, val, test, train_labels, val_labels, test_labels\n",
    "\n",
    "def fit(args):\n",
    "    data = args[\"dataset\"]\n",
    "    binary = args[\"binary\"]\n",
    "\n",
    "    # Update the path to use ../cyber_gnn/ instead of datasets/\n",
    "    path = \"datasets/\" + data\n",
    "    if not path.endswith('/'):\n",
    "        path += '/'\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load the data manually (edge_feat, label, adj, adj_lists, config)\n",
    "    edge_feat = np.load(path + \"edge_feat_scaled.npy\", allow_pickle=True)\n",
    "    edge_feat = torch.tensor(edge_feat, dtype=torch.float, device=device)\n",
    "\n",
    "    # Load the label for multiclass classification\n",
    "    label = np.load(path + \"label_mul.npy\", allow_pickle=True)\n",
    "    label = torch.tensor(label, dtype=torch.long, device=device)\n",
    "    adj = np.load(path + \"adj_random.npy\", allow_pickle=True)\n",
    "    with open(path + 'adj_random_list.dict', 'rb') as file:\n",
    "        adj_lists = pickle.load(file)\n",
    "\n",
    "    config = {\n",
    "        \"num_of_layers\": 3,\n",
    "        \"num_heads_per_layer\": [6, 6, 6],\n",
    "        \"num_features_per_layer\": [edge_feat.shape[1], 8, 8, 8],\n",
    "        \"num_identity_feats\": 8,\n",
    "        \"add_skip_connection\": False,\n",
    "        \"bias\": True,\n",
    "        \"dropout\": 0.2\n",
    "    }\n",
    "\n",
    "    # Initialize LLMGraphTransformer using TinyLlama\n",
    "    llm_graph_transformer = LLMGraphTransformer(model_name=\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\", device=device)\n",
    "\n",
    "    # Define labels for relationship types\n",
    "    labels = ['Normal', 'Audio-Streaming', 'Browsing', 'Chat', 'File-Transfer',\n",
    "            'Email', 'P2P', 'Video-Streaming', 'VOIP']\n",
    "\n",
    "    # Define the optimizer with Adam\n",
    "    optimizer = torch.optim.Adam(llm_graph_transformer.parameters(), lr=1e-5)\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    num_edges = len(edge_feat)\n",
    "    label_cpu = label.cpu().numpy()\n",
    "    unique, counts = np.unique(label_cpu, return_counts=True)\n",
    "\n",
    "    balanced_data, balanced_labels = balance_data(np.arange(num_edges), label_cpu, n_samples_per_label=120)\n",
    "\n",
    "    # Check if saved splits exist, else create and save them\n",
    "    if not os.path.exists(\"data_splits/train.pkl\"):\n",
    "        # Perform initial train-validation-test split and save the splits\n",
    "        train_val, test, train_val_labels, test_labels = train_test_split(\n",
    "            balanced_data, balanced_labels, test_size=0.1, stratify=balanced_labels, random_state=42\n",
    "        )\n",
    "        train, val, train_labels, val_labels = train_test_split(\n",
    "            train_val, train_val_labels, test_size=0.1, stratify=train_val_labels, random_state=42\n",
    "        )\n",
    "        save_data_splits(train, val, test, train_labels, val_labels, test_labels)\n",
    "    else:\n",
    "        # Load the saved splits and their labels for consistent use\n",
    "        train, val, test, train_labels, val_labels, test_labels = load_data_splits()\n",
    "\n",
    "    print(len(train), len(val), len(test))\n",
    "\n",
    "    # Print the distribution of labels for each set\n",
    "    print(\"Label distribution in Train Set:\")\n",
    "    unique_train, counts_train = np.unique(train_labels, return_counts=True)\n",
    "    print(dict(zip(unique_train, counts_train)))\n",
    "\n",
    "    print(\"Label distribution in Validation Set:\")\n",
    "    unique_val, counts_val = np.unique(val_labels, return_counts=True)\n",
    "    print(dict(zip(unique_val, counts_val)))\n",
    "\n",
    "    print(\"Label distribution in Test Set:\")\n",
    "    unique_test, counts_test = np.unique(test_labels, return_counts=True)\n",
    "    print(dict(zip(unique_test, counts_test)))\n",
    "\n",
    "    times = []\n",
    "    trainscores = []\n",
    "    valscores = []\n",
    "\n",
    "    for epoch in range(10):\n",
    "        print(\"Epoch: \", epoch)\n",
    "        random.shuffle(train)\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Print the number of batches\n",
    "        print(f\"Training data size: {len(train)}\")\n",
    "        print(f\"Number of batches: {len(train) // 10}\")\n",
    "        \n",
    "        for batch in range(int(len(train) / 10)):  # Batch size is 10\n",
    "            batch_edges = train[10 * batch:10 * (batch + 1)]\n",
    "            \n",
    "            if len(batch_edges) == 0:\n",
    "                print(f\"Skipping empty batch {batch + 1}\")\n",
    "                continue\n",
    "\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Convert batch_edges to text\n",
    "            batch_text = llm_graph_transformer.generate_text(batch_edges, labels, max_new_tokens=10)\n",
    "\n",
    "            # Generate logits from text inputs\n",
    "            edge_batch = edge_feat[batch_edges]\n",
    "            logits = llm_graph_transformer(batch_text, edge_batch)\n",
    "            \n",
    "            # Ensure logits and labels are both on the same device\n",
    "            logits = logits.to(device)\n",
    "            batch_labels = label[batch_edges].to(device)\n",
    "\n",
    "            # Calculate loss using logits and target labels\n",
    "            loss = loss_fn(logits, batch_labels)\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "            acc_train = f1_score(label_cpu[batch_edges], predicted_labels.cpu().numpy(), average=\"weighted\")\n",
    "\n",
    "            end_time = time.time()\n",
    "            times.append(end_time - start_time)\n",
    "            trainscores.append(acc_train)\n",
    "\n",
    "            # Print the result\n",
    "            print(f'batch: {batch + 1:03d}, loss_train: {loss.item():.4f}, acc_train: {acc_train:.4f}, time: {end_time - start_time:.4f}s')\n",
    "\n",
    "            if batch >= 179:\n",
    "                break\n",
    "\n",
    "        # Perform validation after each epoch\n",
    "        print(f\"Validation after epoch {epoch}:\")\n",
    "        val_acc, val_loss, val_output = predict_(llm_graph_transformer, label, loss_fn, val, device, edge_feat)\n",
    "        print(f\"Validation set results: loss= {val_loss:.4f}, accuracy= {val_acc:.4f}, label acc= {f1_score(label_cpu[val], val_output, average=None)}\")\n",
    "        valscores.append(val_acc)\n",
    "\n",
    "    acc_test, loss_test, predict_output = predict_(llm_graph_transformer, label, loss_fn, test, device, edge_feat)\n",
    "    print(f\"Test set results: loss= {loss_test:.4f}, accuracy= {acc_test:.4f}, label acc= {f1_score(label_cpu[test], predict_output, average=None)}\")\n",
    "    save_model(llm_graph_transformer, optimizer, epoch)\n",
    "\n",
    "\n",
    "\n",
    "def save_model(model, optimizer, epoch, path=\"llm_w_edgefeat.pth\"):\n",
    "    # Get current time and format it\n",
    "    current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    \n",
    "    # Add the directory 'model/' and append the time to the path\n",
    "    path = f\"model/{current_time}_{path}\"\n",
    "    \n",
    "    # Create checkpoint to save model and optimizer state\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    \n",
    "    # Save the checkpoint\n",
    "    torch.save(checkpoint, path)\n",
    "    \n",
    "    # Print confirmation that the model has been saved\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def predict_(model, label, loss_fn, data_idx, device, edge_feat):\n",
    "    predict_output = []\n",
    "    loss = 0.0\n",
    "    num_batches = math.ceil(len(data_idx) / 10)\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "        batch_edges = data_idx[10 * batch:10 * (batch + 1)]\n",
    "        labels = ['Normal', 'Audio-Streaming', 'Browsing', 'Chat', 'File-Transfer',\n",
    "        'Email', 'P2P', 'Video-Streaming', 'VOIP']\n",
    "\n",
    "        # Generate text from batch_edges\n",
    "        batch_text = model.generate_text(batch_edges, labels, max_new_tokens=10)\n",
    "        edge_batch = edge_feat[batch_edges]\n",
    "        # Get logits from the model (floating point values representing class probabilities)\n",
    "        logits = model(batch_text, edge_batch).to(device)  # Use the model to get logits\n",
    "\n",
    "        # Target labels\n",
    "        batch_labels = label[batch_edges].to(device)  # Long type labels for cross_entropy\n",
    "\n",
    "        # Compute the loss using logits (input) and batch_labels (target)\n",
    "        batch_loss = loss_fn(logits, batch_labels)\n",
    "        loss += batch_loss.item()\n",
    "\n",
    "        # Calculate predictions based on logits\n",
    "        predicted_labels = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        predict_output.extend(predicted_labels)\n",
    "\n",
    "    # Normalize loss by the number of batches\n",
    "    loss /= num_batches\n",
    "\n",
    "    # Calculate accuracy using F1 score\n",
    "    acc = f1_score(label.cpu().numpy()[data_idx], predict_output, average=\"weighted\")\n",
    "    return acc, loss, predict_output\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seeds(42) \n",
    "    fit({\n",
    "        \"dataset\": \"Darknet\",\n",
    "        \"binary\": False,\n",
    "        \"residual\": True\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a40968",
   "metadata": {},
   "source": [
    "# Continuous Learning | Phase 2\n",
    "## Unused 120 Sampling + Pre-Fixed Seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac7669e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_986490/3021010499.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model/20241109-162008_llm_w_edgefeat.pth, epoch 9\n",
      "Unused Train Data: 864, Validation: 108, Test: 108\n",
      "Label distribution in Unused Train Set: {0: 96, 1: 96, 2: 96, 3: 96, 4: 96, 5: 96, 6: 96, 7: 96, 8: 96}\n",
      "Label distribution in Unused Validation Set: {0: 12, 1: 12, 2: 12, 3: 12, 4: 12, 5: 12, 6: 12, 7: 12, 8: 12}\n",
      "Label distribution in Unused Test Set: {0: 12, 1: 12, 2: 12, 3: 12, 4: 12, 5: 12, 6: 12, 7: 12, 8: 12}\n",
      "Epoch: 0\n",
      "[Train] batch: 001, loss_train: 0.0130, acc_train: 1.0000\n",
      "[Train] batch: 002, loss_train: 0.0067, acc_train: 1.0000\n",
      "[Train] batch: 003, loss_train: 0.0062, acc_train: 1.0000\n",
      "[Train] batch: 004, loss_train: 0.0760, acc_train: 1.0000\n",
      "[Train] batch: 005, loss_train: 0.0046, acc_train: 1.0000\n",
      "[Train] batch: 006, loss_train: 0.0042, acc_train: 1.0000\n",
      "[Train] batch: 007, loss_train: 0.0114, acc_train: 1.0000\n",
      "[Train] batch: 008, loss_train: 0.1003, acc_train: 1.0000\n",
      "[Train] batch: 009, loss_train: 0.0013, acc_train: 1.0000\n",
      "[Train] batch: 010, loss_train: 1.0652, acc_train: 0.9000\n",
      "[Train] batch: 011, loss_train: 0.2335, acc_train: 0.8933\n",
      "[Train] batch: 012, loss_train: 0.0062, acc_train: 1.0000\n",
      "[Train] batch: 013, loss_train: 0.1408, acc_train: 0.8667\n",
      "[Train] batch: 014, loss_train: 0.0039, acc_train: 1.0000\n",
      "[Train] batch: 015, loss_train: 0.0056, acc_train: 1.0000\n",
      "[Train] batch: 016, loss_train: 1.2160, acc_train: 0.9000\n",
      "[Train] batch: 017, loss_train: 1.8530, acc_train: 0.7600\n",
      "[Train] batch: 018, loss_train: 0.0105, acc_train: 1.0000\n",
      "[Train] batch: 019, loss_train: 0.0142, acc_train: 1.0000\n",
      "[Train] batch: 020, loss_train: 0.0191, acc_train: 1.0000\n",
      "[Train] batch: 021, loss_train: 0.0705, acc_train: 1.0000\n",
      "[Train] batch: 022, loss_train: 1.2190, acc_train: 0.7600\n",
      "[Train] batch: 023, loss_train: 0.3900, acc_train: 0.9000\n",
      "[Train] batch: 024, loss_train: 0.1691, acc_train: 0.9000\n",
      "[Train] batch: 025, loss_train: 0.4150, acc_train: 0.9067\n",
      "[Train] batch: 026, loss_train: 0.2527, acc_train: 0.9000\n",
      "[Train] batch: 027, loss_train: 0.3260, acc_train: 0.9000\n",
      "[Train] batch: 028, loss_train: 0.0668, acc_train: 1.0000\n",
      "[Train] batch: 029, loss_train: 0.1944, acc_train: 0.8667\n",
      "[Train] batch: 030, loss_train: 0.8138, acc_train: 0.9000\n",
      "[Train] batch: 031, loss_train: 0.1189, acc_train: 1.0000\n",
      "[Train] batch: 032, loss_train: 0.5555, acc_train: 0.9333\n",
      "[Train] batch: 033, loss_train: 0.1045, acc_train: 1.0000\n",
      "[Train] batch: 034, loss_train: 0.0564, acc_train: 1.0000\n",
      "[Train] batch: 035, loss_train: 0.0330, acc_train: 1.0000\n",
      "[Train] batch: 036, loss_train: 0.3939, acc_train: 0.9000\n",
      "[Train] batch: 037, loss_train: 0.0224, acc_train: 1.0000\n",
      "[Train] batch: 038, loss_train: 1.5164, acc_train: 0.7733\n",
      "[Train] batch: 039, loss_train: 0.0368, acc_train: 1.0000\n",
      "[Train] batch: 040, loss_train: 0.0289, acc_train: 1.0000\n",
      "[Train] batch: 041, loss_train: 0.5505, acc_train: 0.9000\n",
      "[Train] batch: 042, loss_train: 0.0975, acc_train: 0.9000\n",
      "[Train] batch: 043, loss_train: 0.2818, acc_train: 0.8333\n",
      "[Train] batch: 044, loss_train: 0.1878, acc_train: 1.0000\n",
      "[Train] batch: 045, loss_train: 1.0441, acc_train: 0.8400\n",
      "[Train] batch: 046, loss_train: 0.1449, acc_train: 1.0000\n",
      "[Train] batch: 047, loss_train: 0.4025, acc_train: 0.8000\n",
      "[Train] batch: 048, loss_train: 0.0488, acc_train: 1.0000\n",
      "[Train] batch: 049, loss_train: 0.0335, acc_train: 1.0000\n",
      "[Train] batch: 050, loss_train: 0.0168, acc_train: 1.0000\n",
      "[Train] batch: 051, loss_train: 0.9692, acc_train: 0.6600\n",
      "[Train] batch: 052, loss_train: 0.1057, acc_train: 1.0000\n",
      "[Train] batch: 053, loss_train: 0.5014, acc_train: 0.9000\n",
      "[Train] batch: 054, loss_train: 0.2056, acc_train: 0.8600\n",
      "[Train] batch: 055, loss_train: 0.5492, acc_train: 0.9000\n",
      "[Train] batch: 056, loss_train: 0.3432, acc_train: 0.9000\n",
      "[Train] batch: 057, loss_train: 0.0137, acc_train: 1.0000\n",
      "[Train] batch: 058, loss_train: 0.1097, acc_train: 0.8667\n",
      "[Train] batch: 059, loss_train: 0.0062, acc_train: 1.0000\n",
      "[Train] batch: 060, loss_train: 0.3296, acc_train: 0.7867\n",
      "[Train] batch: 061, loss_train: 0.3115, acc_train: 0.9333\n",
      "[Train] batch: 062, loss_train: 0.1563, acc_train: 0.8571\n",
      "[Train] batch: 063, loss_train: 0.1231, acc_train: 0.9429\n",
      "[Train] batch: 064, loss_train: 0.0831, acc_train: 0.9000\n",
      "[Train] batch: 065, loss_train: 0.6537, acc_train: 0.9400\n",
      "[Train] batch: 066, loss_train: 0.0619, acc_train: 1.0000\n",
      "[Train] batch: 067, loss_train: 0.1149, acc_train: 1.0000\n",
      "[Train] batch: 068, loss_train: 0.5094, acc_train: 0.7833\n",
      "[Train] batch: 069, loss_train: 0.4726, acc_train: 0.9111\n",
      "[Train] batch: 070, loss_train: 0.0056, acc_train: 1.0000\n",
      "[Train] batch: 071, loss_train: 0.1641, acc_train: 0.8933\n",
      "[Train] batch: 072, loss_train: 0.0143, acc_train: 1.0000\n",
      "[Train] batch: 073, loss_train: 0.0138, acc_train: 1.0000\n",
      "[Train] batch: 074, loss_train: 0.0016, acc_train: 1.0000\n",
      "[Train] batch: 075, loss_train: 0.0047, acc_train: 1.0000\n",
      "[Train] batch: 076, loss_train: 0.0263, acc_train: 1.0000\n",
      "[Train] batch: 077, loss_train: 0.0108, acc_train: 1.0000\n",
      "[Train] batch: 078, loss_train: 0.3131, acc_train: 0.9000\n",
      "[Train] batch: 079, loss_train: 0.7495, acc_train: 0.9429\n",
      "[Train] batch: 080, loss_train: 0.3195, acc_train: 0.8933\n",
      "[Train] batch: 081, loss_train: 0.0147, acc_train: 1.0000\n",
      "[Train] batch: 082, loss_train: 0.1375, acc_train: 0.8667\n",
      "[Train] batch: 083, loss_train: 0.0997, acc_train: 1.0000\n",
      "[Train] batch: 084, loss_train: 0.0937, acc_train: 1.0000\n",
      "[Train] batch: 085, loss_train: 0.5625, acc_train: 0.9000\n",
      "[Train] batch: 086, loss_train: 0.0215, acc_train: 1.0000\n",
      "[Val] loss= 0.4287, accuracy= 0.9438, label acc= [1.         0.81481481 1.         1.         0.8        0.92307692\n",
      " 0.95652174 1.         1.        ]\n",
      "Epoch: 1\n",
      "[Train] batch: 001, loss_train: 0.0306, acc_train: 1.0000\n",
      "[Train] batch: 002, loss_train: 0.3245, acc_train: 0.9000\n",
      "[Train] batch: 003, loss_train: 0.0398, acc_train: 1.0000\n",
      "[Train] batch: 004, loss_train: 0.9081, acc_train: 0.8600\n",
      "[Train] batch: 005, loss_train: 0.2245, acc_train: 0.9400\n",
      "[Train] batch: 006, loss_train: 0.0684, acc_train: 1.0000\n",
      "[Train] batch: 007, loss_train: 0.2758, acc_train: 0.8600\n",
      "[Train] batch: 008, loss_train: 0.1890, acc_train: 0.9067\n",
      "[Train] batch: 009, loss_train: 0.2625, acc_train: 0.8667\n",
      "[Train] batch: 010, loss_train: 0.3597, acc_train: 0.8600\n",
      "[Train] batch: 011, loss_train: 0.0134, acc_train: 1.0000\n",
      "[Train] batch: 012, loss_train: 0.2383, acc_train: 0.9000\n",
      "[Train] batch: 013, loss_train: 0.4228, acc_train: 0.7500\n",
      "[Train] batch: 014, loss_train: 0.5562, acc_train: 0.7000\n",
      "[Train] batch: 015, loss_train: 0.5292, acc_train: 0.9429\n",
      "[Train] batch: 016, loss_train: 0.0314, acc_train: 1.0000\n",
      "[Train] batch: 017, loss_train: 0.0055, acc_train: 1.0000\n",
      "[Train] batch: 018, loss_train: 0.0101, acc_train: 1.0000\n",
      "[Train] batch: 019, loss_train: 0.4232, acc_train: 0.9067\n",
      "[Train] batch: 020, loss_train: 0.0767, acc_train: 1.0000\n",
      "[Train] batch: 021, loss_train: 0.0425, acc_train: 1.0000\n",
      "[Train] batch: 022, loss_train: 0.5232, acc_train: 0.7500\n",
      "[Train] batch: 023, loss_train: 0.0179, acc_train: 1.0000\n",
      "[Train] batch: 024, loss_train: 0.0417, acc_train: 1.0000\n",
      "[Train] batch: 025, loss_train: 0.0341, acc_train: 1.0000\n",
      "[Train] batch: 026, loss_train: 0.0259, acc_train: 1.0000\n",
      "[Train] batch: 027, loss_train: 0.0417, acc_train: 1.0000\n",
      "[Train] batch: 028, loss_train: 0.0401, acc_train: 1.0000\n",
      "[Train] batch: 029, loss_train: 0.2196, acc_train: 0.8667\n",
      "[Train] batch: 030, loss_train: 0.1955, acc_train: 0.9000\n",
      "[Train] batch: 031, loss_train: 0.0158, acc_train: 1.0000\n",
      "[Train] batch: 032, loss_train: 0.5821, acc_train: 0.8067\n",
      "[Train] batch: 033, loss_train: 0.0104, acc_train: 1.0000\n",
      "[Train] batch: 034, loss_train: 0.0851, acc_train: 1.0000\n",
      "[Train] batch: 035, loss_train: 0.0920, acc_train: 1.0000\n",
      "[Train] batch: 036, loss_train: 0.0188, acc_train: 1.0000\n",
      "[Train] batch: 037, loss_train: 0.1338, acc_train: 0.8933\n",
      "[Train] batch: 038, loss_train: 0.0442, acc_train: 1.0000\n",
      "[Train] batch: 039, loss_train: 0.0354, acc_train: 1.0000\n",
      "[Train] batch: 040, loss_train: 0.0609, acc_train: 1.0000\n",
      "[Train] batch: 041, loss_train: 0.5233, acc_train: 0.9000\n",
      "[Train] batch: 042, loss_train: 0.3124, acc_train: 0.8571\n",
      "[Train] batch: 043, loss_train: 0.0334, acc_train: 1.0000\n",
      "[Train] batch: 044, loss_train: 0.0180, acc_train: 1.0000\n",
      "[Train] batch: 045, loss_train: 0.7185, acc_train: 0.7556\n",
      "[Train] batch: 046, loss_train: 0.0265, acc_train: 1.0000\n",
      "[Train] batch: 047, loss_train: 0.5830, acc_train: 0.7400\n",
      "[Train] batch: 048, loss_train: 0.1889, acc_train: 0.9000\n",
      "[Train] batch: 049, loss_train: 0.0275, acc_train: 1.0000\n",
      "[Train] batch: 050, loss_train: 0.0952, acc_train: 1.0000\n",
      "[Train] batch: 051, loss_train: 0.0975, acc_train: 1.0000\n",
      "[Train] batch: 052, loss_train: 0.0002, acc_train: 1.0000\n",
      "[Train] batch: 053, loss_train: 0.0129, acc_train: 1.0000\n",
      "[Train] batch: 054, loss_train: 0.0250, acc_train: 1.0000\n",
      "[Train] batch: 055, loss_train: 0.0401, acc_train: 1.0000\n",
      "[Train] batch: 056, loss_train: 0.0081, acc_train: 1.0000\n",
      "[Train] batch: 057, loss_train: 0.0013, acc_train: 1.0000\n",
      "[Train] batch: 058, loss_train: 0.0440, acc_train: 1.0000\n",
      "[Train] batch: 059, loss_train: 0.0020, acc_train: 1.0000\n",
      "[Train] batch: 060, loss_train: 0.4652, acc_train: 0.8667\n",
      "[Train] batch: 061, loss_train: 0.6625, acc_train: 0.8600\n",
      "[Train] batch: 062, loss_train: 0.0094, acc_train: 1.0000\n",
      "[Train] batch: 063, loss_train: 0.0037, acc_train: 1.0000\n",
      "[Train] batch: 064, loss_train: 0.0412, acc_train: 1.0000\n",
      "[Train] batch: 065, loss_train: 0.1120, acc_train: 0.9095\n",
      "[Train] batch: 066, loss_train: 0.0527, acc_train: 1.0000\n",
      "[Train] batch: 067, loss_train: 1.1031, acc_train: 0.8429\n",
      "[Train] batch: 068, loss_train: 0.0074, acc_train: 1.0000\n",
      "[Train] batch: 069, loss_train: 0.0376, acc_train: 1.0000\n",
      "[Train] batch: 070, loss_train: 0.2681, acc_train: 0.9000\n",
      "[Train] batch: 071, loss_train: 0.0614, acc_train: 1.0000\n",
      "[Train] batch: 072, loss_train: 0.0342, acc_train: 1.0000\n",
      "[Train] batch: 073, loss_train: 0.2089, acc_train: 0.9000\n",
      "[Train] batch: 074, loss_train: 0.0121, acc_train: 1.0000\n",
      "[Train] batch: 075, loss_train: 0.0491, acc_train: 1.0000\n",
      "[Train] batch: 076, loss_train: 0.4153, acc_train: 0.9000\n",
      "[Train] batch: 077, loss_train: 0.0914, acc_train: 1.0000\n",
      "[Train] batch: 078, loss_train: 0.0503, acc_train: 1.0000\n",
      "[Train] batch: 079, loss_train: 0.0430, acc_train: 1.0000\n",
      "[Train] batch: 080, loss_train: 0.0061, acc_train: 1.0000\n",
      "[Train] batch: 081, loss_train: 0.0952, acc_train: 0.9333\n",
      "[Train] batch: 082, loss_train: 0.0087, acc_train: 1.0000\n",
      "[Train] batch: 083, loss_train: 0.2112, acc_train: 0.8667\n",
      "[Train] batch: 084, loss_train: 0.3024, acc_train: 0.9000\n",
      "[Train] batch: 085, loss_train: 0.0030, acc_train: 1.0000\n",
      "[Train] batch: 086, loss_train: 0.0738, acc_train: 1.0000\n",
      "[Val] loss= 0.2838, accuracy= 0.9460, label acc= [0.95652174 0.95652174 1.         1.         0.82758621 0.96\n",
      " 0.95652174 0.85714286 1.        ]\n",
      "Epoch: 2\n",
      "[Train] batch: 001, loss_train: 0.0962, acc_train: 1.0000\n",
      "[Train] batch: 002, loss_train: 0.0010, acc_train: 1.0000\n",
      "[Train] batch: 003, loss_train: 0.0072, acc_train: 1.0000\n",
      "[Train] batch: 004, loss_train: 0.1216, acc_train: 0.9333\n",
      "[Train] batch: 005, loss_train: 0.0041, acc_train: 1.0000\n",
      "[Train] batch: 006, loss_train: 0.3513, acc_train: 0.9000\n",
      "[Train] batch: 007, loss_train: 0.0172, acc_train: 1.0000\n",
      "[Train] batch: 008, loss_train: 0.7137, acc_train: 0.8000\n",
      "[Train] batch: 009, loss_train: 0.0073, acc_train: 1.0000\n",
      "[Train] batch: 010, loss_train: 0.0144, acc_train: 1.0000\n",
      "[Train] batch: 011, loss_train: 0.0064, acc_train: 1.0000\n",
      "[Train] batch: 012, loss_train: 0.0069, acc_train: 1.0000\n",
      "[Train] batch: 013, loss_train: 0.1810, acc_train: 0.9333\n",
      "[Train] batch: 014, loss_train: 0.0024, acc_train: 1.0000\n",
      "[Train] batch: 015, loss_train: 0.0271, acc_train: 1.0000\n",
      "[Train] batch: 016, loss_train: 0.0158, acc_train: 1.0000\n",
      "[Train] batch: 017, loss_train: 0.0396, acc_train: 1.0000\n",
      "[Train] batch: 018, loss_train: 0.7884, acc_train: 0.8600\n",
      "[Train] batch: 019, loss_train: 0.0077, acc_train: 1.0000\n",
      "[Train] batch: 020, loss_train: 0.0784, acc_train: 1.0000\n",
      "[Train] batch: 021, loss_train: 0.0471, acc_train: 1.0000\n",
      "[Train] batch: 022, loss_train: 0.0297, acc_train: 1.0000\n",
      "[Train] batch: 023, loss_train: 0.3344, acc_train: 0.9333\n",
      "[Train] batch: 024, loss_train: 0.0088, acc_train: 1.0000\n",
      "[Train] batch: 025, loss_train: 0.0104, acc_train: 1.0000\n",
      "[Train] batch: 026, loss_train: 0.0152, acc_train: 1.0000\n",
      "[Train] batch: 027, loss_train: 0.0019, acc_train: 1.0000\n",
      "[Train] batch: 028, loss_train: 0.0028, acc_train: 1.0000\n",
      "[Train] batch: 029, loss_train: 0.0043, acc_train: 1.0000\n",
      "[Train] batch: 030, loss_train: 0.0052, acc_train: 1.0000\n",
      "[Train] batch: 031, loss_train: 0.3391, acc_train: 0.8933\n",
      "[Train] batch: 032, loss_train: 0.0021, acc_train: 1.0000\n",
      "[Train] batch: 033, loss_train: 0.6164, acc_train: 0.9000\n",
      "[Train] batch: 034, loss_train: 0.0063, acc_train: 1.0000\n",
      "[Train] batch: 035, loss_train: 0.0060, acc_train: 1.0000\n",
      "[Train] batch: 036, loss_train: 0.0056, acc_train: 1.0000\n",
      "[Train] batch: 037, loss_train: 0.0088, acc_train: 1.0000\n",
      "[Train] batch: 038, loss_train: 0.0110, acc_train: 1.0000\n",
      "[Train] batch: 039, loss_train: 0.0025, acc_train: 1.0000\n",
      "[Train] batch: 040, loss_train: 0.0455, acc_train: 1.0000\n",
      "[Train] batch: 041, loss_train: 0.1043, acc_train: 0.9029\n",
      "[Train] batch: 042, loss_train: 0.0014, acc_train: 1.0000\n",
      "[Train] batch: 043, loss_train: 0.0454, acc_train: 1.0000\n",
      "[Train] batch: 044, loss_train: 0.1814, acc_train: 0.8667\n",
      "[Train] batch: 045, loss_train: 0.0849, acc_train: 1.0000\n",
      "[Train] batch: 046, loss_train: 0.0501, acc_train: 1.0000\n",
      "[Train] batch: 047, loss_train: 0.0040, acc_train: 1.0000\n",
      "[Train] batch: 048, loss_train: 0.1635, acc_train: 0.8905\n",
      "[Train] batch: 049, loss_train: 0.1499, acc_train: 0.8667\n",
      "[Train] batch: 050, loss_train: 0.0108, acc_train: 1.0000\n",
      "[Train] batch: 051, loss_train: 0.2917, acc_train: 0.8000\n",
      "[Train] batch: 052, loss_train: 0.0032, acc_train: 1.0000\n",
      "[Train] batch: 053, loss_train: 0.4689, acc_train: 0.7267\n",
      "[Train] batch: 054, loss_train: 0.0277, acc_train: 1.0000\n",
      "[Train] batch: 055, loss_train: 0.0194, acc_train: 1.0000\n",
      "[Train] batch: 056, loss_train: 0.0071, acc_train: 1.0000\n",
      "[Train] batch: 057, loss_train: 0.0033, acc_train: 1.0000\n",
      "[Train] batch: 058, loss_train: 0.0048, acc_train: 1.0000\n",
      "[Train] batch: 059, loss_train: 0.0181, acc_train: 1.0000\n",
      "[Train] batch: 060, loss_train: 0.0004, acc_train: 1.0000\n",
      "[Train] batch: 061, loss_train: 0.0788, acc_train: 1.0000\n",
      "[Train] batch: 062, loss_train: 0.0048, acc_train: 1.0000\n",
      "[Train] batch: 063, loss_train: 0.0928, acc_train: 0.9000\n",
      "[Train] batch: 064, loss_train: 0.0005, acc_train: 1.0000\n",
      "[Train] batch: 065, loss_train: 0.0184, acc_train: 1.0000\n",
      "[Train] batch: 066, loss_train: 0.0050, acc_train: 1.0000\n",
      "[Train] batch: 067, loss_train: 0.0038, acc_train: 1.0000\n",
      "[Train] batch: 068, loss_train: 0.0293, acc_train: 1.0000\n",
      "[Train] batch: 069, loss_train: 0.0229, acc_train: 1.0000\n",
      "[Train] batch: 070, loss_train: 0.0018, acc_train: 1.0000\n",
      "[Train] batch: 071, loss_train: 0.0002, acc_train: 1.0000\n",
      "[Train] batch: 072, loss_train: 0.5174, acc_train: 0.9029\n",
      "[Train] batch: 073, loss_train: 0.0057, acc_train: 1.0000\n",
      "[Train] batch: 074, loss_train: 0.0253, acc_train: 1.0000\n",
      "[Train] batch: 075, loss_train: 0.0017, acc_train: 1.0000\n",
      "[Train] batch: 076, loss_train: 0.0060, acc_train: 1.0000\n",
      "[Train] batch: 077, loss_train: 0.0114, acc_train: 1.0000\n",
      "[Train] batch: 078, loss_train: 0.3494, acc_train: 0.9400\n",
      "[Train] batch: 079, loss_train: 0.0210, acc_train: 1.0000\n",
      "[Train] batch: 080, loss_train: 0.0035, acc_train: 1.0000\n",
      "[Train] batch: 081, loss_train: 0.5355, acc_train: 0.9333\n",
      "[Train] batch: 082, loss_train: 0.0023, acc_train: 1.0000\n",
      "[Train] batch: 083, loss_train: 0.0405, acc_train: 1.0000\n",
      "[Train] batch: 084, loss_train: 0.0736, acc_train: 1.0000\n",
      "[Train] batch: 085, loss_train: 0.2533, acc_train: 0.8667\n",
      "[Train] batch: 086, loss_train: 0.9149, acc_train: 0.8333\n",
      "[Val] loss= 0.3616, accuracy= 0.9339, label acc= [0.95652174 0.8        1.         0.88888889 0.8        0.96\n",
      " 1.         1.         1.        ]\n",
      "Epoch: 3\n",
      "[Train] batch: 001, loss_train: 0.0027, acc_train: 1.0000\n",
      "[Train] batch: 002, loss_train: 0.3738, acc_train: 0.8971\n",
      "[Train] batch: 003, loss_train: 0.0038, acc_train: 1.0000\n",
      "[Train] batch: 004, loss_train: 0.3022, acc_train: 0.9095\n",
      "[Train] batch: 005, loss_train: 0.0950, acc_train: 0.8933\n",
      "[Train] batch: 006, loss_train: 0.0630, acc_train: 1.0000\n",
      "[Train] batch: 007, loss_train: 0.0015, acc_train: 1.0000\n",
      "[Train] batch: 008, loss_train: 0.0110, acc_train: 1.0000\n",
      "[Train] batch: 009, loss_train: 0.0346, acc_train: 1.0000\n",
      "[Train] batch: 010, loss_train: 0.0217, acc_train: 1.0000\n",
      "[Train] batch: 011, loss_train: 0.0009, acc_train: 1.0000\n",
      "[Train] batch: 012, loss_train: 0.0440, acc_train: 1.0000\n",
      "[Train] batch: 013, loss_train: 0.0056, acc_train: 1.0000\n",
      "[Train] batch: 014, loss_train: 0.0160, acc_train: 1.0000\n",
      "[Train] batch: 015, loss_train: 0.0519, acc_train: 1.0000\n",
      "[Train] batch: 016, loss_train: 0.0323, acc_train: 1.0000\n",
      "[Train] batch: 017, loss_train: 0.0008, acc_train: 1.0000\n",
      "[Train] batch: 018, loss_train: 0.0055, acc_train: 1.0000\n",
      "[Train] batch: 019, loss_train: 0.0136, acc_train: 1.0000\n",
      "[Train] batch: 020, loss_train: 0.0067, acc_train: 1.0000\n",
      "[Train] batch: 021, loss_train: 0.0013, acc_train: 1.0000\n",
      "[Train] batch: 022, loss_train: 0.0184, acc_train: 1.0000\n",
      "[Train] batch: 023, loss_train: 0.0225, acc_train: 1.0000\n",
      "[Train] batch: 024, loss_train: 0.0118, acc_train: 1.0000\n",
      "[Train] batch: 025, loss_train: 0.0087, acc_train: 1.0000\n",
      "[Train] batch: 026, loss_train: 0.0568, acc_train: 1.0000\n",
      "[Train] batch: 027, loss_train: 0.0026, acc_train: 1.0000\n",
      "[Train] batch: 028, loss_train: 0.0175, acc_train: 1.0000\n",
      "[Train] batch: 029, loss_train: 0.0111, acc_train: 1.0000\n",
      "[Train] batch: 030, loss_train: 0.0111, acc_train: 1.0000\n",
      "[Train] batch: 031, loss_train: 0.1098, acc_train: 1.0000\n",
      "[Train] batch: 032, loss_train: 0.1302, acc_train: 0.8933\n",
      "[Train] batch: 033, loss_train: 0.1550, acc_train: 0.8600\n",
      "[Train] batch: 034, loss_train: 0.0025, acc_train: 1.0000\n",
      "[Train] batch: 035, loss_train: 0.0010, acc_train: 1.0000\n",
      "[Train] batch: 036, loss_train: 0.0070, acc_train: 1.0000\n",
      "[Train] batch: 037, loss_train: 0.0022, acc_train: 1.0000\n",
      "[Train] batch: 038, loss_train: 0.0009, acc_train: 1.0000\n",
      "[Train] batch: 039, loss_train: 0.0120, acc_train: 1.0000\n",
      "[Train] batch: 040, loss_train: 0.0020, acc_train: 1.0000\n",
      "[Train] batch: 041, loss_train: 0.0101, acc_train: 1.0000\n",
      "[Train] batch: 042, loss_train: 0.0046, acc_train: 1.0000\n",
      "[Train] batch: 043, loss_train: 0.0020, acc_train: 1.0000\n",
      "[Train] batch: 044, loss_train: 0.0007, acc_train: 1.0000\n",
      "[Train] batch: 045, loss_train: 0.0083, acc_train: 1.0000\n",
      "[Train] batch: 046, loss_train: 0.0084, acc_train: 1.0000\n",
      "[Train] batch: 047, loss_train: 0.0028, acc_train: 1.0000\n",
      "[Train] batch: 048, loss_train: 0.0011, acc_train: 1.0000\n",
      "[Train] batch: 049, loss_train: 0.9641, acc_train: 0.7833\n",
      "[Train] batch: 050, loss_train: 0.2455, acc_train: 0.8667\n",
      "[Train] batch: 051, loss_train: 0.0063, acc_train: 1.0000\n",
      "[Train] batch: 052, loss_train: 0.0073, acc_train: 1.0000\n",
      "[Train] batch: 053, loss_train: 0.0015, acc_train: 1.0000\n",
      "[Train] batch: 054, loss_train: 0.0014, acc_train: 1.0000\n",
      "[Train] batch: 055, loss_train: 0.0132, acc_train: 1.0000\n",
      "[Train] batch: 056, loss_train: 0.0013, acc_train: 1.0000\n",
      "[Train] batch: 057, loss_train: 0.0018, acc_train: 1.0000\n",
      "[Train] batch: 058, loss_train: 0.0054, acc_train: 1.0000\n",
      "[Train] batch: 059, loss_train: 0.0663, acc_train: 1.0000\n",
      "[Train] batch: 060, loss_train: 0.0012, acc_train: 1.0000\n",
      "[Train] batch: 061, loss_train: 0.0068, acc_train: 1.0000\n",
      "[Train] batch: 062, loss_train: 0.0001, acc_train: 1.0000\n",
      "[Train] batch: 063, loss_train: 0.0009, acc_train: 1.0000\n",
      "[Train] batch: 064, loss_train: 0.0072, acc_train: 1.0000\n",
      "[Train] batch: 065, loss_train: 0.0208, acc_train: 1.0000\n",
      "[Train] batch: 066, loss_train: 0.0035, acc_train: 1.0000\n",
      "[Train] batch: 067, loss_train: 0.0036, acc_train: 1.0000\n",
      "[Train] batch: 068, loss_train: 0.0144, acc_train: 1.0000\n",
      "[Train] batch: 069, loss_train: 0.0684, acc_train: 1.0000\n",
      "[Train] batch: 070, loss_train: 0.0057, acc_train: 1.0000\n",
      "[Train] batch: 071, loss_train: 0.0081, acc_train: 1.0000\n",
      "[Train] batch: 072, loss_train: 0.0015, acc_train: 1.0000\n",
      "[Train] batch: 073, loss_train: 0.0480, acc_train: 1.0000\n",
      "[Train] batch: 074, loss_train: 0.0005, acc_train: 1.0000\n",
      "[Train] batch: 075, loss_train: 0.0052, acc_train: 1.0000\n",
      "[Train] batch: 076, loss_train: 0.0451, acc_train: 1.0000\n",
      "[Train] batch: 077, loss_train: 0.0052, acc_train: 1.0000\n",
      "[Train] batch: 078, loss_train: 0.3444, acc_train: 0.8600\n",
      "[Train] batch: 079, loss_train: 0.0010, acc_train: 1.0000\n",
      "[Train] batch: 080, loss_train: 0.0022, acc_train: 1.0000\n",
      "[Train] batch: 081, loss_train: 0.0009, acc_train: 1.0000\n",
      "[Train] batch: 082, loss_train: 0.0009, acc_train: 1.0000\n",
      "[Train] batch: 083, loss_train: 0.0027, acc_train: 1.0000\n",
      "[Train] batch: 084, loss_train: 0.0002, acc_train: 1.0000\n",
      "[Train] batch: 085, loss_train: 0.0038, acc_train: 1.0000\n",
      "[Train] batch: 086, loss_train: 0.0251, acc_train: 1.0000\n",
      "[Val] loss= 0.2280, accuracy= 0.9721, label acc= [1.         0.95652174 1.         1.         0.92307692 0.96\n",
      " 1.         0.90909091 1.        ]\n",
      "Epoch: 4\n",
      "[Train] batch: 001, loss_train: 0.1549, acc_train: 0.9400\n",
      "[Train] batch: 002, loss_train: 0.0009, acc_train: 1.0000\n",
      "[Train] batch: 003, loss_train: 0.0599, acc_train: 1.0000\n",
      "[Train] batch: 004, loss_train: 0.0224, acc_train: 1.0000\n",
      "[Train] batch: 005, loss_train: 0.0026, acc_train: 1.0000\n",
      "[Train] batch: 006, loss_train: 0.0121, acc_train: 1.0000\n",
      "[Train] batch: 007, loss_train: 0.0003, acc_train: 1.0000\n",
      "[Train] batch: 008, loss_train: 0.0084, acc_train: 1.0000\n",
      "[Train] batch: 009, loss_train: 0.8025, acc_train: 0.8667\n",
      "[Train] batch: 010, loss_train: 0.0009, acc_train: 1.0000\n",
      "[Train] batch: 011, loss_train: 0.0016, acc_train: 1.0000\n",
      "[Train] batch: 012, loss_train: 0.0038, acc_train: 1.0000\n",
      "[Train] batch: 013, loss_train: 0.0199, acc_train: 1.0000\n",
      "[Train] batch: 014, loss_train: 0.0385, acc_train: 1.0000\n",
      "[Train] batch: 015, loss_train: 0.0052, acc_train: 1.0000\n",
      "[Train] batch: 016, loss_train: 0.0044, acc_train: 1.0000\n",
      "[Train] batch: 017, loss_train: 0.0105, acc_train: 1.0000\n",
      "[Train] batch: 018, loss_train: 0.0173, acc_train: 1.0000\n",
      "[Train] batch: 019, loss_train: 0.2048, acc_train: 0.8600\n",
      "[Train] batch: 020, loss_train: 0.0516, acc_train: 1.0000\n",
      "[Train] batch: 021, loss_train: 0.0069, acc_train: 1.0000\n",
      "[Train] batch: 022, loss_train: 0.0039, acc_train: 1.0000\n",
      "[Train] batch: 023, loss_train: 0.0015, acc_train: 1.0000\n",
      "[Train] batch: 024, loss_train: 0.0064, acc_train: 1.0000\n",
      "[Train] batch: 025, loss_train: 0.0059, acc_train: 1.0000\n",
      "[Train] batch: 026, loss_train: 0.0014, acc_train: 1.0000\n",
      "[Train] batch: 027, loss_train: 0.0034, acc_train: 1.0000\n",
      "[Train] batch: 028, loss_train: 0.0066, acc_train: 1.0000\n",
      "[Train] batch: 029, loss_train: 0.0023, acc_train: 1.0000\n",
      "[Train] batch: 030, loss_train: 0.0012, acc_train: 1.0000\n",
      "[Train] batch: 031, loss_train: 0.0048, acc_train: 1.0000\n",
      "[Train] batch: 032, loss_train: 0.0011, acc_train: 1.0000\n",
      "[Train] batch: 033, loss_train: 0.0547, acc_train: 1.0000\n",
      "[Train] batch: 034, loss_train: 0.0001, acc_train: 1.0000\n",
      "[Train] batch: 035, loss_train: 0.3091, acc_train: 0.8556\n",
      "[Train] batch: 036, loss_train: 0.0050, acc_train: 1.0000\n",
      "[Train] batch: 037, loss_train: 0.0056, acc_train: 1.0000\n",
      "[Train] batch: 038, loss_train: 0.0001, acc_train: 1.0000\n",
      "[Train] batch: 039, loss_train: 0.0024, acc_train: 1.0000\n",
      "[Train] batch: 040, loss_train: 0.0014, acc_train: 1.0000\n",
      "[Train] batch: 041, loss_train: 0.0218, acc_train: 1.0000\n",
      "[Train] batch: 042, loss_train: 0.0014, acc_train: 1.0000\n",
      "[Train] batch: 043, loss_train: 0.0112, acc_train: 1.0000\n",
      "[Train] batch: 044, loss_train: 0.0038, acc_train: 1.0000\n",
      "[Train] batch: 045, loss_train: 0.0020, acc_train: 1.0000\n",
      "[Train] batch: 046, loss_train: 0.0009, acc_train: 1.0000\n",
      "[Train] batch: 047, loss_train: 0.0011, acc_train: 1.0000\n",
      "[Train] batch: 048, loss_train: 0.0012, acc_train: 1.0000\n",
      "[Train] batch: 049, loss_train: 0.0039, acc_train: 1.0000\n",
      "[Train] batch: 050, loss_train: 0.0042, acc_train: 1.0000\n",
      "[Train] batch: 051, loss_train: 0.0033, acc_train: 1.0000\n",
      "[Train] batch: 052, loss_train: 0.0050, acc_train: 1.0000\n",
      "[Train] batch: 053, loss_train: 0.0024, acc_train: 1.0000\n",
      "[Train] batch: 054, loss_train: 0.0022, acc_train: 1.0000\n",
      "[Train] batch: 055, loss_train: 0.0058, acc_train: 1.0000\n",
      "[Train] batch: 056, loss_train: 0.0464, acc_train: 1.0000\n",
      "[Train] batch: 057, loss_train: 0.0022, acc_train: 1.0000\n",
      "[Train] batch: 058, loss_train: 0.0028, acc_train: 1.0000\n",
      "[Train] batch: 059, loss_train: 0.3638, acc_train: 0.9400\n",
      "[Train] batch: 060, loss_train: 0.0319, acc_train: 1.0000\n",
      "[Train] batch: 061, loss_train: 0.2394, acc_train: 0.8905\n",
      "[Train] batch: 062, loss_train: 0.0002, acc_train: 1.0000\n",
      "[Train] batch: 063, loss_train: 0.0013, acc_train: 1.0000\n",
      "[Train] batch: 064, loss_train: 0.0109, acc_train: 1.0000\n",
      "[Train] batch: 065, loss_train: 0.0036, acc_train: 1.0000\n",
      "[Train] batch: 066, loss_train: 0.0020, acc_train: 1.0000\n",
      "[Train] batch: 067, loss_train: 0.0041, acc_train: 1.0000\n",
      "[Train] batch: 068, loss_train: 0.0393, acc_train: 1.0000\n",
      "[Train] batch: 069, loss_train: 0.0019, acc_train: 1.0000\n",
      "[Train] batch: 070, loss_train: 0.0048, acc_train: 1.0000\n",
      "[Train] batch: 071, loss_train: 0.0677, acc_train: 1.0000\n",
      "[Train] batch: 072, loss_train: 0.0111, acc_train: 1.0000\n",
      "[Train] batch: 073, loss_train: 0.0020, acc_train: 1.0000\n",
      "[Train] batch: 074, loss_train: 0.0039, acc_train: 1.0000\n",
      "[Train] batch: 075, loss_train: 0.0099, acc_train: 1.0000\n",
      "[Train] batch: 076, loss_train: 0.4142, acc_train: 0.8000\n",
      "[Train] batch: 077, loss_train: 0.0520, acc_train: 1.0000\n",
      "[Train] batch: 078, loss_train: 0.0013, acc_train: 1.0000\n",
      "[Train] batch: 079, loss_train: 0.0602, acc_train: 1.0000\n",
      "[Train] batch: 080, loss_train: 0.0045, acc_train: 1.0000\n",
      "[Train] batch: 081, loss_train: 0.0036, acc_train: 1.0000\n",
      "[Train] batch: 082, loss_train: 0.0001, acc_train: 1.0000\n",
      "[Train] batch: 083, loss_train: 0.0013, acc_train: 1.0000\n",
      "[Train] batch: 084, loss_train: 0.0068, acc_train: 1.0000\n",
      "[Train] batch: 085, loss_train: 0.0013, acc_train: 1.0000\n",
      "[Train] batch: 086, loss_train: 0.0412, acc_train: 1.0000\n",
      "[Val] loss= 0.2641, accuracy= 0.9721, label acc= [1.         0.95652174 1.         1.         0.92307692 0.96\n",
      " 1.         0.90909091 1.        ]\n",
      "Epoch: 5\n",
      "[Train] batch: 001, loss_train: 0.0060, acc_train: 1.0000\n",
      "[Train] batch: 002, loss_train: 0.0017, acc_train: 1.0000\n",
      "[Train] batch: 003, loss_train: 0.0021, acc_train: 1.0000\n",
      "[Train] batch: 004, loss_train: 0.0449, acc_train: 1.0000\n",
      "[Train] batch: 005, loss_train: 0.0056, acc_train: 1.0000\n",
      "[Train] batch: 006, loss_train: 0.0085, acc_train: 1.0000\n",
      "[Train] batch: 007, loss_train: 0.0026, acc_train: 1.0000\n",
      "[Train] batch: 008, loss_train: 0.0010, acc_train: 1.0000\n",
      "[Train] batch: 009, loss_train: 0.0025, acc_train: 1.0000\n",
      "[Train] batch: 010, loss_train: 0.0012, acc_train: 1.0000\n",
      "[Train] batch: 011, loss_train: 0.0669, acc_train: 1.0000\n",
      "[Train] batch: 012, loss_train: 0.0024, acc_train: 1.0000\n",
      "[Train] batch: 013, loss_train: 0.0026, acc_train: 1.0000\n",
      "[Train] batch: 014, loss_train: 0.0061, acc_train: 1.0000\n",
      "[Train] batch: 015, loss_train: 0.0008, acc_train: 1.0000\n",
      "[Train] batch: 016, loss_train: 0.0012, acc_train: 1.0000\n",
      "[Train] batch: 017, loss_train: 0.0001, acc_train: 1.0000\n",
      "[Train] batch: 018, loss_train: 0.0042, acc_train: 1.0000\n",
      "[Train] batch: 019, loss_train: 0.0013, acc_train: 1.0000\n",
      "[Train] batch: 020, loss_train: 0.0033, acc_train: 1.0000\n",
      "[Train] batch: 021, loss_train: 0.0020, acc_train: 1.0000\n",
      "[Train] batch: 022, loss_train: 0.0002, acc_train: 1.0000\n",
      "[Train] batch: 023, loss_train: 0.0017, acc_train: 1.0000\n",
      "[Train] batch: 024, loss_train: 0.0008, acc_train: 1.0000\n",
      "[Train] batch: 025, loss_train: 0.0029, acc_train: 1.0000\n",
      "[Train] batch: 026, loss_train: 0.0023, acc_train: 1.0000\n",
      "[Train] batch: 027, loss_train: 0.0011, acc_train: 1.0000\n",
      "[Train] batch: 028, loss_train: 0.3854, acc_train: 0.8667\n",
      "[Train] batch: 029, loss_train: 0.0033, acc_train: 1.0000\n",
      "[Train] batch: 030, loss_train: 0.1481, acc_train: 0.8667\n",
      "[Train] batch: 031, loss_train: 0.0019, acc_train: 1.0000\n",
      "[Train] batch: 032, loss_train: 0.0027, acc_train: 1.0000\n",
      "[Train] batch: 033, loss_train: 0.1275, acc_train: 0.9095\n",
      "[Train] batch: 034, loss_train: 0.0027, acc_train: 1.0000\n",
      "[Train] batch: 035, loss_train: 0.0059, acc_train: 1.0000\n",
      "[Train] batch: 036, loss_train: 0.0022, acc_train: 1.0000\n",
      "[Train] batch: 037, loss_train: 0.0016, acc_train: 1.0000\n",
      "[Train] batch: 038, loss_train: 0.0001, acc_train: 1.0000\n",
      "[Train] batch: 039, loss_train: 0.0041, acc_train: 1.0000\n",
      "[Train] batch: 040, loss_train: 0.0010, acc_train: 1.0000\n",
      "[Train] batch: 041, loss_train: 0.0043, acc_train: 1.0000\n",
      "[Train] batch: 042, loss_train: 0.0221, acc_train: 1.0000\n",
      "[Train] batch: 043, loss_train: 0.0074, acc_train: 1.0000\n",
      "[Train] batch: 044, loss_train: 0.0056, acc_train: 1.0000\n",
      "[Train] batch: 045, loss_train: 0.0067, acc_train: 1.0000\n",
      "[Train] batch: 046, loss_train: 0.0056, acc_train: 1.0000\n",
      "[Train] batch: 047, loss_train: 0.0097, acc_train: 1.0000\n",
      "[Train] batch: 048, loss_train: 0.0076, acc_train: 1.0000\n",
      "[Train] batch: 049, loss_train: 0.0007, acc_train: 1.0000\n",
      "[Train] batch: 050, loss_train: 0.0042, acc_train: 1.0000\n",
      "[Train] batch: 051, loss_train: 0.0144, acc_train: 1.0000\n",
      "[Train] batch: 052, loss_train: 0.0056, acc_train: 1.0000\n",
      "[Train] batch: 053, loss_train: 0.0030, acc_train: 1.0000\n",
      "[Train] batch: 054, loss_train: 0.0927, acc_train: 1.0000\n",
      "[Train] batch: 055, loss_train: 0.0076, acc_train: 1.0000\n",
      "[Train] batch: 056, loss_train: 0.0286, acc_train: 1.0000\n",
      "[Train] batch: 057, loss_train: 0.0154, acc_train: 1.0000\n",
      "[Train] batch: 058, loss_train: 0.0002, acc_train: 1.0000\n",
      "[Train] batch: 059, loss_train: 0.0018, acc_train: 1.0000\n",
      "[Train] batch: 060, loss_train: 0.0840, acc_train: 0.8600\n",
      "[Train] batch: 061, loss_train: 0.0015, acc_train: 1.0000\n",
      "[Train] batch: 062, loss_train: 0.0030, acc_train: 1.0000\n",
      "[Train] batch: 063, loss_train: 0.0026, acc_train: 1.0000\n",
      "[Train] batch: 064, loss_train: 0.0034, acc_train: 1.0000\n",
      "[Train] batch: 065, loss_train: 0.0012, acc_train: 1.0000\n",
      "[Train] batch: 066, loss_train: 0.0019, acc_train: 1.0000\n",
      "[Train] batch: 067, loss_train: 0.0052, acc_train: 1.0000\n",
      "[Train] batch: 068, loss_train: 0.0009, acc_train: 1.0000\n",
      "[Train] batch: 069, loss_train: 0.0018, acc_train: 1.0000\n",
      "[Train] batch: 070, loss_train: 0.0025, acc_train: 1.0000\n",
      "[Train] batch: 071, loss_train: 0.0021, acc_train: 1.0000\n",
      "[Train] batch: 072, loss_train: 0.0018, acc_train: 1.0000\n",
      "[Train] batch: 073, loss_train: 0.0044, acc_train: 1.0000\n",
      "[Train] batch: 074, loss_train: 0.0025, acc_train: 1.0000\n",
      "[Train] batch: 075, loss_train: 0.0023, acc_train: 1.0000\n",
      "[Train] batch: 076, loss_train: 0.0002, acc_train: 1.0000\n",
      "[Train] batch: 077, loss_train: 0.0007, acc_train: 1.0000\n",
      "[Train] batch: 078, loss_train: 0.0086, acc_train: 1.0000\n",
      "[Train] batch: 079, loss_train: 0.0011, acc_train: 1.0000\n",
      "[Train] batch: 080, loss_train: 0.0137, acc_train: 1.0000\n",
      "[Train] batch: 081, loss_train: 0.1231, acc_train: 0.9067\n",
      "[Train] batch: 082, loss_train: 0.0016, acc_train: 1.0000\n",
      "[Train] batch: 083, loss_train: 0.0162, acc_train: 1.0000\n",
      "[Train] batch: 084, loss_train: 0.0034, acc_train: 1.0000\n",
      "[Train] batch: 085, loss_train: 0.0026, acc_train: 1.0000\n",
      "[Train] batch: 086, loss_train: 0.0024, acc_train: 1.0000\n",
      "[Val] loss= 0.2157, accuracy= 0.9721, label acc= [1.         0.95652174 1.         1.         0.92307692 0.96\n",
      " 1.         0.90909091 1.        ]\n",
      "Epoch: 6\n",
      "[Train] batch: 001, loss_train: 0.0028, acc_train: 1.0000\n",
      "[Train] batch: 002, loss_train: 0.0019, acc_train: 1.0000\n",
      "[Train] batch: 003, loss_train: 0.0005, acc_train: 1.0000\n",
      "[Train] batch: 004, loss_train: 0.0032, acc_train: 1.0000\n",
      "[Train] batch: 005, loss_train: 0.0006, acc_train: 1.0000\n",
      "[Train] batch: 006, loss_train: 0.0021, acc_train: 1.0000\n",
      "[Train] batch: 007, loss_train: 0.0023, acc_train: 1.0000\n",
      "[Train] batch: 008, loss_train: 0.0008, acc_train: 1.0000\n",
      "[Train] batch: 009, loss_train: 0.0027, acc_train: 1.0000\n",
      "[Train] batch: 010, loss_train: 0.0009, acc_train: 1.0000\n",
      "[Train] batch: 011, loss_train: 0.0043, acc_train: 1.0000\n",
      "[Train] batch: 012, loss_train: 0.0014, acc_train: 1.0000\n",
      "[Train] batch: 013, loss_train: 0.0053, acc_train: 1.0000\n",
      "[Train] batch: 014, loss_train: 0.0032, acc_train: 1.0000\n",
      "[Train] batch: 015, loss_train: 0.0011, acc_train: 1.0000\n",
      "[Train] batch: 016, loss_train: 0.0033, acc_train: 1.0000\n",
      "[Train] batch: 017, loss_train: 0.0001, acc_train: 1.0000\n",
      "[Train] batch: 018, loss_train: 0.0014, acc_train: 1.0000\n",
      "[Train] batch: 019, loss_train: 0.0042, acc_train: 1.0000\n",
      "[Train] batch: 020, loss_train: 0.0014, acc_train: 1.0000\n",
      "[Train] batch: 021, loss_train: 0.0039, acc_train: 1.0000\n",
      "[Train] batch: 022, loss_train: 0.0029, acc_train: 1.0000\n",
      "[Train] batch: 023, loss_train: 0.0018, acc_train: 1.0000\n",
      "[Train] batch: 024, loss_train: 0.0019, acc_train: 1.0000\n",
      "[Train] batch: 025, loss_train: 0.0014, acc_train: 1.0000\n",
      "[Train] batch: 026, loss_train: 0.0011, acc_train: 1.0000\n",
      "[Train] batch: 027, loss_train: 0.0010, acc_train: 1.0000\n",
      "[Train] batch: 028, loss_train: 0.0670, acc_train: 1.0000\n",
      "[Train] batch: 029, loss_train: 0.0008, acc_train: 1.0000\n",
      "[Train] batch: 030, loss_train: 0.0043, acc_train: 1.0000\n",
      "[Train] batch: 031, loss_train: 0.0025, acc_train: 1.0000\n",
      "[Train] batch: 032, loss_train: 0.0010, acc_train: 1.0000\n",
      "[Train] batch: 033, loss_train: 0.0010, acc_train: 1.0000\n",
      "[Train] batch: 034, loss_train: 0.0003, acc_train: 1.0000\n",
      "[Train] batch: 035, loss_train: 0.0001, acc_train: 1.0000\n",
      "[Train] batch: 036, loss_train: 0.0013, acc_train: 1.0000\n",
      "[Train] batch: 037, loss_train: 0.0121, acc_train: 1.0000\n",
      "[Train] batch: 038, loss_train: 0.0022, acc_train: 1.0000\n",
      "[Train] batch: 039, loss_train: 0.0026, acc_train: 1.0000\n",
      "[Train] batch: 040, loss_train: 0.0010, acc_train: 1.0000\n",
      "[Train] batch: 041, loss_train: 0.2054, acc_train: 0.8600\n",
      "[Train] batch: 042, loss_train: 0.0006, acc_train: 1.0000\n",
      "[Train] batch: 043, loss_train: 0.0001, acc_train: 1.0000\n",
      "[Train] batch: 044, loss_train: 0.0008, acc_train: 1.0000\n",
      "[Train] batch: 045, loss_train: 0.0006, acc_train: 1.0000\n",
      "[Train] batch: 046, loss_train: 0.0028, acc_train: 1.0000\n",
      "[Train] batch: 047, loss_train: 0.0029, acc_train: 1.0000\n",
      "[Train] batch: 048, loss_train: 0.0010, acc_train: 1.0000\n",
      "[Train] batch: 049, loss_train: 0.0018, acc_train: 1.0000\n",
      "[Train] batch: 050, loss_train: 0.0006, acc_train: 1.0000\n",
      "[Train] batch: 051, loss_train: 0.0016, acc_train: 1.0000\n",
      "[Train] batch: 052, loss_train: 0.0034, acc_train: 1.0000\n",
      "[Train] batch: 053, loss_train: 0.0026, acc_train: 1.0000\n",
      "[Train] batch: 054, loss_train: 0.0016, acc_train: 1.0000\n",
      "[Train] batch: 055, loss_train: 0.0048, acc_train: 1.0000\n",
      "[Train] batch: 056, loss_train: 0.0001, acc_train: 1.0000\n",
      "[Train] batch: 057, loss_train: 0.0109, acc_train: 1.0000\n",
      "[Train] batch: 058, loss_train: 0.0027, acc_train: 1.0000\n",
      "[Train] batch: 059, loss_train: 0.0249, acc_train: 1.0000\n",
      "[Train] batch: 060, loss_train: 0.0118, acc_train: 1.0000\n",
      "[Train] batch: 061, loss_train: 0.0056, acc_train: 1.0000\n",
      "[Train] batch: 062, loss_train: 0.0024, acc_train: 1.0000\n",
      "[Train] batch: 063, loss_train: 0.1105, acc_train: 0.9000\n",
      "[Train] batch: 064, loss_train: 0.0051, acc_train: 1.0000\n",
      "[Train] batch: 065, loss_train: 0.0013, acc_train: 1.0000\n",
      "[Train] batch: 066, loss_train: 0.0096, acc_train: 1.0000\n",
      "[Train] batch: 067, loss_train: 0.0011, acc_train: 1.0000\n",
      "[Train] batch: 068, loss_train: 0.0014, acc_train: 1.0000\n",
      "[Train] batch: 069, loss_train: 0.0035, acc_train: 1.0000\n",
      "[Train] batch: 070, loss_train: 0.0008, acc_train: 1.0000\n",
      "[Train] batch: 071, loss_train: 0.0010, acc_train: 1.0000\n",
      "[Train] batch: 072, loss_train: 0.0008, acc_train: 1.0000\n",
      "[Train] batch: 073, loss_train: 0.0016, acc_train: 1.0000\n",
      "[Train] batch: 074, loss_train: 0.0037, acc_train: 1.0000\n",
      "[Train] batch: 075, loss_train: 0.0015, acc_train: 1.0000\n",
      "[Train] batch: 076, loss_train: 0.0017, acc_train: 1.0000\n",
      "[Train] batch: 077, loss_train: 0.0042, acc_train: 1.0000\n",
      "[Train] batch: 078, loss_train: 0.0011, acc_train: 1.0000\n",
      "[Train] batch: 079, loss_train: 0.0397, acc_train: 1.0000\n",
      "[Train] batch: 080, loss_train: 0.0022, acc_train: 1.0000\n",
      "[Train] batch: 081, loss_train: 0.0026, acc_train: 1.0000\n",
      "[Train] batch: 082, loss_train: 0.0025, acc_train: 1.0000\n",
      "[Train] batch: 083, loss_train: 0.0036, acc_train: 1.0000\n",
      "[Train] batch: 084, loss_train: 0.0009, acc_train: 1.0000\n",
      "[Train] batch: 085, loss_train: 0.0032, acc_train: 1.0000\n",
      "[Train] batch: 086, loss_train: 0.0025, acc_train: 1.0000\n",
      "[Val] loss= 0.2062, accuracy= 0.9628, label acc= [0.95652174 0.88       1.         1.         0.96       0.96\n",
      " 1.         0.90909091 1.        ]\n",
      "Epoch: 7\n",
      "[Train] batch: 001, loss_train: 0.0014, acc_train: 1.0000\n",
      "[Train] batch: 002, loss_train: 0.0011, acc_train: 1.0000\n",
      "[Train] batch: 003, loss_train: 0.0017, acc_train: 1.0000\n",
      "[Train] batch: 004, loss_train: 0.0038, acc_train: 1.0000\n",
      "[Train] batch: 005, loss_train: 0.0062, acc_train: 1.0000\n",
      "[Train] batch: 006, loss_train: 0.0021, acc_train: 1.0000\n",
      "[Train] batch: 007, loss_train: 0.0022, acc_train: 1.0000\n",
      "[Train] batch: 008, loss_train: 0.0030, acc_train: 1.0000\n",
      "[Train] batch: 009, loss_train: 0.0016, acc_train: 1.0000\n",
      "[Train] batch: 010, loss_train: 0.0008, acc_train: 1.0000\n",
      "[Train] batch: 011, loss_train: 0.0023, acc_train: 1.0000\n",
      "[Train] batch: 012, loss_train: 0.0001, acc_train: 1.0000\n",
      "[Train] batch: 013, loss_train: 0.0012, acc_train: 1.0000\n",
      "[Train] batch: 014, loss_train: 0.0007, acc_train: 1.0000\n",
      "[Train] batch: 015, loss_train: 0.0030, acc_train: 1.0000\n",
      "[Train] batch: 016, loss_train: 0.0013, acc_train: 1.0000\n",
      "[Train] batch: 017, loss_train: 0.0008, acc_train: 1.0000\n",
      "[Train] batch: 018, loss_train: 0.0031, acc_train: 1.0000\n",
      "[Train] batch: 019, loss_train: 0.0018, acc_train: 1.0000\n",
      "[Train] batch: 020, loss_train: 0.0024, acc_train: 1.0000\n",
      "[Train] batch: 021, loss_train: 0.0012, acc_train: 1.0000\n",
      "[Train] batch: 022, loss_train: 0.0010, acc_train: 1.0000\n",
      "[Train] batch: 023, loss_train: 0.0012, acc_train: 1.0000\n",
      "[Train] batch: 024, loss_train: 0.0033, acc_train: 1.0000\n",
      "[Train] batch: 025, loss_train: 0.0006, acc_train: 1.0000\n",
      "[Train] batch: 026, loss_train: 0.0001, acc_train: 1.0000\n",
      "[Train] batch: 027, loss_train: 0.0006, acc_train: 1.0000\n",
      "[Train] batch: 028, loss_train: 0.0015, acc_train: 1.0000\n",
      "[Train] batch: 029, loss_train: 0.0018, acc_train: 1.0000\n",
      "[Train] batch: 030, loss_train: 0.0022, acc_train: 1.0000\n",
      "[Train] batch: 031, loss_train: 0.3906, acc_train: 0.9400\n",
      "[Train] batch: 032, loss_train: 0.0001, acc_train: 1.0000\n",
      "[Train] batch: 033, loss_train: 0.0002, acc_train: 1.0000\n",
      "[Train] batch: 034, loss_train: 0.0010, acc_train: 1.0000\n",
      "[Train] batch: 035, loss_train: 0.0006, acc_train: 1.0000\n",
      "[Train] batch: 036, loss_train: 0.0000, acc_train: 1.0000\n",
      "[Train] batch: 037, loss_train: 0.0053, acc_train: 1.0000\n",
      "[Train] batch: 038, loss_train: 0.0030, acc_train: 1.0000\n",
      "[Train] batch: 039, loss_train: 0.0411, acc_train: 1.0000\n",
      "[Train] batch: 040, loss_train: 0.0022, acc_train: 1.0000\n",
      "[Train] batch: 041, loss_train: 0.0004, acc_train: 1.0000\n",
      "[Train] batch: 042, loss_train: 0.0564, acc_train: 1.0000\n",
      "[Train] batch: 043, loss_train: 0.0068, acc_train: 1.0000\n",
      "[Train] batch: 044, loss_train: 0.0008, acc_train: 1.0000\n",
      "[Train] batch: 045, loss_train: 0.0032, acc_train: 1.0000\n",
      "[Train] batch: 046, loss_train: 0.0009, acc_train: 1.0000\n",
      "[Train] batch: 047, loss_train: 0.0262, acc_train: 1.0000\n",
      "[Train] batch: 048, loss_train: 0.0238, acc_train: 1.0000\n",
      "[Train] batch: 049, loss_train: 0.0010, acc_train: 1.0000\n",
      "[Train] batch: 050, loss_train: 0.0016, acc_train: 1.0000\n",
      "[Train] batch: 051, loss_train: 0.1042, acc_train: 0.9067\n",
      "[Train] batch: 052, loss_train: 0.0053, acc_train: 1.0000\n",
      "[Train] batch: 053, loss_train: 0.0017, acc_train: 1.0000\n",
      "[Train] batch: 054, loss_train: 0.0052, acc_train: 1.0000\n",
      "[Train] batch: 055, loss_train: 0.0010, acc_train: 1.0000\n",
      "[Train] batch: 056, loss_train: 0.0037, acc_train: 1.0000\n",
      "[Train] batch: 057, loss_train: 0.0053, acc_train: 1.0000\n",
      "[Train] batch: 058, loss_train: 0.0008, acc_train: 1.0000\n",
      "[Train] batch: 059, loss_train: 0.0124, acc_train: 1.0000\n",
      "[Train] batch: 060, loss_train: 0.0008, acc_train: 1.0000\n",
      "[Train] batch: 061, loss_train: 0.0023, acc_train: 1.0000\n",
      "[Train] batch: 062, loss_train: 0.0026, acc_train: 1.0000\n",
      "[Train] batch: 063, loss_train: 0.0017, acc_train: 1.0000\n",
      "[Train] batch: 064, loss_train: 0.0028, acc_train: 1.0000\n",
      "[Train] batch: 065, loss_train: 0.0030, acc_train: 1.0000\n",
      "[Train] batch: 066, loss_train: 0.0018, acc_train: 1.0000\n",
      "[Train] batch: 067, loss_train: 0.0001, acc_train: 1.0000\n",
      "[Train] batch: 068, loss_train: 0.0001, acc_train: 1.0000\n",
      "[Train] batch: 069, loss_train: 0.0000, acc_train: 1.0000\n",
      "[Train] batch: 070, loss_train: 0.0005, acc_train: 1.0000\n",
      "[Train] batch: 071, loss_train: 0.0012, acc_train: 1.0000\n",
      "[Train] batch: 072, loss_train: 0.0009, acc_train: 1.0000\n",
      "[Train] batch: 073, loss_train: 0.0031, acc_train: 1.0000\n",
      "[Train] batch: 074, loss_train: 0.0019, acc_train: 1.0000\n",
      "[Train] batch: 075, loss_train: 0.0012, acc_train: 1.0000\n",
      "[Train] batch: 076, loss_train: 0.0037, acc_train: 1.0000\n",
      "[Train] batch: 077, loss_train: 0.0225, acc_train: 1.0000\n",
      "[Train] batch: 078, loss_train: 0.0010, acc_train: 1.0000\n",
      "[Train] batch: 079, loss_train: 0.0005, acc_train: 1.0000\n",
      "[Train] batch: 080, loss_train: 0.0027, acc_train: 1.0000\n",
      "[Train] batch: 081, loss_train: 0.0005, acc_train: 1.0000\n",
      "[Train] batch: 082, loss_train: 0.0021, acc_train: 1.0000\n",
      "[Train] batch: 083, loss_train: 0.0049, acc_train: 1.0000\n",
      "[Train] batch: 084, loss_train: 0.0018, acc_train: 1.0000\n",
      "[Train] batch: 085, loss_train: 0.0115, acc_train: 1.0000\n",
      "[Train] batch: 086, loss_train: 0.0009, acc_train: 1.0000\n",
      "[Val] loss= 0.2349, accuracy= 0.9721, label acc= [1.         0.95652174 1.         1.         0.92307692 0.96\n",
      " 1.         0.90909091 1.        ]\n",
      "Epoch: 8\n",
      "[Train] batch: 001, loss_train: 0.0020, acc_train: 1.0000\n",
      "[Train] batch: 002, loss_train: 0.0014, acc_train: 1.0000\n",
      "[Train] batch: 003, loss_train: 0.0056, acc_train: 1.0000\n",
      "[Train] batch: 004, loss_train: 0.0016, acc_train: 1.0000\n",
      "[Train] batch: 005, loss_train: 0.0014, acc_train: 1.0000\n",
      "[Train] batch: 006, loss_train: 0.0005, acc_train: 1.0000\n",
      "[Train] batch: 007, loss_train: 0.0023, acc_train: 1.0000\n",
      "[Train] batch: 008, loss_train: 0.0034, acc_train: 1.0000\n",
      "[Train] batch: 009, loss_train: 0.0050, acc_train: 1.0000\n",
      "[Train] batch: 010, loss_train: 0.0014, acc_train: 1.0000\n",
      "[Train] batch: 011, loss_train: 0.0013, acc_train: 1.0000\n",
      "[Train] batch: 012, loss_train: 0.0036, acc_train: 1.0000\n",
      "[Train] batch: 013, loss_train: 0.0008, acc_train: 1.0000\n",
      "[Train] batch: 014, loss_train: 0.0006, acc_train: 1.0000\n",
      "[Train] batch: 015, loss_train: 0.0116, acc_train: 1.0000\n",
      "[Train] batch: 016, loss_train: 0.0012, acc_train: 1.0000\n",
      "[Train] batch: 017, loss_train: 0.0019, acc_train: 1.0000\n",
      "[Train] batch: 018, loss_train: 0.0008, acc_train: 1.0000\n",
      "[Train] batch: 019, loss_train: 0.0015, acc_train: 1.0000\n",
      "[Train] batch: 020, loss_train: 0.0005, acc_train: 1.0000\n",
      "[Train] batch: 021, loss_train: 0.0006, acc_train: 1.0000\n",
      "[Train] batch: 022, loss_train: 0.0019, acc_train: 1.0000\n",
      "[Train] batch: 023, loss_train: 0.0031, acc_train: 1.0000\n",
      "[Train] batch: 024, loss_train: 0.0008, acc_train: 1.0000\n",
      "[Train] batch: 025, loss_train: 0.0005, acc_train: 1.0000\n",
      "[Train] batch: 026, loss_train: 0.0008, acc_train: 1.0000\n",
      "[Train] batch: 027, loss_train: 0.0013, acc_train: 1.0000\n",
      "[Train] batch: 028, loss_train: 0.0002, acc_train: 1.0000\n",
      "[Train] batch: 029, loss_train: 0.0006, acc_train: 1.0000\n",
      "[Train] batch: 030, loss_train: 0.0103, acc_train: 1.0000\n",
      "[Train] batch: 031, loss_train: 0.0008, acc_train: 1.0000\n",
      "[Train] batch: 032, loss_train: 0.0022, acc_train: 1.0000\n",
      "[Train] batch: 033, loss_train: 0.0018, acc_train: 1.0000\n",
      "[Train] batch: 034, loss_train: 0.0022, acc_train: 1.0000\n",
      "[Train] batch: 035, loss_train: 0.0004, acc_train: 1.0000\n",
      "[Train] batch: 036, loss_train: 0.0015, acc_train: 1.0000\n",
      "[Train] batch: 037, loss_train: 0.0005, acc_train: 1.0000\n",
      "[Train] batch: 038, loss_train: 0.0018, acc_train: 1.0000\n",
      "[Train] batch: 039, loss_train: 0.0008, acc_train: 1.0000\n",
      "[Train] batch: 040, loss_train: 0.0151, acc_train: 1.0000\n",
      "[Train] batch: 041, loss_train: 0.0023, acc_train: 1.0000\n",
      "[Train] batch: 042, loss_train: 0.0013, acc_train: 1.0000\n",
      "[Train] batch: 043, loss_train: 0.0010, acc_train: 1.0000\n",
      "[Train] batch: 044, loss_train: 0.0010, acc_train: 1.0000\n",
      "[Train] batch: 045, loss_train: 0.0016, acc_train: 1.0000\n",
      "[Train] batch: 046, loss_train: 0.0021, acc_train: 1.0000\n",
      "[Train] batch: 047, loss_train: 0.0011, acc_train: 1.0000\n",
      "[Train] batch: 048, loss_train: 0.0001, acc_train: 1.0000\n",
      "[Train] batch: 049, loss_train: 0.0017, acc_train: 1.0000\n",
      "[Train] batch: 050, loss_train: 0.0017, acc_train: 1.0000\n",
      "[Train] batch: 051, loss_train: 0.0007, acc_train: 1.0000\n",
      "[Train] batch: 052, loss_train: 0.0017, acc_train: 1.0000\n",
      "[Train] batch: 053, loss_train: 0.0004, acc_train: 1.0000\n",
      "[Train] batch: 054, loss_train: 0.0009, acc_train: 1.0000\n",
      "[Train] batch: 055, loss_train: 0.0018, acc_train: 1.0000\n",
      "[Train] batch: 056, loss_train: 0.0010, acc_train: 1.0000\n",
      "[Train] batch: 057, loss_train: 0.0018, acc_train: 1.0000\n",
      "[Train] batch: 058, loss_train: 0.0013, acc_train: 1.0000\n",
      "[Train] batch: 059, loss_train: 0.0009, acc_train: 1.0000\n",
      "[Train] batch: 060, loss_train: 0.0005, acc_train: 1.0000\n",
      "[Train] batch: 061, loss_train: 0.0008, acc_train: 1.0000\n",
      "[Train] batch: 062, loss_train: 0.0013, acc_train: 1.0000\n",
      "[Train] batch: 063, loss_train: 0.0057, acc_train: 1.0000\n",
      "[Train] batch: 064, loss_train: 0.0017, acc_train: 1.0000\n",
      "[Train] batch: 065, loss_train: 0.0017, acc_train: 1.0000\n",
      "[Train] batch: 066, loss_train: 0.0007, acc_train: 1.0000\n",
      "[Train] batch: 067, loss_train: 0.0007, acc_train: 1.0000\n",
      "[Train] batch: 068, loss_train: 0.0029, acc_train: 1.0000\n",
      "[Train] batch: 069, loss_train: 0.0010, acc_train: 1.0000\n",
      "[Train] batch: 070, loss_train: 0.0017, acc_train: 1.0000\n",
      "[Train] batch: 071, loss_train: 0.0014, acc_train: 1.0000\n",
      "[Train] batch: 072, loss_train: 0.0006, acc_train: 1.0000\n",
      "[Train] batch: 073, loss_train: 0.0009, acc_train: 1.0000\n",
      "[Train] batch: 074, loss_train: 0.0020, acc_train: 1.0000\n",
      "[Train] batch: 075, loss_train: 0.0004, acc_train: 1.0000\n",
      "[Train] batch: 076, loss_train: 0.0009, acc_train: 1.0000\n",
      "[Train] batch: 077, loss_train: 0.0004, acc_train: 1.0000\n",
      "[Train] batch: 078, loss_train: 0.0005, acc_train: 1.0000\n",
      "[Train] batch: 079, loss_train: 0.0000, acc_train: 1.0000\n",
      "[Train] batch: 080, loss_train: 0.0006, acc_train: 1.0000\n",
      "[Train] batch: 081, loss_train: 0.0031, acc_train: 1.0000\n",
      "[Train] batch: 082, loss_train: 0.0120, acc_train: 1.0000\n",
      "[Train] batch: 083, loss_train: 0.0004, acc_train: 1.0000\n",
      "[Train] batch: 084, loss_train: 0.0019, acc_train: 1.0000\n",
      "[Train] batch: 085, loss_train: 0.0073, acc_train: 1.0000\n",
      "[Train] batch: 086, loss_train: 0.0007, acc_train: 1.0000\n",
      "[Val] loss= 0.2339, accuracy= 0.9721, label acc= [1.         0.95652174 1.         1.         0.92307692 0.96\n",
      " 1.         0.90909091 1.        ]\n",
      "Epoch: 9\n",
      "[Train] batch: 001, loss_train: 0.0003, acc_train: 1.0000\n",
      "[Train] batch: 002, loss_train: 0.0004, acc_train: 1.0000\n",
      "[Train] batch: 003, loss_train: 0.0008, acc_train: 1.0000\n",
      "[Train] batch: 004, loss_train: 0.0001, acc_train: 1.0000\n",
      "[Train] batch: 005, loss_train: 0.0007, acc_train: 1.0000\n",
      "[Train] batch: 006, loss_train: 0.0005, acc_train: 1.0000\n",
      "[Train] batch: 007, loss_train: 0.0011, acc_train: 1.0000\n",
      "[Train] batch: 008, loss_train: 0.0009, acc_train: 1.0000\n",
      "[Train] batch: 009, loss_train: 0.0023, acc_train: 1.0000\n",
      "[Train] batch: 010, loss_train: 0.0005, acc_train: 1.0000\n",
      "[Train] batch: 011, loss_train: 0.0015, acc_train: 1.0000\n",
      "[Train] batch: 012, loss_train: 0.0011, acc_train: 1.0000\n",
      "[Train] batch: 013, loss_train: 0.0013, acc_train: 1.0000\n",
      "[Train] batch: 014, loss_train: 0.0004, acc_train: 1.0000\n",
      "[Train] batch: 015, loss_train: 0.0012, acc_train: 1.0000\n",
      "[Train] batch: 016, loss_train: 0.0012, acc_train: 1.0000\n",
      "[Train] batch: 017, loss_train: 0.0059, acc_train: 1.0000\n",
      "[Train] batch: 018, loss_train: 0.0004, acc_train: 1.0000\n",
      "[Train] batch: 019, loss_train: 0.0009, acc_train: 1.0000\n",
      "[Train] batch: 020, loss_train: 0.0004, acc_train: 1.0000\n",
      "[Train] batch: 021, loss_train: 0.0005, acc_train: 1.0000\n",
      "[Train] batch: 022, loss_train: 0.0006, acc_train: 1.0000\n",
      "[Train] batch: 023, loss_train: 0.0003, acc_train: 1.0000\n",
      "[Train] batch: 024, loss_train: 0.0018, acc_train: 1.0000\n",
      "[Train] batch: 025, loss_train: 0.0026, acc_train: 1.0000\n",
      "[Train] batch: 026, loss_train: 0.0004, acc_train: 1.0000\n",
      "[Train] batch: 027, loss_train: 0.0009, acc_train: 1.0000\n",
      "[Train] batch: 028, loss_train: 0.0008, acc_train: 1.0000\n",
      "[Train] batch: 029, loss_train: 0.0020, acc_train: 1.0000\n",
      "[Train] batch: 030, loss_train: 0.0001, acc_train: 1.0000\n",
      "[Train] batch: 031, loss_train: 0.0008, acc_train: 1.0000\n",
      "[Train] batch: 032, loss_train: 0.0020, acc_train: 1.0000\n",
      "[Train] batch: 033, loss_train: 0.0014, acc_train: 1.0000\n",
      "[Train] batch: 034, loss_train: 0.0004, acc_train: 1.0000\n",
      "[Train] batch: 035, loss_train: 0.0024, acc_train: 1.0000\n",
      "[Train] batch: 036, loss_train: 0.0034, acc_train: 1.0000\n",
      "[Train] batch: 037, loss_train: 0.0019, acc_train: 1.0000\n",
      "[Train] batch: 038, loss_train: 0.0010, acc_train: 1.0000\n",
      "[Train] batch: 039, loss_train: 0.0010, acc_train: 1.0000\n",
      "[Train] batch: 040, loss_train: 0.0012, acc_train: 1.0000\n",
      "[Train] batch: 041, loss_train: 0.0013, acc_train: 1.0000\n",
      "[Train] batch: 042, loss_train: 0.0007, acc_train: 1.0000\n",
      "[Train] batch: 043, loss_train: 0.0004, acc_train: 1.0000\n",
      "[Train] batch: 044, loss_train: 0.0007, acc_train: 1.0000\n",
      "[Train] batch: 045, loss_train: 0.0025, acc_train: 1.0000\n",
      "[Train] batch: 046, loss_train: 0.0011, acc_train: 1.0000\n",
      "[Train] batch: 047, loss_train: 0.0000, acc_train: 1.0000\n",
      "[Train] batch: 048, loss_train: 0.0014, acc_train: 1.0000\n",
      "[Train] batch: 049, loss_train: 0.0004, acc_train: 1.0000\n",
      "[Train] batch: 050, loss_train: 0.0005, acc_train: 1.0000\n",
      "[Train] batch: 051, loss_train: 0.0007, acc_train: 1.0000\n",
      "[Train] batch: 052, loss_train: 0.0007, acc_train: 1.0000\n",
      "[Train] batch: 053, loss_train: 0.0003, acc_train: 1.0000\n",
      "[Train] batch: 054, loss_train: 0.0013, acc_train: 1.0000\n",
      "[Train] batch: 055, loss_train: 0.0014, acc_train: 1.0000\n",
      "[Train] batch: 056, loss_train: 0.0007, acc_train: 1.0000\n",
      "[Train] batch: 057, loss_train: 0.0012, acc_train: 1.0000\n",
      "[Train] batch: 058, loss_train: 0.0010, acc_train: 1.0000\n",
      "[Train] batch: 059, loss_train: 0.0024, acc_train: 1.0000\n",
      "[Train] batch: 060, loss_train: 0.0019, acc_train: 1.0000\n",
      "[Train] batch: 061, loss_train: 0.0012, acc_train: 1.0000\n",
      "[Train] batch: 062, loss_train: 0.0011, acc_train: 1.0000\n",
      "[Train] batch: 063, loss_train: 0.0012, acc_train: 1.0000\n",
      "[Train] batch: 064, loss_train: 0.0001, acc_train: 1.0000\n",
      "[Train] batch: 065, loss_train: 0.0007, acc_train: 1.0000\n",
      "[Train] batch: 066, loss_train: 0.0014, acc_train: 1.0000\n",
      "[Train] batch: 067, loss_train: 0.0022, acc_train: 1.0000\n",
      "[Train] batch: 068, loss_train: 0.0016, acc_train: 1.0000\n",
      "[Train] batch: 069, loss_train: 0.0014, acc_train: 1.0000\n",
      "[Train] batch: 070, loss_train: 0.0000, acc_train: 1.0000\n",
      "[Train] batch: 071, loss_train: 0.0006, acc_train: 1.0000\n",
      "[Train] batch: 072, loss_train: 0.0001, acc_train: 1.0000\n",
      "[Train] batch: 073, loss_train: 0.0021, acc_train: 1.0000\n",
      "[Train] batch: 074, loss_train: 0.0043, acc_train: 1.0000\n",
      "[Train] batch: 075, loss_train: 0.0004, acc_train: 1.0000\n",
      "[Train] batch: 076, loss_train: 0.0012, acc_train: 1.0000\n",
      "[Train] batch: 077, loss_train: 0.0013, acc_train: 1.0000\n",
      "[Train] batch: 078, loss_train: 0.0015, acc_train: 1.0000\n",
      "[Train] batch: 079, loss_train: 0.0018, acc_train: 1.0000\n",
      "[Train] batch: 080, loss_train: 0.0014, acc_train: 1.0000\n",
      "[Train] batch: 081, loss_train: 0.0005, acc_train: 1.0000\n",
      "[Train] batch: 082, loss_train: 0.0010, acc_train: 1.0000\n",
      "[Train] batch: 083, loss_train: 0.0000, acc_train: 1.0000\n",
      "[Train] batch: 084, loss_train: 0.0013, acc_train: 1.0000\n",
      "[Train] batch: 085, loss_train: 0.0007, acc_train: 1.0000\n",
      "[Train] batch: 086, loss_train: 0.0015, acc_train: 1.0000\n",
      "[Val] loss= 0.2317, accuracy= 0.9721, label acc= [1.         0.95652174 1.         1.         0.92307692 0.96\n",
      " 1.         0.90909091 1.        ]\n",
      "[Test] loss= 0.1175, accuracy= 0.9814, label acc= [1.         1.         0.96       0.95652174 0.96       1.\n",
      " 0.95652174 1.         1.        ]\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model and optimizer\n",
    "def load_model(model, optimizer, path, device='cpu'):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(f\"Model loaded from {path}, epoch {epoch}\")\n",
    "    return model, optimizer, epoch\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = LLMGraphTransformer(model_name=\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\", device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "model, optimizer, start_epoch = load_model(model, optimizer, path=\"model/20241109-162008_llm_w_edgefeat.pth\", device=device)\n",
    "labels = ['Normal', 'Audio-Streaming', 'Browsing', 'Chat', 'File-Transfer', 'Email', 'P2P', 'Video-Streaming', 'VOIP']\n",
    "\n",
    "# Define all indices in the dataset\n",
    "label_cpu = label.cpu().numpy()  # Assume 'label' is the tensor of actual labels\n",
    "\n",
    "# Define the indices of the entire dataset\n",
    "all_data_indices = np.arange(len(label_cpu))\n",
    "\n",
    "# Find unused data indices\n",
    "used_data_indices = np.array(train.tolist() + val.tolist() + test.tolist())\n",
    "unused_data_indices = np.setdiff1d(all_data_indices, used_data_indices)\n",
    "\n",
    "# Sample balanced data and labels from unused data only\n",
    "balanced_data, balanced_labels = balance_data(unused_data_indices, label_cpu[unused_data_indices], n_samples_per_label=120)\n",
    "\n",
    "# Split unused balanced data into train, val, and test sets\n",
    "unused_train, unused_temp, unused_train_labels, unused_temp_labels = train_test_split(\n",
    "    balanced_data, balanced_labels, test_size=0.2, stratify=balanced_labels, random_state=42\n",
    ")\n",
    "unused_val, unused_test, unused_val_labels, unused_test_labels = train_test_split(\n",
    "    unused_temp, unused_temp_labels, test_size=0.5, stratify=unused_temp_labels, random_state=42\n",
    ")\n",
    "\n",
    "# Display split results\n",
    "print(f\"Unused Train Data: {len(unused_train)}, Validation: {len(unused_val)}, Test: {len(unused_test)}\")\n",
    "\n",
    "print(\"Label distribution in Unused Train Set:\", dict(zip(*np.unique(unused_train_labels, return_counts=True))))\n",
    "print(\"Label distribution in Unused Validation Set:\", dict(zip(*np.unique(unused_val_labels, return_counts=True))))\n",
    "print(\"Label distribution in Unused Test Set:\", dict(zip(*np.unique(unused_test_labels, return_counts=True))))\n",
    "\n",
    "# Training loop on unused data\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    print(\"Epoch:\", epoch)\n",
    "    unused_train, unused_train_labels = shuffle(unused_train, unused_train_labels, random_state=42)  # Shuffle together\n",
    "    \n",
    "    for batch in range(int(len(unused_train) / 10)):  # Batch size set to 10\n",
    "        batch_edges = unused_train[10 * batch:10 * (batch + 1)]\n",
    "        batch_labels = unused_train_labels[10 * batch:10 * (batch + 1)]\n",
    "        \n",
    "        # Generate predictions using model\n",
    "        batch_text = model.generate_text(batch_edges, labels, max_new_tokens=10)\n",
    "        edge_batch = edge_feat[batch_edges]\n",
    "        logits = model(batch_text, edge_batch).to(device)\n",
    "        \n",
    "        batch_labels_tensor = torch.tensor(batch_labels, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Calculate loss and backpropagate\n",
    "        loss = loss_fn(logits, batch_labels_tensor)\n",
    "        optimizer.zero_grad()  # Zero gradients before backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and print training accuracy\n",
    "        predicted_labels = torch.argmax(logits, dim=-1)\n",
    "        acc_train = f1_score(batch_labels, predicted_labels.cpu().numpy(), average=\"weighted\")\n",
    "        print(f'[Train] batch: {batch + 1:03d}, loss_train: {loss.item():.4f}, acc_train: {acc_train:.4f}')\n",
    "\n",
    "    # Validation after each epoch\n",
    "    model.eval()  # Set model to evaluation mode for validation\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        val_acc, val_loss, val_output = predict_(model, label, loss_fn, unused_val, device, edge_feat)\n",
    "        print(f\"[Val] loss= {val_loss:.4f}, accuracy= {val_acc:.4f}, label acc= {f1_score(unused_val_labels, val_output, average=None)}\")\n",
    "\n",
    "    \n",
    "\n",
    "# Final test evaluation\n",
    "model.eval()  # Set model to evaluation mode for testing\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    acc_test, loss_test, predict_output = predict_(model, label, loss_fn, unused_test, device, edge_feat)\n",
    "    print(f\"[Test] loss= {loss_test:.4f}, accuracy= {acc_test:.4f}, label acc= {f1_score(unused_test_labels, predict_output, average=None)}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "392fdfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splits and labels saved successfully.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b8d8035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splits and labels saved successfully.\n"
     ]
    }
   ],
   "source": [
    "def con1_save_data_splits(train, val, test, train_labels, val_labels, test_labels, path=\"data_splits/\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    with open(os.path.join(path, \"con1_train.pkl\"), \"wb\") as f:\n",
    "        pickle.dump((train, train_labels), f)\n",
    "    with open(os.path.join(path, \"con1_val.pkl\"), \"wb\") as f:\n",
    "        pickle.dump((val, val_labels), f)\n",
    "    with open(os.path.join(path, \"con1_test.pkl\"), \"wb\") as f:\n",
    "        pickle.dump((test, test_labels), f)\n",
    "    print(\"Data splits and labels saved successfully.\")\n",
    "con1_save_data_splits(unused_train, unused_val, unused_test, unused_train_labels, unused_val_labels, unused_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8625a60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] loss= 0.2312, accuracy= 0.9814, label acc= [0.95652174 0.96       0.95652174 1.         0.96       1.\n",
      " 1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "acc_test, loss_test, predict_output = predict_(model, label, loss_fn, test, device, edge_feat)\n",
    "print(f\"[Test] loss= {loss_test:.4f}, accuracy= {acc_test:.4f}, label acc= {f1_score(test_labels, predict_output, average=None)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72591123",
   "metadata": {},
   "source": [
    "# Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "564f79e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to model/20241109-164753_llm_w_edgefeat.pth\n"
     ]
    }
   ],
   "source": [
    "save_model(model, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5609fb42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7c2a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "920054e9",
   "metadata": {},
   "source": [
    "# Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c9a0906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_986490/2436966967.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model/20241109-164753_llm_w_edgefeat.pth, epoch 9\n"
     ]
    }
   ],
   "source": [
    "### import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pickle\n",
    "import math\n",
    "from sklearn.utils import resample\n",
    "\n",
    "class LLMGraphTransformer(nn.Module):\n",
    "    def __init__(self, model_name=\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\", device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # Load the tokenizer and model for TinyLlama\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(self.device)\n",
    "\n",
    "        # Ensure padding token is set for TinyLlama\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "        # New layers to process edge features\n",
    "        self.edge_fc = nn.Linear(77, 64).to(self.device)  \n",
    "        self.edge_dropout = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self, batch_text, edge_features):\n",
    "        # Tokenize text\n",
    "        inputs = self.tokenizer(batch_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
    "        outputs = self.model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "        \n",
    "        # Get the logits for the last token in each sequence (for classification purposes)\n",
    "        text_logits = outputs.logits[:, -1, :]  # Only take the last token's logits for classification\n",
    "\n",
    "        # Process edge features through a fully connected layer\n",
    "        edge_emb = self.edge_fc(edge_features)  # Project edge features to a lower-dimensional space\n",
    "        edge_emb = self.edge_dropout(edge_emb)\n",
    "\n",
    "        # Concatenate the text logits and the edge feature embeddings\n",
    "        combined_logits = torch.cat((text_logits, edge_emb), dim=1)\n",
    "        \n",
    "        return combined_logits\n",
    "\n",
    "    def generate_text(self, graph_data, labels, max_new_tokens=50):\n",
    "        # Convert the graph adjacency list to text directly within this method\n",
    "        batch_text = []\n",
    "        for node, neighbors in enumerate(graph_data):\n",
    "            if isinstance(neighbors, (list, set, np.ndarray)):\n",
    "                for neighbor in neighbors:\n",
    "                    question = f\"What is the relationship between Node {node} and Node {neighbor}? Choices: {', '.join(labels)}.\"\n",
    "                    batch_text.append(question)\n",
    "            else:\n",
    "                question = f\"What is the relationship between Node {node} and Node {neighbors}? Choices: {', '.join(labels)}.\"\n",
    "                batch_text.append(question)\n",
    "\n",
    "        # Tokenize and generate predictions\n",
    "        inputs = self.tokenizer(batch_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
    "        outputs = self.model.generate(\n",
    "            inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "        generated_text = [self.tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "        return generated_text\n",
    "\n",
    "\n",
    "\n",
    "def predict_(model, label, loss_fn, data_idx, device, edge_feat):\n",
    "    predict_output = []\n",
    "    loss = 0.0\n",
    "    num_batches = math.ceil(len(data_idx) / 10)\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "        batch_edges = data_idx[10 * batch:10 * (batch + 1)]\n",
    "        labels = ['Normal', 'Audio-Streaming', 'Browsing', 'Chat', 'File-Transfer',\n",
    "        'Email', 'P2P', 'Video-Streaming', 'VOIP']\n",
    "\n",
    "        # Generate text from batch_edges\n",
    "        batch_text = model.generate_text(batch_edges, labels, max_new_tokens=10)\n",
    "        edge_batch = edge_feat[batch_edges]\n",
    "        # Get logits from the model (floating point values representing class probabilities)\n",
    "        logits = model(batch_text, edge_batch).to(device)  # Use the model to get logits\n",
    "\n",
    "        # Target labels\n",
    "        batch_labels = label[batch_edges].to(device)  # Long type labels for cross_entropy\n",
    "\n",
    "        # Compute the loss using logits (input) and batch_labels (target)\n",
    "        batch_loss = loss_fn(logits, batch_labels)\n",
    "        loss += batch_loss.item()\n",
    "\n",
    "        # Calculate predictions based on logits\n",
    "        predicted_labels = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        predict_output.extend(predicted_labels)\n",
    "\n",
    "    # Normalize loss by the number of batches\n",
    "    loss /= num_batches\n",
    "\n",
    "    # Calculate accuracy using F1 score\n",
    "    acc = f1_score(label.cpu().numpy()[data_idx], predict_output, average=\"weighted\")\n",
    "    return acc, loss, predict_output\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def load_model(model, optimizer, path, device='cpu'):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(f\"Model loaded from {path}, epoch {epoch}\")\n",
    "    return model, optimizer, epoch\n",
    "# Load the model and optimizer\n",
    "model = LLMGraphTransformer(model_name=\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\", device=device)\n",
    "model, optimizer, start_epoch = load_model(model, optimizer, path=\"model/20241109-164753_llm_w_edgefeat.pth\", device=device)\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(llm_graph_transformer.parameters(), lr=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13fe2cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.1116, accuracy= 0.9907, label acc= [1.         1.         1.         0.95652174 0.96       1.\n",
      " 1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Set device to CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Path setup\n",
    "path = \"datasets/Darknet/\"\n",
    "if not path.endswith('/'):\n",
    "    path += '/'\n",
    "\n",
    "\n",
    "# Load dataset files\n",
    "edge_feat = torch.tensor(np.load(path + \"edge_feat_scaled.npy\", allow_pickle=True), dtype=torch.float, device=device)\n",
    "label = torch.tensor(np.load(path + \"label_mul.npy\", allow_pickle=True), dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "\n",
    "# Labels for relationship types\n",
    "labels = ['Normal', 'Audio-Streaming', 'Browsing', 'Chat', 'File-Transfer', 'Email', 'P2P', 'Video-Streaming', 'VOIP']\n",
    "\n",
    "# Test the model\n",
    "label_cpu = label.cpu().numpy()\n",
    "acc_test, loss_test, predict_output = predict_(model, label, loss_fn, unused_test, device, edge_feat)\n",
    "print(f\"Test set results: loss= {loss_test:.4f}, accuracy= {acc_test:.4f}, label acc= {f1_score(label_cpu[unused_test], predict_output, average=None)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a3db0b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.2312, accuracy= 0.9815, f1_score(weighted)= 0.9814\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "         Normal       1.00      0.92      0.96        12\n",
      "Audio-Streaming       0.92      1.00      0.96        12\n",
      "       Browsing       1.00      0.92      0.96        12\n",
      "           Chat       1.00      1.00      1.00        12\n",
      "  File-Transfer       0.92      1.00      0.96        12\n",
      "          Email       1.00      1.00      1.00        12\n",
      "            P2P       1.00      1.00      1.00        12\n",
      "Video-Streaming       1.00      1.00      1.00        12\n",
      "           VOIP       1.00      1.00      1.00        12\n",
      "\n",
      "       accuracy                           0.98       108\n",
      "      macro avg       0.98      0.98      0.98       108\n",
      "   weighted avg       0.98      0.98      0.98       108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "# Set device to CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Path setup\n",
    "path = \"datasets/Darknet/\"\n",
    "if not path.endswith('/'):\n",
    "    path += '/'\n",
    "\n",
    "edge_feat = torch.tensor(np.load(path + \"edge_feat_scaled.npy\", allow_pickle=True), dtype=torch.float, device=device)\n",
    "label = torch.tensor(np.load(path + \"label_mul.npy\", allow_pickle=True), dtype=torch.long, device=device)\n",
    "\n",
    "# Labels for relationship types\n",
    "labels = ['Normal', 'Audio-Streaming', 'Browsing', 'Chat', 'File-Transfer', 'Email', 'P2P', 'Video-Streaming', 'VOIP']\n",
    "\n",
    "# Test the model\n",
    "label_cpu = label.cpu().numpy()\n",
    "f1_test, loss_test, predict_output = predict_(model, label, loss_fn, test, device, edge_feat)\n",
    "\n",
    "# Calculate accuracy and F1 score for weighted average\n",
    "accuracy = accuracy_score(label_cpu[test], predict_output)\n",
    "f1_weighted = f1_score(label_cpu[test], predict_output, average=\"weighted\")\n",
    "report = classification_report(label_cpu[test], predict_output, target_names=labels)\n",
    "\n",
    "# Print test set results\n",
    "print(f\"Test set results: loss= {loss_test:.4f}, accuracy= {accuracy:.4f}, f1_score(weighted)= {f1_weighted:.4f}\")\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3c64859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.1116, accuracy= 0.9907, f1_score(weighted)= 0.9907\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "         Normal       1.00      1.00      1.00        12\n",
      "Audio-Streaming       1.00      1.00      1.00        12\n",
      "       Browsing       1.00      1.00      1.00        12\n",
      "           Chat       1.00      0.92      0.96        12\n",
      "  File-Transfer       0.92      1.00      0.96        12\n",
      "          Email       1.00      1.00      1.00        12\n",
      "            P2P       1.00      1.00      1.00        12\n",
      "Video-Streaming       1.00      1.00      1.00        12\n",
      "           VOIP       1.00      1.00      1.00        12\n",
      "\n",
      "       accuracy                           0.99       108\n",
      "      macro avg       0.99      0.99      0.99       108\n",
      "   weighted avg       0.99      0.99      0.99       108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "# Set device to CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Path setup\n",
    "path = \"datasets/Darknet/\"\n",
    "if not path.endswith('/'):\n",
    "    path += '/'\n",
    "\n",
    "edge_feat = torch.tensor(np.load(path + \"edge_feat_scaled.npy\", allow_pickle=True), dtype=torch.float, device=device)\n",
    "label = torch.tensor(np.load(path + \"label_mul.npy\", allow_pickle=True), dtype=torch.long, device=device)\n",
    "\n",
    "# Labels for relationship types\n",
    "labels = ['Normal', 'Audio-Streaming', 'Browsing', 'Chat', 'File-Transfer', 'Email', 'P2P', 'Video-Streaming', 'VOIP']\n",
    "\n",
    "# Test the model\n",
    "label_cpu = label.cpu().numpy()\n",
    "f1_test, loss_test, predict_output = predict_(model, label, loss_fn, unused_test, device, edge_feat)\n",
    "\n",
    "# Calculate accuracy and F1 score for weighted average\n",
    "accuracy = accuracy_score(label_cpu[unused_test], predict_output)\n",
    "f1_weighted = f1_score(label_cpu[unused_test], predict_output, average=\"weighted\")\n",
    "report = classification_report(label_cpu[unused_test], predict_output, target_names=labels)\n",
    "\n",
    "# Print test set results\n",
    "print(f\"Test set results: loss= {loss_test:.4f}, accuracy= {accuracy:.4f}, f1_score(weighted)= {f1_weighted:.4f}\")\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d15694d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
